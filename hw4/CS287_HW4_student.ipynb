{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS287_HW4_student.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SWhUGaNR26Zt",
        "Om7RMzm7G5gn",
        "3drb-dhOG5go",
        "9tQ7ymW126Zz",
        "QqqeDtxY26Z-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ7lCmzV26Zp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54a97764-c5ed-494d-fb0a-1195e2912879"
      },
      "source": [
        "################################\n",
        "# RUN THIS CELL\n",
        "################################\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"style.css\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "style.css"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWhUGaNR26Zt"
      },
      "source": [
        "<div class='header_mint'>\n",
        "\n",
        "# <img style=\"float: left; padding-right: 10px; width: 60px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295/CS287/E-115B: Deep Learning for NLP\n",
        "\n",
        "<br/>\n",
        "<hr color=black>\n",
        "\n",
        "## Homework 4: Great Party Trick - Second Edition\n",
        "### THE MINT BOOK\n",
        "\n",
        "**Harvard University**<br/>\n",
        "**Fall 2021**<br/>\n",
        "**Instructor**: Chris Tanner<br/>\n",
        "**Release Date**: Oct 19 (Tues) @ 11:59pm<br/>\n",
        "<font color=\"red\">**Due Date**: Nov 2 (Tues) @ 11:59pm</font>\n",
        "\n",
        "<hr color=black>\n",
        "<center>\n",
        "<div class='quote'>\n",
        "\n",
        "_\"The goal will be to make this one short.\"_\n",
        "\n",
        "    Chris Tanner (October 17, 2021)\n",
        "</div>\n",
        "</center>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om7RMzm7G5gn"
      },
      "source": [
        "<div class='header_mint'>\n",
        "    \n",
        "# LEARNING OBJECTIVES\n",
        "\n",
        "</div>\n",
        "<br/>\n",
        "\n",
        "The purpose of this homework is to help you:\n",
        "\n",
        "- think critically about serious ethical issues\n",
        "- gain practical experience using Transformers for generation\n",
        "- understand the power and shortcomings of GPT-2\n",
        "- become more comfortable reading research papers\n",
        "\n",
        "To assist you reach these learning objectives, this homework is structured into three parts:\n",
        "- <span style=\"background-color: #FDFFB6\"><b>Foundation (concepts):</b></span> demonstrate an understanding of the core concepts taught in lectures\n",
        "- <span style=\"background-color: #FFC8C8\"><b>Application (programming):</b></span> gain experience putting that knowledge into practice \n",
        "- <span style=\"background-color: #CAFFBF\"><b>Research (creating new knowledge):</b></span> use your current NLP knowledge and skills to go beyond the course material, to grasp cutting-edge results and to critically accept or challenge that information. This serves as practice for you to research your own NLP interests and to be well-equipped to continuously learn the latest, greatest NLP work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3drb-dhOG5go"
      },
      "source": [
        "<div class='header_mint'>\n",
        "    \n",
        "## SUPPORT\n",
        "\n",
        "</div>\n",
        "\n",
        "- **Supplemental Resources:** See the list of [supplemental resources](https://harvard-iacs.github.io/CS287/supplemental) for a wealth of rich information concerning Machine Learning, NLP, and Math. Some of the courses listed concern the exact topics covered in this homework and lectures.\n",
        "- **Sanity Check cells:** We provide several 'sanity check' cells which allow you to see our expected outputs. You should ensure your code produces the same. <span style=\"background-color: #FDFFB6\"><b>**NOTE:** We are not claiming that passing the sanity check cells indicates that you have _fully_ implemented everything correctly; rather, they provide simple checks to help inform you if you are on the right track.</span>\n",
        "- **Ed**: If you are stuck on anything conceptual (not code) about the content from lectures, please post a question on Ed. This is your community, and please contribute and help each other out. If your questions concern the homework, you can post these on Ed, too, but make sure you are not posting any of your code or solutions in general. If you think you've spotted a bug in our homework questions, or something that needs clarifying, please let us know on Ed! We want to correct these issues ASAP.\n",
        "- **OH:** After having given a wholehearted attempt, if you are having trouble with the homework, please come to Office Hours.\n",
        "- **Classmates:** We have a strict policy about the homeworks being individual. You are free to discuss _concepts_ with one another, to help each other learn the material. However, no student shall ever discuss their solutions or see another student's solutions to any problem. Once you see someone's coding solution, it's nearly impossible to harness that information in a way that you can write your own unique solution. You've been robbed of a learning opportunity and will likely just regurgitate someone else's work. As a reminder, if you want to take a shortcut on any problem by looking online for already-existing solutions, that's permissible, but you must cite your sources. Otherwise, it constitutes cheating. Posting any pieces of this homework online for others to see if a flagrant violation of our academic policy.\n",
        "- **Other:** I want everyone to be and feel fully supported. If there's anything else we, as a teaching staff, can do to further assist in your learning, please let us know. Related, at the end of this homework assignment, you are expected to complete an anonymous feedback form. I urge you to critically and earnestly think about your own learning, communicate to us your thoughts, and to optionally tell us possible adjustments we could make so that you meet our learning expectations and you achieve your own learning goals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhL5ardpG5gp"
      },
      "source": [
        "<div class='header_purple'>\n",
        "    \n",
        "# 1. FOUNDATION (CONCEPTS) [8 points]\n",
        "\n",
        "</div>\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91_Vwp4SG5gq"
      },
      "source": [
        "<div class='q_purple'><b>1.1 Debiasing [2 points]</b>\n",
        "\n",
        "In Lecture 13, Ellie Lasater-Guttmann gave a lecture on biased embeddings, where we mostly discussed type-based word embeddings. At the end, she mentioned that contextualized word representations (aka token-based embeddings) are not immune, as they can lead to undesirable consequences, too. In general, which do you think would be harder to debias, contextualized word representations (i.e., token-based) or word embeddings (i.e., type-based)? Explain your reasoning in 3-5 sentences.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVWC1p9lG5gq"
      },
      "source": [
        "## My Answer:\n",
        "\n",
        "I think it is harder to debias contextualized word representations than word embeddings. In particular, I believe that in many cases the bias that appear when using certain words in certain situations are represented not only by the word itself but more by the context in which the word is used. In this sense, the contextualized word representations capture better the context in which a word is used and thus these are the embeddings/vectors that will capture the information from the context as well as the information from the word itself. In this sense, the approach we had of modifying the corpus for debiasing our representations might be harder to achieve since we would not only need to change the word itself but rather change the whole context of the word.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI261sI526Zy"
      },
      "source": [
        "<div class='q_purple'><b>1.2 Responsibility [6 points]</b>\n",
        "\n",
        "Large language models (LLMs) like BERT and GPT-2 are usually developed by big software companies who have the computational resources and interest to create such. Keep in mind that LLMs will continue to be developed by a myriad of companies for the foreseeable future (for decades, I'd guess), so this is not a dig at the creators of BERT (Google) or GPT-2 (OpenAI). Let's say you've just graduated from Harvard and are working for a company named Transparent AI (I'm making up this name, so if such a company exists, it's not intentional). You are on a team of 50 people who work on building a LLM, and your specific job is to make an aspect of the model more parallelizable and computationally efficient.\n",
        "\n",
        "**Q1 (3 points)**: Do you feel you have any responsibility to help ensure the model's predictions are equitable and as bias-free as possible? If so, then what are possible actions you should/would take? To be clear, this question isn't asking what technical aspects you would add to the model; it's asking what human-level actions you would take, such as speaking with others, volunteering to do extra engineering work, etc. Please discuss with a minimum of 3 sentences.\n",
        "\n",
        "**Q2 (3 points)**: The company is large and very hierarchical. Everyone has quarterly expectations, a manager to appease, yet a finite amount of time and personal lives and hobbies outside of work. Where should responsibility lie in addressing these issues? Meaning, who all should be accountable? Please discuss with a minimum of 3 sentences.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tQ7ymW126Zz"
      },
      "source": [
        "## My answer\n",
        "\n",
        "Q1: I do have a responsibility to help ensure the model's predictions are equitable and as bias-free as possible. I could probably set up recurrent meetings with other developers to make sure we are all doing something to make our part to have a bias-free model and pplan methods to continually test if our embeddings are starting to show significant bias. I could also volunteer to get bias-free datasets or convert portions of our training datasets to be bias free. I could also try to get partnership with institutes/universities that do work to prevent bias in AI to do workshops/get projects going that prevent the predictions to be biased.\n",
        "\n",
        "Q2: I think mostly the managers should receive the responsibility of making sure that the predictions are unbiased. I say this because they are the ones responsible of managing the team and as such they should make sure that the work needed to prevent biases in our predictions is distributed among the team members. IN that sense, everyone in the team will be responsible for some small piece of this task and as such the whole team will be responsible for making sure the predictions are not biased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG-wtTycG5gs"
      },
      "source": [
        "<div class='header_pink'>\n",
        "    \n",
        "# 2. APPLICATION (PROGRAMMING) [60 points]\n",
        "\n",
        "</div>\n",
        "\n",
        "\n",
        "# Fine-Tuning GPT2 for Song Lyric Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2xDryYs0Ou3"
      },
      "source": [
        "You've made it to the last programming part! We promise this one will be short(ish).\n",
        "\n",
        "In this section, we'll experiment with OpenAI's GPT2 model in a song lyric generation task. Like BERT, GPT2 is a *large language model* (LLM), and has\n",
        "achieved historically stunning results on a wide variety of NLP tasks.\n",
        "As in the last homework, because training GPT2 from scratch is \n",
        "infeasible on your laptop, we will borrow the pretrained GPT2 weights published on [HuggingFace](https://huggingface.co/transformers/model_doc/gpt2.html) and finetune them for our downstream task.\n",
        "\n",
        "You will:\n",
        "1. Pick your favorite artists (either from our provided dataset, or feel free to bring your own!)\n",
        "2. Finetune GPT2 on your artists' lyrics\n",
        "3. Generate novel lyrics\n",
        "\n",
        "As before, *we strongly recommend you use Colab for this assignment*. Unless you're one of the lucky few who've managed to snag an RTX 3080 amidst this silicon shortage (or have a similarly beefy GPU on hand), **you will not be able to run this notebook on your local machine.**\n",
        "\n",
        "**DISCLAIMER**: We offer the dataset of lyrics on an as-is basis for pedagogical purposes. We do not support, condone, or affiliate with any problematic messages potentially embedded in this dataset. To view the original source, see [here](https://www.kaggle.com/deepshah16/song-lyrics-dataset).\n",
        "\n",
        "*A side note*: you've probably heard of [GPT3](https://en.wikipedia.org/wiki/GPT-3), OpenAI's successor to GPT2. So why aren't we using GPT3? For a couple reasons:\n",
        "* The model is ginourmous. It's an order of magnitude *bigger* than GPT2. As you will soon see, Colab's GPUs can barely handle the *smallest* GPT2 model. If we gave you GPT3, we'd need a dedicated datacenter just for this class.\n",
        "* The model is **closed source**. In a rather controversial move, OpenAI awarded Microsoft [exclusive access](https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-model/) to the model. Researchers can still use a public API to query to the model, but only Microsoft has access to the weights directly.\n",
        "\n",
        "In the past, OpenAI has cited social concerns around releasing GPT3 openly to the public, claiming that GPT3-generated text can be difficult to distinguish from human-authored text. Malicious individuals could, for example, use it to generate large quantities of fake news. Hence, even prior to the Microsoft deal, OpenAI has never openly released the training weights.\n",
        "\n",
        "All that's to say, we won't be able to use GPT3 in the assignment. But if you're curious to learn more, do check out [the original paper](https://arxiv.org/abs/2005.14165). Also, if you're a fan of text-based dungeon crawlers, take a look at [AI Dungeon](https://play.aidungeon.io/main/home), which uses GPT3 to drive their\n",
        "\"Dragon AI\" (the free tier uses GPT2).\n",
        "\n",
        "All right, enough chat, let's dive in! (after a brief warning message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXlbi0REBZbk"
      },
      "source": [
        "**WARNING**: GPT2 is *huge*. We will use the smallest model in the GPT2 family, but even so, it will stretch Colab to breaking. More so than in previous homeworks, we will have to pay attention to memory usage. For tips on writing\n",
        "memory-efficient PyTorch code, check out their [FAQ](https://pytorch.org/docs/stable/notes/faq.html) and their blerb on [memory management](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management).\n",
        "\n",
        "Some tools that may come in handy for debugging memory issues:\n",
        "* [torch.cuda.memory_allocated()](https://pytorch.org/docs/stable/generated/torch.cuda.memory_allocated.html#torch.cuda.memory_allocated)\n",
        "* [torch.cuda.max_memory_allocated()](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html)\n",
        "* [gc.collect()](https://docs.python.org/3/library/gc.html)\n",
        "* [torch.cuda.empty_cache()](https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html)\n",
        "\n",
        "You can find the size (in bytes) of any tensor using\n",
        "```python\n",
        "t = make_huge_pytorch_tensor()\n",
        "bytes_per_element = t.element_size()\n",
        "total_elements = t.nelement()\n",
        "\n",
        "total_size = bytes_per_element * total_elements\n",
        "```\n",
        "\n",
        "One final note, whenever a cell fails to execute, temporary variables declared\n",
        "in that cell may not always be deleted by Python. To handle a memory leak, you can try to force a garbage collection cycle and release any cached GPU memory:\n",
        "\n",
        "```python\n",
        "import gc  # Python's garbage collection interface\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "```\n",
        "\n",
        "However, any variables still in reference somewhere in the notebook will not be\n",
        "garbage-collected. The easist way to refresh the entire slate is to just\n",
        "restart your kernel and run everything from the top.\n",
        "\n",
        "**The important takeaway**: If you get an error that looks like\n",
        "\n",
        "```\n",
        "RuntimeError: CUDA out of memory\n",
        "```\n",
        "The easiest (sometimes only) way out of this is to restart the whole notebook and run it from the top. **Simply rerunning your cells without restarting will probably give you another memory error**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MTbztBUKeNh"
      },
      "source": [
        "<div class='header_pink'>\n",
        "\n",
        "## Setting up\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqi5PL4iKh1g"
      },
      "source": [
        "As in the previous homework, let's install tranformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4D8vcsqCZtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0b2b62-d8d8-423a-8f62-79f51db199b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz_lz5YLC97w"
      },
      "source": [
        "PARENT_DIR = '/content/drive/MyDrive/hw4/' # for google colab. adjust accordingly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "N8KSS0KyyCDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e55558e-b5f2-4452-b9e9-d222917c8f33"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqrHjjIUKrsW"
      },
      "source": [
        "Double-check Colab has allocated a GPU for you. If not, remember to select from the top bar `Runtime --> Change runtime type` and set `Hardware Acceleration` to\n",
        "`GPU`. Then run the following cell to print out diagnostic info about your GPU: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i767xxThLG1z",
        "outputId": "5d7a62ba-7b0a-40cc-a341-a0183e9e65bc"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov  3 02:21:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    40W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6gxnsvZLj8y"
      },
      "source": [
        "Here are some suggested imports for you to use. Feel free to add any additional you would like! (as long as they don't trivialize the assignment, of course)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tq8TXyJwrSCf"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2TokenizerFast, GPT2Model, get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neXglk0MLtC8"
      },
      "source": [
        "Because you may need to restart and rerun your notebook many times as you deal with memory issues, you may want to keep your data files on Google drive rather than in Colab's local storage. The local storage is cleared every time Colab allocates a new compute instance for you. To learn more about connecting to Drive, see [this example notebook](https://colab.research.google.com/notebooks/io.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH_sLgSiNJhj"
      },
      "source": [
        "<div class='header_pink'>\n",
        "\n",
        "## Data preparation\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM74jUUiMlEk"
      },
      "source": [
        "<div class='q_pink'><b>2.1 Assemble the data [3 points]</b>\n",
        "    \n",
        "We've pre-processed a lyrics dataset for you. It contains 3 columns: the artist, title, and lyrics. Every artist in the dataset has at least 100 unique songs, and there are 12 unique artists represented. \n",
        "\n",
        "For the sake of training speed, we encourage you to choose 4 artists of the 12 to use. Then, from the data, create a series of prompts for GPT-2 by creating a single string per song. Each string should contain, in the following order:\n",
        "- An `<|endoftext|>` token \n",
        "- The tokens `song artist:`\n",
        "- The name of the artist\n",
        "- The tokens `song title:`\n",
        "- The title of the song\n",
        "- The tokens `song lyrics`:\n",
        "- The lyrics of the song\n",
        "- An `<|endoftext|>` token \n",
        "\n",
        "This is so that we can use this formatting as a prompt for GPT-2 to generate titles and lyrics later, conditioned on artist.\n",
        "\n",
        "Then, split your dataset into a train and validation set. We recommend using 80-90% for training.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "collapsed": true,
        "id": "FmAOAMx7aTl_",
        "outputId": "f1ca8ec4-f5de-4856-acaa-43c852551a94"
      },
      "source": [
        "# Read in, process, and split your data below\n",
        "df = pd.read_csv(f'{PARENT_DIR}combined_lyrics.csv')\n",
        "print(df.shape)\n",
        "sub_df = df[df['Artist'].isin(['Eminem', 'Lady Gaga', 'Beyoncé', 'Maroon 5'])]\n",
        "sub_df.iloc[:100,:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2491, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Artist</th>\n",
              "      <th>Title</th>\n",
              "      <th>Lyric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>Lose Yourself</td>\n",
              "      <td>look if you had one shot or one opportunity to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>The Monster</td>\n",
              "      <td>rihanna i'm friends with the monster that's un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>River</td>\n",
              "      <td>ed sheeran i've been a liar been a thief been ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>Venom</td>\n",
              "      <td>i got a song filled with shit for the strongwi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>Berzerk</td>\n",
              "      <td>eminem now this shit's about to kick off this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>When the Music Stops</td>\n",
              "      <td>bizarre music reality sometimes it's hard to t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>Rabbit Run</td>\n",
              "      <td>verse some days i just wanna up and call it qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>Underground</td>\n",
              "      <td>a lot of people ask me where the fuck i've bee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>Just the Two of Us</td>\n",
              "      <td>produced by bass brothers   baby your dada lov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Eminem</td>\n",
              "      <td>No Apologies</td>\n",
              "      <td>in my mind i'm a fighter my heart's a lighter ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Artist  ...                                              Lyric\n",
              "0   Eminem  ...  look if you had one shot or one opportunity to...\n",
              "1   Eminem  ...  rihanna i'm friends with the monster that's un...\n",
              "2   Eminem  ...  ed sheeran i've been a liar been a thief been ...\n",
              "3   Eminem  ...  i got a song filled with shit for the strongwi...\n",
              "4   Eminem  ...  eminem now this shit's about to kick off this ...\n",
              "..     ...  ...                                                ...\n",
              "95  Eminem  ...  bizarre music reality sometimes it's hard to t...\n",
              "96  Eminem  ...  verse some days i just wanna up and call it qu...\n",
              "97  Eminem  ...  a lot of people ask me where the fuck i've bee...\n",
              "98  Eminem  ...  produced by bass brothers   baby your dada lov...\n",
              "99  Eminem  ...  in my mind i'm a fighter my heart's a lighter ...\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvEQBAGpHsEG",
        "outputId": "6ca7f39a-e765-48eb-9111-3737c8239e8c"
      },
      "source": [
        "EOT_TOKEN = '<|endoftext|>'\n",
        "\n",
        "def string_definer(text):\n",
        "  output_string = EOT_TOKEN\n",
        "  output_string += ' song artist: '\n",
        "  output_string += text['Artist']\n",
        "  output_string += ' song title: '\n",
        "  output_string += text['Title']\n",
        "  output_string += ' song lyrics: '\n",
        "  output_string += text['Lyric']\n",
        "  output_string += ' ' + EOT_TOKEN\n",
        "  return output_string\n",
        "\n",
        "concatenated_text = sub_df.apply(string_definer, axis=1)\n",
        "concatenated_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      <|endoftext|> song artist: Eminem song title: ...\n",
              "1      <|endoftext|> song artist: Eminem song title: ...\n",
              "2      <|endoftext|> song artist: Eminem song title: ...\n",
              "3      <|endoftext|> song artist: Eminem song title: ...\n",
              "4      <|endoftext|> song artist: Eminem song title: ...\n",
              "                             ...                        \n",
              "705    <|endoftext|> song artist: Maroon 5 song title...\n",
              "706    <|endoftext|> song artist: Maroon 5 song title...\n",
              "707    <|endoftext|> song artist: Maroon 5 song title...\n",
              "708    <|endoftext|> song artist: Maroon 5 song title...\n",
              "709    <|endoftext|> song artist: Maroon 5 song title...\n",
              "Length: 710, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9FX3zS8KfQq",
        "outputId": "92562c7b-f108-4480-9362-ba6d1abcd91a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_text, test_text  = train_test_split(\n",
        "    concatenated_text.tolist(), train_size=0.90, test_size=0.10\n",
        ")\n",
        "print(f'Training Size: {len(train_text)}')\n",
        "print(f'Testing Size: {len(test_text)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Size: 639\n",
            "Testing Size: 71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnZ0CLIUNUbW"
      },
      "source": [
        "<div class='q_pink'><b>2.2 Make a SongLyrics Dataset [3 points]</b>\n",
        "\n",
        "Using your split  data, prepare a SongLyrics dataset to use for training.\n",
        "\n",
        "It's up to you how you'd like to structure your dataset, as long as it works with your model and training code further down. Some suggestions:\n",
        "* Use [GPT2TokenizerFast](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2tokenizerfast), a speedier variant of the tokenizer, written in Rust\n",
        "* `GPT2TokenizerFast.from_pretrained('gpt2')` will load the GPT2 (small version) weights from HuggingFace. The [docs for the parent class `PreTrainedTokenizer`](https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html#pretrainedtokenizer) may also be helpful. \n",
        "* By default GPT2 does not include padding tokens. For reasons that will soon be clear, we won't really need them in either in this assignment, but if you'd like, feel free to add your own padding token with `tokenizer.add_special_tokens()`\n",
        "* When tokenizing, ensure that your examples are shorter than the max sequence\n",
        "length accepted by GPT2 (1024 tokens). An easy way to assure this is to set `truncation=True` when calling your tokenizer. See [the docs](https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase) for a full and exhaustive list of all the useful customizations you can try\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fvLesPNiG9vH"
      },
      "source": [
        "class SongLyrics(Dataset):\n",
        "    def __init__(self, data: List[str]):\n",
        "        #raise NotImplementedError   # TODO: implement        pass\n",
        "        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "        self.tokenized_strings = self.tokenizer(data, truncation=True)['input_ids']\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        #raise NotImplementedError   # TODO: implement        pass\n",
        "        return len(self.tokenized_strings)\n",
        "    def __getitem__(self, idx: int):\n",
        "        #raise NotImplementedError   # TODO: implement        pass\n",
        "        return torch.tensor(self.tokenized_strings[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1epEi3D8hYM"
      },
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98g8MGgb8yxp",
        "outputId": "4b4b53d9-248c-41c6-f844-ac8562398be4"
      },
      "source": [
        "tokenizer.vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUxrSdCoPrco"
      },
      "source": [
        "train_ds = SongLyrics(train_text)\n",
        "test_ds = SongLyrics(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRXOkvmRP5HA",
        "outputId": "36c96e64-b1ba-4d1b-b0ee-0f8feecaf142"
      },
      "source": [
        "print(len(train_ds))\n",
        "print(len(test_ds))\n",
        "print(train_ds[0])\n",
        "print(test_ds[0])\n",
        "print(train_text[0])\n",
        "print(test_text[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "639\n",
            "71\n",
            "tensor([50256,  3496,  6802,    25,  1526,  2049,   642,  3496,  3670,    25,\n",
            "        40011,    72,  3496, 15844,    25,   345,   260,   884,   257, 44109,\n",
            "         1312,   760,   345,  5938,   290,   523,   466,  1312,  1312, 48004,\n",
            "         1096,  1312,   766,   345,   503,   345,  1239, 19951,   257,  5273,\n",
            "          326,   356,  1239,  4888,   475,   663,   523,  6283,   663,  1223,\n",
            "          649,  4998,  7666,   326,  1312,   423,   329,   345,  1312,  1969,\n",
            "          616,  2951,   618,   545,  3436,  4240,   644,   340,    67,   307,\n",
            "          588,   284,   787,   345, 44567,   220,   662,  1312, 18869,  1577,\n",
            "          345,  1223,  1365,   621,  1997,   345,   303,  1683,   550,   257,\n",
            "         7387,   290,   257,  5443, 18854,   262,   995,   340, 27934,   523,\n",
            "         3049,   220,   220,  6029,   479, 14246,    72,   534, 35919, 37472,\n",
            "          866,   616, 22531,   523,  3387,  1309,   502, 17666,  2245,   340,\n",
            "          878,   340,  6140,   220,   220,   523,  1577,   340,   510,   290,\n",
            "        17666, 16614,   290,  4104,   534,  5101,   290,  7405,  1973,   262,\n",
            "         3996,   290,   618,   345, 13279,   345, 28329, 13721,   262,  1243,\n",
            "         1312, 31992,   287,   534,  1027,   644,  1312,   531,   220,   662,\n",
            "         1312, 18869,  1577,   345,  1223,  1365,   345,   266,  1236,  1577,\n",
            "          502,  1223,  1365,   621,  1997,   345,   303,  1683,   550,   621,\n",
            "         1997,   220,   425,  1683,   550,   257,  7387,   290,   257,  5443,\n",
            "        18854,   257,  7387,   290,   257,  5443, 18854,   262,   995,   340,\n",
            "        27934,   523,  3049,   340, 27934,   523,  3049,   220,   220,  6029,\n",
            "          479, 14246,    72,   534, 35919, 37472,   866,   616, 22531,   523,\n",
            "         3387,  1309,   502, 17666,  2245,   340,   878,   340,  6140,   220,\n",
            "          220,  1312, 18548,  4043,   284,  1011,   345,  1363,  9353,   832,\n",
            "          534,  4190, 36544,   319,   534,   736, 12692,   502,   351,   534,\n",
            "        23361,  3613,   502,   422,  3589,   905,   502,   703,   284,  1337,\n",
            "          651,  2279,   503, 37472,  8347, 41429,   895,  3380,   477,   625,\n",
            "          534,  1986,   703,   881,  2392,  1276,   356,  4043, 17666,   892,\n",
            "          326,  1312,   460,  4043,   220,   220,  6029,   479, 14246,    72,\n",
            "          534, 35919, 37472,   866,   616, 22531,   523,  3387,  1309,   502,\n",
            "        17666,  2245,   340,   878,   340,  6140, 17207, 10194,   220,   220,\n",
            "         6029,   479, 14246,    72,   765,   284,  1577,   345,  1223,  1365,\n",
            "          621,   428,   534, 35919, 37472,   866,   616, 22531, 17207, 10194,\n",
            "          523,  3387,  1309,   502,   765,   284,  1577,   345,  1223,  1365,\n",
            "          621,   428, 17666,  2245,   340,   878,   340,  6140,   220,   220,\n",
            "        17666,  2245,   340,   878,   340,  6140, 17666,  2245,   340,   878,\n",
            "          340,  6140, 17666,  2245,   340,   878,   340,  6140, 17666,  2245,\n",
            "          340,   878,   340,  6140,   220, 50256])\n",
            "tensor([50256,  3496,  6802,    25, 11182, 42192,  3496,  3670,    25,  7772,\n",
            "        36555,   357,    34,  6391, 13333,   367,    16,    45,    16, 13268,\n",
            "            8,  3496, 15844,    25,  2513,  2513,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,  2513,  2513,  7506,  5156,   670,   340,\n",
            "         1312,  1101,   257,  1479, 21551,   220,  8011,  1479, 21551,  2124,\n",
            "           24,  1312,  1101,   257,  1479, 21551,  1479,  2124,    20,  1312,\n",
            "         1101,   257,  1479,  2124,    22, 21551,  1479,  2124,    20,   220,\n",
            "         1312,  1101,   257,  1479, 21551,  5156,   220,  1281,   374,  3301,\n",
            "        36225,   993,  9267,   283,   296,  1689,   308,  8126, 11752,   300,\n",
            "         6081,   765,   534,  2089, 19661,   374,  3301, 36225,   993,  9267,\n",
            "          283,   296,  1689,   308,  8126, 11752,   300,  6081,   765,   534,\n",
            "         2089, 19661,   220,   220,  1312,   765,   534, 13400,  1312,   765,\n",
            "          534,  4369,  1312,   765,   534,  2279,   355,   890,   355,   663,\n",
            "         1479,  1312,   765,   534,  1842,  1842,  1842,  1842,  1312,   765,\n",
            "          534,  1842,  1312,   765,   534, 10512,   262,  3638,   286,   534,\n",
            "         1021,  1312,   765,   534, 11620,   301,  4185,   276,  9245,   287,\n",
            "          262,  6450,  1312,   765,   534,  1842,  1842,  1842,  1842,  1312,\n",
            "          765,   534,  1842,  1842,  1842,  1842,  1312,   765,   534,  1842,\n",
            "          220,   662,   345,   760,   326,  1312,   765,   345,   290,   345,\n",
            "          760,   326,  1312,   761,   345,  1312,   765,   340,  2089,   534,\n",
            "         2089, 19661,   220,   220,  1312,   765,   534,  1842,   290,  1312,\n",
            "          765,   534, 15827,   345,   290,   502,   714,  3551,   257,  2089,\n",
            "        19661, 11752,  1219,  1219,  1219,  1219,  1219,  1312,   765,   534,\n",
            "         1842,   290,   477,   534, 18854,   338, 15827,   345,   290,   502,\n",
            "          714,  3551,   257,  2089, 19661, 11752,  1219,  1219,  1219,  1219,\n",
            "         1219,  1219,  1219,  1219,  1219,  1219,  1219,  4978,   287,   257,\n",
            "         2089, 19661, 11752,  1219,  1219,  1219,  1219,  1219,  1219,  1219,\n",
            "         1219,  1219,  1219,  1219,  4978,   287,   257,  2089, 19661,   220,\n",
            "         8011,  1312,  1101,   257,  1479, 21551,  1479,  2124,    20,  2089,\n",
            "        19661,  1479,  2124,    22, 21551,  1479,  2124,    20,  1312,  1101,\n",
            "          257,  1479, 21551,  5156,   220,  1281, 25480,   374,  3301, 36225,\n",
            "          993,  9267,   283,   296,  1689,   308,  8126, 11752,   300,  6081,\n",
            "          765,   534,  2089, 19661,   374,  3301, 36225,   993,  9267,   283,\n",
            "          296,  1689,   308,  8126, 11752,   300,  6081,   765,   534,  2089,\n",
            "        19661,   220,   220,  1312,   765,   534,  9961,  1312,   765,   534,\n",
            "         1486,   705, 25587,   345,   260,   257,  4301,   355,   890,   355,\n",
            "          345,   260,  6164,  1312,   765,   534,  1842,  1842,  1842,  1842,\n",
            "         1312,   765,   534,  1842,  1312,   765,   534, 30731,   534,  9421,\n",
            "        14031,   427, 42298,   765,   345,   287,   616,  8286,  4324,  5156,\n",
            "          340,   338,  6639,  1312,   765,   534,  1842,  1842,  1842,  1842,\n",
            "         1312,   765,   534,  1842,   220,   662,   345,   760,   326,  1312,\n",
            "          765,   345,   290,   345,   760,   326,  1312,   761,   345,  2728,\n",
            "         1312,  1101,   257,  1479, 21551,  5156,  1312,   765,   340,  2089,\n",
            "          534,  2089, 19661,   220,   220,  1312,   765,   534,  1842,   290,\n",
            "         1312,   765,   534, 15827,   345,   290,   502,   714,  3551,   257,\n",
            "         2089, 19661, 11752,  1219,  1219,  1219,  1219,  1219,  1312,   765,\n",
            "          534,  1842,   290,   477,   534, 18854,   338, 15827,   345,   290,\n",
            "          502,   714,  3551,   257,  2089, 19661, 11752,  1219,  1219,  1219,\n",
            "         1219,  1219,  1219,  1219,  1219,  1219,  1219,  1219,  4978,   287,\n",
            "          257,  2089, 19661, 11752,  1219,  1219,  1219,  1219,  1219,  1219,\n",
            "         1219,  1219,  1219,  1219,  1219,  4978,   287,   257,  2089, 19661,\n",
            "          220,  8011,  1312,  1101,   257,  1479, 21551,  1479,  2124,    20,\n",
            "         2089, 19661,  1479,  2124,    22, 21551,  1479,  2124,    20,  1312,\n",
            "         1101,   257,  1479, 21551,  5156,   220,  1281,   374,  3301, 36225,\n",
            "          993,  9267,   283,   296,  1689,   308,  8126, 11752,   300,  6081,\n",
            "          765,   534,  2089, 19661,   374,  3301, 36225,   993,  9267,   283,\n",
            "          296,  1689,   308,  8126, 11752,   300,  6081,   765,   534,  2089,\n",
            "        19661,   220,   220,  2513,  2513,  6977,  5156,   670,   340,  1445,\n",
            "          326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,  1445,\n",
            "          326, 21551,  7165,  2513,  2513,  6977,  5156,   670,   340,  1445,\n",
            "          326, 21551,  7165,  2513,  2513,   220,  6977,  5156,   670,   340,\n",
            "         1445,   326, 21551,  7165,   308,  8126,   308,  8126,   308,  8126,\n",
            "          308,  8126,  1312,  1101,   257,  1479, 21551,  5156,   220, 14608,\n",
            "         1312,   765,   534,  1842,   290,  1312,   765,   534, 15827,  1312,\n",
            "          765,   534,  1842,  1312,   836,   470, 18869,   307,  2460, 11223,\n",
            "         1569,  2821,  5680,   716,   454,  2123, 11223,  1569,  2821, 20486,\n",
            "         2710,  6362, 11223,  1569,  2821,  5680,   716,   454,  1312,   836,\n",
            "          470, 18869,   307,  2460,   220,   220,  1312,   765,   534,  1842,\n",
            "          290,  1312,   765,   534, 15827,   345,   290,   502,   714,  3551,\n",
            "          257,  2089, 19661,  1312,   765,   534,  1842,   290,   477,   534,\n",
            "        18854,   338, 15827,   345,   290,   502,   714,  3551,   257,  2089,\n",
            "        19661, 11752,  1219,  1219,  1219,  1219,  1219,  1219,  1219,  1219,\n",
            "         1219,  1219,  1219,   765,   534,  2089, 19661,  4978,   287,   257,\n",
            "         2089, 19661,   765,   534,  2089, 19661, 11752,  1219,  1219,  1219,\n",
            "         1219,  1219,  1219,  1219,  1219,  1219,  1219,  1219,   765,   534,\n",
            "         2089, 19661,  4978,   287,   257,  2089, 19661,   220,   220,  1312,\n",
            "         1101,   257,  1479, 21551,  2089, 19661,  2089,   305,  2089, 19661,\n",
            "         2089,   305,  2089, 19661,  2089,   305,   220, 50256])\n",
            "<|endoftext|> song artist: Maroon 5 song title: Kiwi song lyrics: youre such a flirt i know you hurt and so do i i empathize i see you out you never cared a conversation that we never shared but its so strange its something new amazing feelings that i have for you i close my eyes when im alone wonder what itd be like to make you moan  pre i wanna give you something better than anything youve ever had a stronger and a faster lover the world it disappears so fast   sweet kiwi your juices dripping down my chin so please let me dont stop it before it begins   so give it up and dont pretend and spread your arms and legs across the bed and when you shake you wont regret the things i whisper in your ear what i said  pre i wanna give you something better you wann give me something better than anything youve ever had than anything ive ever had a stronger and a faster lover a stronger and a faster lover the world it disappears so fast it disappears so fast   sweet kiwi your juices dripping down my chin so please let me dont stop it before it begins   i cant wait to take you home fingers through your hair kisses on your back scratch me with your nails save me from myself show me how to care get everything out dripping everywhere lipstick smeared all over your face how much longer must we wait dont think that i can wait   sweet kiwi your juices dripping down my chin so please let me dont stop it before it begins hey yeah   sweet kiwi want to give you something better than this your juices dripping down my chin hey yeah so please let me want to give you something better than this dont stop it before it begins   dont stop it before it begins dont stop it before it begins dont stop it before it begins dont stop it before it begins <|endoftext|>\n",
            "<|endoftext|> song artist: Lady Gaga song title: Bad Romance (Chew Fu H1N1 Fix) song lyrics: walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i want your love love love love i want your love i want your psycho your vertigo shtick want you in my rear window baby it's sick i want your love love love love i want your love  pre you know that i want you and you know that i need you cause i'm a free bitch baby i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk  fashion baby work it move that bitch crazy gaga gaga gaga gaga i'm a free bitch baby  breakdown i want your love and i want your revenge i want your love i don't wanna be friends je veux ton amour et je veux ta revanche je veux ton amour i don't wanna be friends   i want your love and i want your revenge you and me could write a bad romance i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh want your bad romance caught in a bad romance want your bad romance ohohohohohohohohohohohoh want your bad romance caught in a bad romance   i'm a free bitch bad romance badro bad romance badro bad romance badro <|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jL35FzxP6iV"
      },
      "source": [
        "<div class='q_pink'><b>2.3 Dataloaders [2 points]</b>\n",
        "\n",
        "Make new dataloaders out of your dataset. For the dataloader, you may find the option `pin_memory=True` to be helpful, as it caches tensors in GPU memory for quicker access. See [this thread](https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723) for details.\n",
        "\n",
        "**VERY IMPORTANT**: On both of your dataloaders, set `batch_size=1`. Any larger of a batch_size and **you will run into memory issues**.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "82mqff4hHj_7"
      },
      "source": [
        "train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, drop_last=True, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=1, shuffle=False, drop_last=True, pin_memory=True)\n",
        "#raise NotImplementedError   # TODO: implement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtawwd_gQeYV"
      },
      "source": [
        "<div class='header_pink'>\n",
        "\n",
        "## Model and training\n",
        "\n",
        "</div>\n",
        "\n",
        "To access the GPT2 weights, we will use HuggingFace's GPT2Model class. Please take a moment to familiarize yourself with the class's [API](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2model), especially the `forward()` method and its return type.\n",
        "\n",
        "We can instantiate a GPT2Model like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jeyCGXkOzQaV"
      },
      "source": [
        "model = GPT2Model.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYekuVZazbiH"
      },
      "source": [
        "This gives you a PyTorch module that you can use just like any other. To feed an input through the model, simply do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RwOjx9x7zlQv"
      },
      "source": [
        "example = next(iter(train_dl))\n",
        "output = model(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYfRwXNW0YW7"
      },
      "source": [
        "We can inspect the shape of our output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6bhbA_LNoNmd",
        "outputId": "368e0f2d-4d1c-48fd-c834-d4429329d470"
      },
      "source": [
        "print(example.shape)\n",
        "print(output[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 723])\n",
            "torch.Size([1, 723, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EbQlIOEE1Nw",
        "outputId": "4679527a-6cc5-4eff-97a3-a4b5311813d0"
      },
      "source": [
        "flatten_output = torch.squeeze(output[0])#output[0].view(output[0].shape[1], output[0].shape[2])#torch.flatten(output[0],2)\n",
        "flatten_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([723, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcSkbUAh2VGv",
        "outputId": "d9bdf436-82f6-49a8-f82c-72b9dbd799d6"
      },
      "source": [
        "example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[50256,  3496,  6802,    25, 11182, 42192,  3496,  3670,    25, 45328,\n",
              "         12860,  3496, 15844,    25,   705, 25587,  1312,  9149,   345,  1053,\n",
              "          1239,  1775,   257,   905,   588,   428,   878,  8066,   905,   345,\n",
              "          1223,   326,   345,   460,   470,  8856, 18869,  1011,   257, 27185,\n",
              "           788,  1280,   510,   262, 41160,   705, 25587,  1312,  9149,   345,\n",
              "          1053,  1239,  1775,   257,   905,   588,   428,   878,  8066,   905,\n",
              "           345,  1223,   326,   345,   460,   470,  8856, 18869,  1011,   257,\n",
              "         27185,   788,  1280,   510,   262, 41160,  5156,   340,   338,   257,\n",
              "         22601, 12860,   220,   220,  1312, 18869,   760,   644,   466,   345,\n",
              "           588,   644,   561,   345,   910,   611,  1312,   531,  1312,  1549,\n",
              "           466,  1997,   284,   345,   345,   765,  1312, 18869,   760, 10194,\n",
              "           340,   338, 23036,   649,  6116,   731,   345,   561,   588,   345,\n",
              "          1053,   587,  8196,   259,     6,  2245,  3763,  3387,  3032,  5156,\n",
              "           345,  1053,  1392, 15910,   287,   616,  1182, 27627,  6386, 17943,\n",
              "           345,  1053,  1683,   550,  1312,  2982,   644,   345,   531,   287,\n",
              "           534,  3993,   345, 18869,  5806,   345, 18869, 47618,  1312,   588,\n",
              "           340,  5210,  1312,   588,   340,  3049,  1312,   760,   326,  2769,\n",
              "           866,   345,   821,   257, 22601,   220,   662,   655,   910,   262,\n",
              "          5536,  2456,   290,  1312,  1183,  1282,  1312,  1183, 36580,  1096,\n",
              "           345,   345,   821,  8066,  4425,   534,  2000,   836,   470,   345,\n",
              "           760,  5156,   340,   338,   257, 22601, 12860,   220,   220,   705,\n",
              "         25587,  1312,  9149,   345,  1053,  1239,  1775,   257,   905,   588,\n",
              "           428,   878,  8066,   905,   345,  1223,   326,   345,   460,   470,\n",
              "          8856, 18869,  1011,   257, 27185,   788,  1280,   510,   262, 41160,\n",
              "          5156,   340,   338,   257, 22601, 12860,   705, 25587,  1312,  9149,\n",
              "           345,  1053,  1239,  1775,   257,   905,   588,   428,   878,  8066,\n",
              "           905,   345,  1223,   326,   345,   460,   470,  8856,   705, 25587,\n",
              "          1312,  1101,  1654,   345,   821, 10032,   286,   852,  1728,  5156,\n",
              "           340,   338,   257, 22601, 12860,  5156,   340,   338,   257, 22601,\n",
              "         12860,   220,   220,   618,   345,   821,  3436,   644,   466,   345,\n",
              "          4320,  1312, 18869,   760,   534,  8842,   836,   470,  2300,   703,\n",
              "          2642,   340,  1244,   307,  1312, 18869,   766,   760,   644,  1312,\n",
              "          1612, 30731,   290,  1257, 17144,   290,  1612,   340,   338,   655,\n",
              "          1180,   618,   345,   821,   351,   502,  1312,   765,   340,   477,\n",
              "          1312,  1392,   257,  4822,  1234,   340,   319,   345,   290,  1312,\n",
              "          1839,   470,  1560,   534,  3200,   705, 25587,  1312,   716,   534,\n",
              "         11778,   345,   765,   340,  1327,   345,   765,   340,  5802,  1312,\n",
              "           588,   340,   890,  1239,  1576,  1312,  6991,   326,  1312,  1839,\n",
              "           470, 17438,   220,   662,   655,   910,   262,  5536,  2456,   290,\n",
              "          1312,  1183,  1282,  1312,  1183, 36580,  1096,   345,   345,   821,\n",
              "          8066,  4425,   534,  2000,   836,   470,   345,   760,  5156,   340,\n",
              "           338,   257, 22601, 12860,   220,   220,   705, 25587,  1312,  9149,\n",
              "           345,  1053,  1239,  1775,   257,   905,   588,   428,   878,  8066,\n",
              "           905,   345,  1223,   326,   345,   460,   470,  8856, 18869,  1011,\n",
              "           257, 27185,   788,  1280,   510,   262, 41160,  5156,   340,   338,\n",
              "           257, 22601, 12860,   705, 25587,  1312,  9149,   345,  1053,  1239,\n",
              "          1775,   257,   905,   588,   428,   878,  8066,   905,   345,  1223,\n",
              "           326,   345,   460,   470,  8856,   705, 25587,  1312,  1101,  1654,\n",
              "           345,   821, 10032,   286,   852,  1728,  5156,   340,   338,   257,\n",
              "         22601, 12860,  5156,   340,   338,   257, 22601, 12860,   220,   220,\n",
              "           308,   320,  1326,   308,   320,  1326,   326,   308,   320,  1326,\n",
              "           308,   320,  1326,   326, 11752,  1577,   345,  1577,   345,   326,\n",
              "          2030, 15492,  2030, 15492, 22601, 12860,   588,   340,   588,   326,\n",
              "           479,  9760,   479,  9760,  3797, 11752,  5156,   340,   338,   257,\n",
              "         22601, 12860,   308,   320,  1326,   308,   320,  1326,   326,   308,\n",
              "           320,  1326,   308,   320,  1326,   326, 11752,  1577,   345,  1577,\n",
              "           345,   326,  2030, 15492,  2030, 15492, 22601, 12860,   588,   340,\n",
              "           588,   326,   479,  9760,   479,  9760,  3797, 11752,  5156,   340,\n",
              "           338,   257, 22601, 12860,   220,   220,   705, 25587,  1312,  9149,\n",
              "           345,  1053,  1239,  1775,   257,   905,   588,   428,   878,  8066,\n",
              "           905,   345,  1223,   326,   345,   460,   470,  8856, 18869,  1011,\n",
              "           257, 27185,   788,  1280,   510,   262, 41160,  5156,   340,   338,\n",
              "           257, 22601, 12860,   705, 25587,  1312,  9149,   345,  1053,  1239,\n",
              "          1775,   257,   905,   588,   428,   878, 18869,   905,   345,  1223,\n",
              "           326,   345,   460,   470,  8856, 18869,  1011,   257, 27185,   788,\n",
              "          1280,   510,   262, 41160,  5156,   340,   338,   257, 22601, 12860,\n",
              "           705, 25587,  1312,  9149,   345,  1053,  1239,  1775,   257,   905,\n",
              "           588,   428,   878,  8066,   905,   345,  1223,   326,   345,   460,\n",
              "           470,  8856,   705, 25587,  1312,  1101,  1654,   345,   821, 10032,\n",
              "           286,   852,  1728,   220,   220,  5156,   340,   338,   257, 22601,\n",
              "         12860,  5156,   340,   338,   257, 22601, 12860,   905,   905,   905,\n",
              "           905,   905,   905,   905,   905,  5156,   340,   338,   257, 22601,\n",
              "         12860,   220, 50256]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beAKdmsi0bjG"
      },
      "source": [
        "What do you think each of those dimensions corresponds to? (a rhetorical question, no need to type an answer here! But please do think about this before reading on)\n",
        "\n",
        "```\n",
        " _________________________________________\n",
        "( Hmmm... what do those dimensions means? )\n",
        " -----------------------------------------\n",
        "        o   ^__^\n",
        "         o  (oo)\\_______\n",
        "            (__)\\       )\\/\\\n",
        "                ||----w |\n",
        "                ||     ||\n",
        "```\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "...\n",
        "\n",
        "All right so the dimensions are\n",
        "\n",
        "$$ B \\times N \\times H$$\n",
        "where:\n",
        "* $B$ is batch size\n",
        "* $N$ is sequence length\n",
        "* $H$ is hidden size\n",
        "\n",
        "As the documentation indicates, `last_hidden_state` corresponds to the output of the final decoder block in GPT2, constructing an embedded representation of each of the $N$ inputs. Each embedded representation is a vector of length $H$. The (small) GPT2 model uses $H = 768$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-7_3CO426Z5"
      },
      "source": [
        "\n",
        "\n",
        "<div class='q_pink'><b>2.4 GPT2 Language Model [16 points]</b>\n",
        "\n",
        "Below, you will find skeleton code outlining the shape of a GPT2 language model. Using the base GPT2Model class, we will construct on top a language model head that predicts the next word after the current input. For example, suppose our sentence is:\n",
        "\n",
        "```\n",
        "The quick brown fox jumps over the lazy\n",
        "```\n",
        "\n",
        "From this sentence, the GPT2Model constructs a sequence of 8 embeddings\n",
        "\n",
        "$$ h_1, h_2, ..., h_8$$\n",
        "where\n",
        "\n",
        "$$ h_1  = f(The)$$\n",
        "$$ h_2 = f(The\\; quick)$$\n",
        "$$ h_3 = f(The\\; quick\\; brown)$$\n",
        "\n",
        "That's to say, the $i$th embedding is constructed as a function over the previous $i$ inputs. Then given the embedding $h_i$, we'd like to predict the $i+1$st word. So our end output should be 8 words\n",
        "\n",
        "$$ w_1, w_2, w_3, ..., w_8$$\n",
        "\n",
        "where\n",
        "\n",
        "$$ w_1 = g(h_1)$$\n",
        "$$ w_2 = g(h_2)$$\n",
        "$$ w_3 = g(h_3)$$\n",
        "\n",
        "And ideally, after training we should have that \n",
        "\n",
        "$$w_1 \\approx quick$$\n",
        "$$w_2 \\approx brown$$\n",
        "$$w_3 \\approx fox$$\n",
        "\n",
        "All the way until $w_8 \\approx dog$ (or whatever mammal the fox was feeling like jumping over today).\n",
        "\n",
        "So to recap, we have the weights from the base model GPT2Model, which produces a hidden vector for each token in the input sequence. Our goal is to use GPT2Model to build a language model that achieves the above prediction task. The only component missing is some way to transform the GPT2 embeddings into predicted words, then use a loss function to compare the predictions to the actual next words. As in HW2, pay close attention to the alignments between input words and predicted words, so as to ensure that your model isn't simply predicting the next word.\n",
        "\n",
        "**CAUTION:** While you have the freedom to design this as you'd like, take the easy route and be frugal where you can! Your sequence length can be up to 1024 tokens, and the vocabulary size is over 50,000. Even though we're using a batch size of only 1, keep in mind the sheer size (in MB) of the tensors at play, not to mention the gradients PyTorch is saving along the way!\n",
        "   \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "72A542YbucPV"
      },
      "source": [
        "\n",
        "def flatten_tensors(gpt_output: torch.tensor) -> torch.tensor:\n",
        "    return gpt_output.view(gpt_output[1], gpt_output[2])\n",
        "\n",
        "class GPT2LanguageModel(torch.nn.Module):\n",
        "    def __init__(self, output_vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = 768 # parameters fixed by GPT2\n",
        "        self.model = GPT2Model.from_pretrained('gpt2')\n",
        "        self.linear = torch.nn.Linear(self.hidden_size, 512) #output_vocab_size)\n",
        "        self.dropout = torch.nn.Dropout(p=0.2)\n",
        "        self.linear2 = torch.nn.Linear(512, output_vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, input_ids):\n",
        "        gpt_result = self.model(input_ids)\n",
        "        output = self.linear(gpt_result[0])\n",
        "        output = self.dropout(output)\n",
        "        output = self.linear2(output)\n",
        "        return output\n",
        "\n",
        "def compute_loss(preds, target_ids):\n",
        "    preds = preds[:,:-1,:]\n",
        "    target_ids = target_ids[:, 1:]\n",
        "    preds_squeeze = torch.squeeze(preds,0)\n",
        "    target_ids_squeeze = torch.squeeze(target_ids,0)\n",
        "    return torch.nn.CrossEntropyLoss()(preds_squeeze, target_ids_squeeze)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocp_-zty9HLG"
      },
      "source": [
        "<div class='q_pink'><b>2.5 Train the model [10 points]</b>\n",
        "\n",
        "It's time to train the beast! **Train your model below, printing out train and validation loss at regular intervals.**\n",
        "\n",
        "We (and the [Huggingface team](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py)) recommend using `Adam` or `AdamW` as an optimizer with a learning rate of `5e-5`, no weight decay, and default beta and epsilon parameters (`betas = [0.9, 0.999]`, `eps = 1e-8`). You may need to tweak these parameters slightly to achieve good model performance. \n",
        "\n",
        "You may want to consider using a [linear scheduler](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup) for the learning rate. By default, Huggingface uses a linear scheduler with no warmup and which takes a scheduler step for every optimizer step (instead of every epoch).\n",
        "\n",
        "Finally, since we are forced to use a batch size of 1 due to memory constraints, you may also want to consider using [gradient accumulation](http://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html). In PyTorch, this just means doing multiple `loss.backward()` across batches before calling `optimizer.step()` and `optimizer.zero_grad()`.\n",
        "\n",
        "If everything goes to plan, you should see times of around 5 minutes per epoch, depending on the size of your datasets. Training for 5 - 10 epochs is plenty (you could probably get away with even fewer).\n",
        "\n",
        "To take advantage of GPU acceleration, as in the previous homework, remember to move your model to the GPU:\n",
        "\n",
        "```\n",
        "model.cuda()\n",
        "```\n",
        "\n",
        "as well as your training examples as you receive them:\n",
        "\n",
        "```\n",
        "example_tensor = example_tensor.cuda()\n",
        "```\n",
        "\n",
        "**MEGA CAUTION**: If you haven't read any of our previous warnings yet, read this one!\n",
        "\n",
        "When you move your model and examples to the GPU, you will start dipping into the very finite pool of VRAM (video RAM) available on Colab's cut-rate GPUs. If you've followed our previous advice (batch_size=1, frugal model), you should be fine. However, the moment you run this cell, **your model and examples will stay in GPU memory**. Worse, if your cell fails with an error or you halt execution prematurely, your cell may leak memory.\n",
        "\n",
        "For example, Colab has probably allocated for you a Tesla K80 GPU with 12 GB of VRAM. For us, running the training cell will take almost 7GB of VRAM. Once this cell finishes (or throws an error), that 7GB of VRAM continues hanging around. That means the next time we run this cell, if that memory hasn't been released (and it probably hasn't), PyTorch will try to allocate *another 7GB*, but because there isn't another 7GB free, it will return a `CUDA out of memory` error. To manually free that memory, you can try some of our tips from above (manual garbage collection, deleting unused variables, emptying the cache), but it probably won't get everything. \n",
        "\n",
        "**If all else fails, the easiest (sometimes only) way to empty your memory is to restart your kernel and rerun the notebook from the start.**\n",
        "\n",
        "To do so, on the top bar select `Runtime --> Restart and run all`.\n",
        "    \n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qQ5N0cfW9JC1"
      },
      "source": [
        "# implement your training loop here\n",
        "def train(vocab_size: int, train_dl: DataLoader, val_dl: DataLoader):\n",
        "    model = None\n",
        "    losses = {\n",
        "        'train': [], # keep track of your losses in these lists\n",
        "        'val': []\n",
        "    }\n",
        "    \n",
        "    model = GPT2LanguageModel(vocab_size)# Seq2Seq(train_dl.dataset.source_vocab_size, train_dl.dataset.target_vocab_size, 512, PADDING_IDX)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    #loss_function = nn.CrossEntropyLoss() #NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)#, weight_decay=1e-5)\n",
        "    #scheduler = get_linear_schedule_with_warmup(optimizer, 2000, 10) #worked well 200 and 1200\n",
        "\n",
        "    epochs = 6\n",
        "    for epoch in range(epochs):\n",
        "        train_running_loss = 0.0\n",
        "        for i, train_batch in enumerate(train_dl):\n",
        "            train_batch = train_batch.to(device)\n",
        "\n",
        "            train_preds = model(train_batch)\n",
        "            train_targets = train_batch\n",
        "\n",
        "            train_loss = compute_loss(train_preds, train_targets)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_running_loss += train_loss.item()\n",
        "        \n",
        "        #scheduler.step()\n",
        "        \n",
        "        print('Epoch %d, training_loss %.3f'% \n",
        "            (epoch + 1, train_running_loss / len(train_dl))) \n",
        "        \n",
        "        losses['train'].append(train_running_loss/len(train_dl))\n",
        "        \n",
        "        val_running_loss = 0.0\n",
        "        for i, val_batch in enumerate(val_dl):\n",
        "            with torch.no_grad():\n",
        "                val_batch = val_batch.to(device)\n",
        "\n",
        "                val_preds = model(val_batch)\n",
        "                val_targets = val_batch\n",
        "\n",
        "                val_loss = compute_loss(val_preds, val_targets)\n",
        "\n",
        "                val_running_loss += val_loss.item() \n",
        "\n",
        "        print('Epoch %d, validation_loss %.3f'% \n",
        "            (epoch + 1, val_running_loss / len(val_dl))) \n",
        "        \n",
        "        losses['val'].append(val_running_loss/len(val_dl))\n",
        "        \n",
        "    print(\"Finished training!\")\n",
        "    \n",
        "    return model, losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQfo3nB43uZK",
        "outputId": "cdd29c82-1f15-4303-9275-fc0c950dd444"
      },
      "source": [
        "model, losses = train(tokenizer.vocab_size, train_dl, test_dl)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, training_loss 6.208\n",
            "Epoch 1, validation_loss 4.719\n",
            "Epoch 2, training_loss 4.325\n",
            "Epoch 2, validation_loss 4.098\n",
            "Epoch 3, training_loss 3.741\n",
            "Epoch 3, validation_loss 3.812\n",
            "Epoch 4, training_loss 3.309\n",
            "Epoch 4, validation_loss 3.614\n",
            "Epoch 5, training_loss 2.937\n",
            "Epoch 5, validation_loss 3.529\n",
            "Epoch 6, training_loss 2.576\n",
            "Epoch 6, validation_loss 3.486\n",
            "Finished training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB6SquKYCzgU"
      },
      "source": [
        "<div class='header_pink'>\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2iJwgeH26Z6"
      },
      "source": [
        "\n",
        "<div class='q_pink'><b>2.6 Generate text [6 points]</b>\n",
        "    \n",
        "Now the fun part, playing with our trained model! To start, fill out the method below that generates a novel sequence of text. Given some primer text, you should use GPT2 to continually generate the next token until it hits either the max sequence length or the end of text token `<|endoftext|>`.\n",
        "\n",
        "Some hints:\n",
        "* Observe that with a single invocation, GPT2 will only predict one additional new token. The other $N - 1$ predictions are for tokens in your input sequence. Thus, you will have to call GPT2 many times, feeding back the primer plus the new predicted word, to produce a sequence of novel words.\n",
        "* To generate different examples with each call, rather than calling `argmax` over your word probabilities, why not use them as sampling probabilities instead? That is, if your vocabulary is `[ball, cow, planet, grass]` and your probabilities for the next word are `[0.1, 0.5, 0.2, 0.2]`, then with probability $0.5$ the next word should be `cow`.\n",
        "* To convert an index back into the original token, use HuggingFace's `tokenizer.decode()` method. Check out the [API](https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase) for details.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "CBTzDus326Z6"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(primer: str, max_length=50):\n",
        "    tokenized_primer = torch.tensor(tokenizer(primer, truncation=True)['input_ids'])\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenized_primer = tokenized_primer.to(device)\n",
        "    next_word_preds = model(tokenized_primer)[-1,:]\n",
        "\n",
        "    # Temperature sampling\n",
        "    temp = 2\n",
        "    next_word_preds = F.softmax(next_word_preds/temp, dim=0)\n",
        "    # Top k \n",
        "    k = 15\n",
        "    sorted_vals, _ = torch.sort(next_word_preds)#[-20]\n",
        "    top_k_cut = sorted_vals[-20]\n",
        "    zero_tensor = torch.zeros(next_word_preds.shape[0]).to(device)\n",
        "    next_word_preds_k = torch.where(next_word_preds > top_k_cut, next_word_preds, zero_tensor)\n",
        "\n",
        "    next_token = torch.multinomial(next_word_preds_k, 1).item()\n",
        "    next_word = tokenizer.decode(next_token)\n",
        "\n",
        "    j = len(primer.split(' '))\n",
        "    \n",
        "    del tokenized_primer\n",
        "\n",
        "    #print(\"This is j \", j)\n",
        "    while j <= max_length:\n",
        "        #print(\"have not found eot_token\")\n",
        "        #print(\"This is primer \", primer)\n",
        "        if next_word == EOT_TOKEN:\n",
        "            #print(\"Found end of text token\")\n",
        "            break\n",
        "        primer = primer + ' ' + next_word\n",
        "        tokenized_primer = torch.tensor(tokenizer(primer, truncation=True)['input_ids'])\n",
        "        tokenized_primer = tokenized_primer.to(device)\n",
        "        next_word_preds = model(tokenized_primer)[-1,:]\n",
        "\n",
        "        # Temperature sampling\n",
        "        #temp = 0.8\n",
        "        next_word_preds = F.softmax(next_word_preds/temp, dim=0)\n",
        "        # Top k \n",
        "        #k = 7\n",
        "        sorted_vals, _ = torch.sort(next_word_preds)#[-20]\n",
        "        top_k_cut = sorted_vals[-20]\n",
        "        zero_tensor = torch.zeros(next_word_preds.shape[0]).to(device)\n",
        "        next_word_preds_k = torch.where(next_word_preds > top_k_cut, next_word_preds, zero_tensor)\n",
        "\n",
        "        next_token = torch.multinomial(next_word_preds_k, 1).item()\n",
        "        next_word = tokenizer.decode(next_token)  \n",
        "        j += 1\n",
        "        del tokenized_primer\n",
        "\n",
        "    return primer\n",
        "\n",
        "    #import pdb\n",
        "    #pdb.set_trace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq529eEpFxn8"
      },
      "source": [
        "Try running the cell below to test your `generate()` function. Feel free to play around with different primers to get a feel for how your model behaves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ROTxVDJDROlO",
        "outputId": "b8ffeecc-6294-4c7c-bfa7-0607b3378619"
      },
      "source": [
        "\n",
        "n_examples = 5\n",
        "max_length = 20\n",
        "primer = \"i wonder if\" #\"your prompt here\"\n",
        "for i in range(n_examples):\n",
        "    ex = generate(primer, max_length)\n",
        "    print(f'Ex {i}: {ex}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ex 0: i wonder if  saw est er o oh  yeah  uh  o oo oho  yeah  4  o oh  i  only  like  one\n",
            "Ex 1: i wonder if  artist  best  world  way  that  way  you  could  see    when  you  gotta  know    you  are  between\n",
            "Ex 2: i wonder if  much  feel  again  there  is  where  they 're  along  from  hell  be  gonna    until  he    \n",
            "Ex 3: i wonder if  artist  real  part  hook    if  i  feel  hard  as  when  i  look  when  that  girl  your  girls\n",
            "Ex 4: i wonder if  good  eyes  good  eyes  if  in  a  girl  who  she  d  again    your  face  and  that  life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkGqLe_tL9wN"
      },
      "source": [
        "\n",
        "<div class='q_pink'><b>2.7 Unconditioned Generation [5 points]</b>\n",
        "    \n",
        "    \n",
        "Now lets do a few particular examples. Generate $10$ songs without conditioning (i.e. with only the `<|endoftext|>` token for a primer). Write a few sentences evaluating how your model performs. \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_phXEbTwpm_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6790e6b2-8a80-4f80-ab81-7f8df53a9e62"
      },
      "source": [
        "n_examples = 10\n",
        "max_length = 100\n",
        "primer = EOT_TOKEN\n",
        "for i in range(n_examples):\n",
        "    ex = generate(primer, max_length)\n",
        "    print(f'Ex {i}: {ex}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ex 0: <|endoftext|>  song  artist :  song  will    i  won 't  be  too  many  years    we  must  we  will  i  and  i  will      it 's  tough  and  it 's  like  i  can  be  there  before  this  love  it  does  in  our  love  our  words  the  city  up  together  i  i  won 't  come    like  i    did  y  let  we    you  and  yeah    won  ' cause  i  in  us    the  night  of  our    we  both    we    must    you  ' tin  y  yeah    hook  i    let 's  work   i\n",
            "Ex 1: <|endoftext|>  song :  radio  one  two  i 'm  still  more  people  and  if  you  only  baby  baby    i  oh  baby    and  if  i  love  o oo oh  you    where  how  did  you  fall  a  u  want    you  give  ya  hey  my  style    how  did  i    oh  yeah  yeah    your  life    there  were  i  no  life  without  my    baby  i  dont  wanna  oh    where  in  myself    your  baby  girl  uh  wo ah  u      you  you    if    k im at  all  i  don 't    do  you  \n",
            "Ex 2: <|endoftext|>  song  artist  moment  t cause  he  don g  know    that  guy  oh h  oh  h    what    who    and  there    oh  so  there  huh  huh  ah  huh    who  wo ah  honey      in  the  air  i  feel  so  high  inside  i    don  or  don  and  i  will  i  know    and  i  hey  when  you h  �  that  4    that    that  guy    yeah      in  it  like  i  hey    oh  i  now      when  i  well    when  i    what    that  \n",
            "Ex 3: <|endoftext|>  song  artist  thing  nothing 's  hurt ' em  so  let  me  let  ' round  ' til h oo oh  yeah  g aga\n",
            "Ex 4: <|endoftext|>  song  artist :  version  both  l oon    g 's  boyfriend  that  friend  yeah  on  the  track    so  around  the  9  years  and  the  shit  you 're  feel ing at  your  bed  i 'm  lucky  if  you  can 't  sing  ' til  i  love      and    a  ' t  ' cause  i  don 't  do    so  not  in  love  when  you  sing  e he  said  in  name  so    and  and  a  '  p  wo oh  oh  there  ain  else    and    so  yeah  well  that  '  was  b  he  ' cause  and  what  did\n",
            "Ex 5: <|endoftext|>  song  artist :  Lady  Gaga  no  Love  oh  love  your  money  baby    the  money  just  like  all  the  years  baby  you      don 't  make  you    just  like  i  ' n  i    love  i  know  i  would  be  so  insecure    oh  when  you  know  your    your  love    oh  yeah  you  oh  as  first    so  in  both  so  yeah  and  we 're  so  just  like    ' cause    baby  the  i  just    and  this  shit  we  better  '  other  if  wo ah  oh    i  need  more    and  and  i\n",
            "Ex 6: <|endoftext|>  song  artist  best :  c iest  and  people  better  why  are  the  way  more  more  or  anymore    who  you  gonna  tell  your  girl  with      who      you  know    don 't  listen  but  just  feel  what  you  ' cause    b  for    but  if  my  boy  girl      but  now    in  your  girl  oh  well    i  can 't  have  ' just  as  now    y  so  damn  fine  baby    baby    b  i    but  it  is  alright  ' cause  we  like  baby  a  girl  in  mine  oh  well    i\n",
            "Ex 7: <|endoftext|>  song  artist :  break  r  wrong  right  now  this  song  the  way  that  sh ll ow    it    i  feel  your  eyes  and  its  so  fine  i  feel  so    and  then  i  want  all  it  comes  down  on  my  life    this  o ugh  hey  my  baby  oh    it 'll  have  us      this    e  yeah  the  right  with    st      that    the  hell  that  girl  in  other  i  once  o ho  i  just  a  woman  on  and  that  i  want  when  baby  when  i  hope  yeah  and    the  the  life\n",
            "Ex 8: <|endoftext|>  song  artist :  Eminem  song  song  or  do  you  get  nothing  so  dont  stop    '  never  pay  m uth el �  mother f uf  oh  oh    so  hey  again  im  right  now  im  living  in  the  way  im  giving  you  this  ' cause  im  right    now  oh      when  i  do    and  i  will  '  yeah  when  you  like  its  only  for  you  like  my  mother  first  phone  ' ay  a hh h h h  h  ne i  n  mi  and  the  life  the  same  now  i 'm a  kids  the  same    now  im\n",
            "Ex 9: <|endoftext|>  song  artist :  Version  artist  song  were  the  c  must    a  i  won 't  you re id end  all  of  i  must    a    you re  in �  but  you  did  not  wear  your  extra  extra  a    you re am os    you  are  you  ' cause    it 's  true    it  just  do  it  it    a  4  you  must    somebody    you  would  not  give  up  a    you  are  you    it  is  alright    right  behind  you      maybe  we  will  get  a    you    may a  s  you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDagi0wo9pNN"
      },
      "source": [
        "From the unconditional generation of text, we can see that the model learned some of the structure of the data. We can see that the predictions have the 'song artist: ' portion, many of the predictions have some form of the artist name. Additionally, we can see that there si some variety in the words, not all the words are the same and some words that are expected appear such as 'gaga' (from lady gaga songs) or 'eminem (from eminem songs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmq85ywsH66h"
      },
      "source": [
        "<div class='q_pink'><b>2.8 Lyric Completion [5 points]</b>\n",
        "\n",
        "### Lyric Completion Evaluation\n",
        "\n",
        "Now let's use the beginning of each song as a primer. For a couple of songs in the validation set, omit the final $\\{10\\%, 30\\%, 50\\%, 70\\%, 90\\%\\}$ of each song and generate the remaining lyrics for the song. \n",
        "\n",
        "Write a few sentences comparing your predictions to the true lyrics. How do these differ between artists? How does the length of the primer affect the style and accuracy of the completed lyrics?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "w3bf9hDkCUBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a28b35-ca19-4c1e-da34-0c343e35507a"
      },
      "source": [
        "import math\n",
        "\n",
        "iter_obj = iter(test_dl)\n",
        "song1 = next(iter_obj)\n",
        "song2 = next(iter_obj)\n",
        "\n",
        "#import pdb\n",
        "#pdb.set_trace()\n",
        "\n",
        "song1_tokens = tokenizer.decode(song1[0])#.split(':')\n",
        "song2_tokens = tokenizer.decode(song2[0])#.split(':')\n",
        "\n",
        "split_song1_tokens = song1_tokens.split(':')\n",
        "split_song2_tokens = song2_tokens.split(':')\n",
        "print(song1_tokens)\n",
        "print(song2_tokens)\n",
        "\n",
        "print(\"Song 1 artist \", split_song1_tokens[1])\n",
        "print(\"Song 1 title \", split_song1_tokens[2])\n",
        "\n",
        "print(\"Song 2 artist \", split_song2_tokens[1])\n",
        "print(\"Song 2 title \", split_song2_tokens[2])\n",
        "\n",
        "song1_lyrics = split_song1_tokens[3].split(' ')\n",
        "num_words_song1 = len(song1_lyrics)\n",
        "\n",
        "song2_lyrics = split_song2_tokens[3].split(' ')\n",
        "num_words_song2 = len(song2_lyrics)\n",
        "\n",
        "percent_to_remove = [0.10, 0.30, 0.50, 0.70, 0.90]\n",
        "\n",
        "def generate_perc_func(percent_list, song_tokens):\n",
        "  song_tokens_split = song_tokens.split(':')\n",
        "  song_lyrics_list = song_tokens_split[3].split(' ')\n",
        "  #import pdb\n",
        "  #pdb.set_trace()\n",
        "  num_words_song = len(song_lyrics_list)\n",
        "  print(f\"Length of song is {num_words_song}\")\n",
        "  for percent in percent_list:\n",
        "    print(f\"Generating last {percent * 100}%\")\n",
        "    subset_lyrics = song_lyrics_list[:-math.floor(percent * num_words_song)]\n",
        "    print(f\"Length of subset lyrics is {len(subset_lyrics)}\")\n",
        "    primer_lyrics = ' '.join(subset_lyrics)\n",
        "    print(f\"Primer lyrics are {primer_lyrics}\")\n",
        "    primer = ':'.join(song_tokens_split[:3]) + ': ' + primer_lyrics\n",
        "    print(f\"Primer length is {len(song_tokens_split[0]) + len(song_tokens_split[1]) + len(song_tokens_split[2]) + len(subset_lyrics)}\")\n",
        "    #import pdb\n",
        "    #pdb.set_trace()\n",
        "    generated_lyrics = generate(primer, num_words_song)\n",
        "    print(f\"The length of generated lyrics is {len(generated_lyrics.split(' '))}\")\n",
        "    print(f\"Generated last {percent * 100}% lyrics : {generated_lyrics}\")\n",
        "    print(\"\\n\")\n",
        "    print(\"-\"*40)\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Generation for song 1\")\n",
        "print(\"\\n\")\n",
        "generate_perc_func(percent_to_remove, song1_tokens)\n",
        "\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Generation for song 2\")\n",
        "print(\"\\n\")\n",
        "generate_perc_func(percent_to_remove, song2_tokens)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|> song artist: Lady Gaga song title: Bad Romance (Chew Fu H1N1 Fix) song lyrics: walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i want your love love love love i want your love i want your psycho your vertigo shtick want you in my rear window baby it's sick i want your love love love love i want your love  pre you know that i want you and you know that i need you cause i'm a free bitch baby i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk  fashion baby work it move that bitch crazy gaga gaga gaga gaga i'm a free bitch baby  breakdown i want your love and i want your revenge i want your love i don't wanna be friends je veux ton amour et je veux ta revanche je veux ton amour i don't wanna be friends   i want your love and i want your revenge you and me could write a bad romance i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh want your bad romance caught in a bad romance want your bad romance ohohohohohohohohohohohoh want your bad romance caught in a bad romance   i'm a free bitch bad romance badro bad romance badro bad romance badro <|endoftext|>\n",
            "<|endoftext|> song artist: Lady Gaga song title: Christmas Tree song lyrics: lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la   lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious  refrain space cowboy light you up put you on top let's fa la la la la la la la la let's go light you up put you on top let's fa la la la la la la la la let's go   space cowboy  lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious   lady gaga here here here rapapumpum the best time of the year rapapumpum take off my stockings where i'm spreading christmas cheer yes if you want us to we will   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious   lady gaga  space cowboy space cowboy lady gaga lady gaga and she goes space cowboy lady gaga lady gaga here we go cherry cherry boom boom <|endoftext|>\n",
            "Song 1 artist   Lady Gaga song title\n",
            "Song 1 title   Bad Romance (Chew Fu H1N1 Fix) song lyrics\n",
            "Song 2 artist   Lady Gaga song title\n",
            "Song 2 title   Christmas Tree song lyrics\n",
            "\n",
            "\n",
            "Generation for song 1\n",
            "\n",
            "\n",
            "Length of song is 641\n",
            "Generating last 10.0%\n",
            "Length of subset lyrics is 577\n",
            "Primer lyrics are  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i want your love love love love i want your love i want your psycho your vertigo shtick want you in my rear window baby it's sick i want your love love love love i want your love  pre you know that i want you and you know that i need you cause i'm a free bitch baby i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk  fashion baby work it move that bitch crazy gaga gaga gaga gaga i'm a free bitch baby  breakdown i want your love and i want your revenge i want your love i don't wanna be friends je veux ton amour et je veux ta revanche je veux ton amour i don't wanna be friends   i want your love and i want your revenge you\n",
            "Primer length is 666\n",
            "The length of generated lyrics is 688\n",
            "Generated last 10.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Bad Romance (Chew Fu H1N1 Fix) song lyrics:  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i want your love love love love i want your love i want your psycho your vertigo shtick want you in my rear window baby it's sick i want your love love love love i want your love  pre you know that i want you and you know that i need you cause i'm a free bitch baby i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk  fashion baby work it move that bitch crazy gaga gaga gaga gaga i'm a free bitch baby  breakdown i want your love and i want your revenge i want your love i don't wanna be friends je veux ton amour et je veux ta revanche je veux ton amour i don't wanna be friends   i want your love and i want your revenge you  and  that  is  it  bad  heaven 's  living  home  on  air  im  trying  ' cause  when  i  want  your  power  you        i  think  you 'd  even  before  they  came  so    and  it 's  in  my  right  you  better  follow  the  sex  is  an  ambulance  i  do\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 30.0%\n",
            "Length of subset lyrics is 449\n",
            "Primer lyrics are  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i want your love love love love i want your love i want your psycho your vertigo shtick want you in my rear window baby it's sick i want your love love love love i want your love  pre you know that i want you and you know that i need you cause i'm a free bitch baby i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free\n",
            "Primer length is 538\n",
            "The length of generated lyrics is 801\n",
            "Generated last 30.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Bad Romance (Chew Fu H1N1 Fix) song lyrics:  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i want your love love love love i want your love i want your psycho your vertigo shtick want you in my rear window baby it's sick i want your love love love love i want your love  pre you know that i want you and you know that i need you cause i'm a free bitch baby i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free    lets  walk    i  want  you  baby    i  want  i  want  im  right  there  with  you  there    and    i  oh  i  don 't  even  know  '  thing  one  one  more  with  somebody  else    i  don 't  know    ' cause  i  love  your  baby  it  was  not  what  i  know  it  will  come  well    i    you  and  i  im  right    on  it    with    who    it    for  me      ' cause  once  i 'll  be  busy    so  long  baby  it  was    not  what  im  what  they  want  you    and    im  a  fashion  the    im  a  boys  who os n ' round      the  lights  i 'm  going  to  tie  the  r aps ine  this  ain 't  not hin in    that  i  love    your  guy    and  my  g ibi  hey  i  don 't  oh  know      im  on  your  hands  s  want    your  you      a  baby    g aga oh  hey  yeah    my\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 50.0%\n",
            "Length of subset lyrics is 321\n",
            "Primer lyrics are  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i\n",
            "Primer length is 410\n",
            "The length of generated lyrics is 547\n",
            "Generated last 50.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Bad Romance (Chew Fu H1N1 Fix) song lyrics:  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you know that i want you and you know that i need you i want it bad your bad romance   i want your love and i want your revenge you and me could write a bad romance ohohohohohoh i want your love and all your lover's revenge you and me could write a bad romance ohohohohohohohohohohohoh caught in a bad romance ohohohohohohohohohohohoh caught in a bad romance  hook i'm a free bitch free x5 bad romance free x7 bitch free x5 i'm a free bitch baby  posthook raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your horror i want your design 'cause youre a criminal as long as youre mine i  wanna  be  free  my  girls    i  don 't  need  you  or  you  better  show  that  girl  b out  c  won 't  be  your  lucky  oh  ha  we  can  ' b  a  i  can    my  honey  hey  o  o oh  hey  it  said  this  it  it 's  said  it  it  ' nt  baby  don ' 've  a    a  baby  like  a  ice  red    a  love    e ja  o oh    ya    y all  now  you 're    e  wo ah    le  la  le  forever      a  boy    e    e    hi  d  baby    l ala    b  my  oh    l  ourselves  both\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 70.0%\n",
            "Length of subset lyrics is 193\n",
            "Primer lyrics are  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you\n",
            "Primer length is 282\n",
            "The length of generated lyrics is 398\n",
            "Generated last 70.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Bad Romance (Chew Fu H1N1 Fix) song lyrics:  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk passion baby work it i'm a free bitch  hook free bitch x9 i'm a free bitch free x5 i'm a free x7 bitch free x5  i'm a free bitch baby  post raraahahah romaromama gaga oh lala want your bad romance raraahahah romaromama gaga oh lala want your bad romance   i want your ugly i want your disease i want your everything as long as its free i want your love love love love i want your love i want your drama the touch of your hand i want your leatherstudded kiss in the sand i want your love love love love i want your love love love love i want your love  pre you  see  all  the  loving  you  wanna  touch  you  i      baby  come  on  now      just  come  to  me    just  come  on  me  come  kiss      and  you    in  here  hey    i  wanna  come  by    want    right  come  to  all  right  c 'm  fine        well  baby  all  down  baby  i  don 't �  it 's  true    you  and  i    don 't  several  americ an ster ic  '  minute  a  baby    l én  y os ny    lets  fight  o oo o o oh �    the    well  \n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 90.0%\n",
            "Length of subset lyrics is 65\n",
            "Primer lyrics are  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby\n",
            "Primer length is 154\n",
            "The length of generated lyrics is 1161\n",
            "Generated last 90.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Bad Romance (Chew Fu H1N1 Fix) song lyrics:  walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby work it move that bitch crazy walk walk fashion baby  work  it 's  time  you 'll  leave    it 's  just  like  you    you  do  it  oh  but  baby  it 's  a  girl  you  do  it    it  oh    let  me  tell  myself    right  home  babe    it  won 't  make  something  too  many    in  it  in  it  ' n ' t  huh  m ah  ah  oh  babe  ' y  dont  i  love  oh  baby    its  like  oh  but  you    you  like    let  me  hey  hey    o uu  u  you    come    give  you  life  on  the  life  and  i 'm  a  girl  so  who  who    yeah  who  y  yeah    be  gonna    give  e    that    i  and  somebody    so  why  you  know  i  want  you  for  that  i  baby  oh  well  let  just  give  all  of  this  love      there  lets  fly  without  ya  this  one  h oo  is    in  this    that  feels  real  pretty  fine  now  i  was you    would  not  love  what  i  don 't  love  im  giving  all  all  of      it  isn 't  someone  else 's    in    if  this  it  is  exactly  you  in  the  way  baby      it  i 'll  like  that    i  would    not  be  ' cause  i  i  can 't  know  i  and  from  tomorrow  you  must a  like  who    and  if  you  from    you      i  would  don  really  just  think  ' cause  the  men  you d aw ah      so    i 'll    like  a  '  wo ah  yeah  yeah  oh  well  baby  but  you  when  we  can  you    in    i  won  im  ' cause  you    they 're  gonna    you        that  can  ' t      it 's  like    now    we  could  yeah  not  just  inside      she s  like  i  ' 't ero  yes  but  im  a  sister  who  needs  me    and  yeah    what  he  ain '  mean  oh  hey  i  got  so  bad  as  a  well  and  i  on  the  life  in  the  long  night      it  ' t    and  hey  but  i    on  the  both  i  is    so  oh  yeah  it  has  to  am  he  so  oh    and  you  you    in    yeah  yeah  now    well  well  now    it  won 't  i  am    right  after  this  4    a  yeah  and    if    is    and  i  isn  '  longer  than  this  i    and  you  so  i  i  want    i  know  i  is    you  yeah  yeah    yeah    so    i  and  i    i  don  yeah    yeah  yeah    yeah    so  o ow    o ooo ooo  is  there  without    no  more  wo  a      oh  i  i  yeah  i  but    wo  yeah  ne  wo    no  o oh  no  wo  o  de  there    ah oh  i  yeah  ah  i  well  wo  o  oh  you  yeah  uh  now  yeah  yeah  i  o  u  i  yeah  yeah  yeah  yeah  wo  wo  wo  so  yeah  yeah  u  oh  n  a  yes  but  i  no  uh  n  so  ne  wo  yeah  o  yeah oh  a  i  i  i  wo  a  y    yes  oh  uh  there  wo  alright  i  a\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generation for song 2\n",
            "\n",
            "\n",
            "Length of song is 336\n",
            "Generating last 10.0%\n",
            "Length of subset lyrics is 303\n",
            "Primer lyrics are  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la   lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious  refrain space cowboy light you up put you on top let's fa la la la la la la la la let's go light you up put you on top let's fa la la la la la la la la let's go   space cowboy  lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious   lady gaga here here here rapapumpum the best time of the year rapapumpum take off my stockings where i'm spreading christmas cheer yes if you want us to we will   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my\n",
            "Primer length is 376\n",
            "The length of generated lyrics is 354\n",
            "Generated last 10.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Christmas Tree song lyrics:  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la   lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious  refrain space cowboy light you up put you on top let's fa la la la la la la la la let's go light you up put you on top let's fa la la la la la la la la let's go   space cowboy  lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious   lady gaga here here here rapapumpum the best time of the year rapapumpum take off my stockings where i'm spreading christmas cheer yes if you want us to we will   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my  bab ı t ans oh  oh  oh  oh  n r  v  yeah    oh  o  woo    y  ve  y  y  oh oh\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 30.0%\n",
            "Length of subset lyrics is 236\n",
            "Primer lyrics are  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la   lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious  refrain space cowboy light you up put you on top let's fa la la la la la la la la let's go light you up put you on top let's fa la la la la la la la la let's go   space cowboy  lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we\n",
            "Primer length is 309\n",
            "The length of generated lyrics is 410\n",
            "Generated last 30.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Christmas Tree song lyrics:  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la   lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious  refrain space cowboy light you up put you on top let's fa la la la la la la la la let's go light you up put you on top let's fa la la la la la la la la let's go   space cowboy  lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we  will  ya  and  when  he  don 't  wanna  be    ' cause  baby  come  speech h town    now    i 'm  just  my  g aga oo  boy  come  there  boy  girl    i  ' tin  ice  d n ave  wo  boom  o ow    j  em a my  ho oh  oh oh    l  on  ' cause      and  from  from    you  are  you  in  the  new  she  yeah  we  could  move  right  behind  myself  lady  g aga  oh  oh    g  ice    t  type  \n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 50.0%\n",
            "Length of subset lyrics is 168\n",
            "Primer lyrics are  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la   lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious  refrain space cowboy light you up\n",
            "Primer length is 241\n",
            "The length of generated lyrics is 387\n",
            "Generated last 50.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Christmas Tree song lyrics:  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la   lady gaga ho ho ho rapapumpum under the mistletoe rapapumpum yes everybody knows rapapumpum we will take off our clothes rapapumpum yes if you want us to we will you   lady gaga ohoh a christmas my christmas tree's delicious ohoh a christmas my christmas tree's delicious  refrain space cowboy light you up  for  that  should  i    please  ladies  not  just  this    oh  let 's  go  oh    this    i  give  you  something  you  could    oh    so  lady  g aga ate  life  and  love  in  my  love  hey  boy  i  know  we  are  better  now  o ow  baby  now    i 'll  know    you  do  it  so    lady  g  is  ya    lady  god    we  all    you  and    l ack  and  the  lady  just  like  a yo  girl  you  and  oh  oh    lady  g aga  cried    and    we  all    oh  love  oh    and  honey    and\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 70.0%\n",
            "Length of subset lyrics is 101\n",
            "Primer lyrics are  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top\n",
            "Primer length is 174\n",
            "The length of generated lyrics is 444\n",
            "Generated last 70.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Christmas Tree song lyrics:  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top let's fa la la la la la la la la  break lady gaga the only place you'll want to be is underneath my christmas tree the only place you want to be is underneath my christmas tree   lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on top  spin  oh    i  can 't  hear  it  ' cause  i    got  to  death  a  little  bit  a  bit  k h  � ch ane  we  can  hurt  n oo  o o o gue et    ' cause  y  yeah  yeah  g  �  k  e  na    let 's  beat  your  skin  we  could  take  it    ' cause  yeah    got    t  e e i  i    shake  your    when  i  n  n  oh h  let  me  follow  with    the  world  knows  you  and  baby  she os    it 's  well  she  es    it 's  oh  there  on  the  mon  or  the  art  of    so  on  the  summer  road  lets  cause    e  y  y  oh  oh h        it  just  come  together  baby  come  on    i  baby  come  mine  baby    the  thing  about  i    woo  baby  everybody  tell  myself    and  let  me  behind    y  cause  h ah  ha  long  be    oh  ch à  mi    lady  g  i    so  i  \n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "Generating last 90.0%\n",
            "Length of subset lyrics is 34\n",
            "Primer lyrics are  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on\n",
            "Primer length is 107\n",
            "The length of generated lyrics is 410\n",
            "Generated last 90.0% lyrics : <|endoftext|> song artist: Lady Gaga song title: Christmas Tree song lyrics:  lady gaga rapapumpum rapapumpum rapapumpum rapapumpum  refrain lady gaga light me up put me on top let's fa la la la la la la la la light me up put me on  top  me  fashion  of  a  g aga  p aul os a  hay k ino  a  �  a  g  americ ab ie  white  hey  girl  we  want  love  in  the  middle  i  am  e    g aga  i  m  ass    i    j my  ass    e  i  like  that  girl  oh  yeah  well  one  more  people    i  i  m    ass    my  brain   ive  an  di  when  baby      a  s  i    my  baby   ive  a  x  i    g ale    i  better    i  hey    hey    i      i  like  me  dat  hey    it  ain 't  me  y  my  freedom    i    d    ' ope    my  mother man  yeah    im a  white  im  a  dirty  m  ay    i  can  i    my  h  u hh  he a  girl  and  i    j  my   i  know  there  l  c ah  yeah    black  dirty  cause  i    l    k y  j  baby    k  oh    i    i  well    we    honey    lady  g aga  honey  now  o oll ah    h  \n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bUXZWuL-B-c"
      },
      "source": [
        "From the examples above we can see that the pattern is that for the lyrics missing 10% or 30% of the words, the prediction have words that are related to the rest of the song or that appear before. However, as we deleted more of the lyrics (for 50%, 70% and 90%) we see that the predicted words are very different from the actual song lyrics. In particular, we see that many of the predicted words are either blank spaces or very simple, one-syllable words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-P2aVIa1Iau"
      },
      "source": [
        "<div class='q_pink'><b>2.9 Song Title Generation [5 points]</b>\n",
        "\n",
        "\n",
        "Now condition on only the artist names and generate several song titles per artist. Write a few sentences about the generated titles. How realistic are these titles? How do they compare to the other titles' of the artist?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "NPmr5PfX1IvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee138a53-baff-40b3-c9f5-bf3ea0acdfdb"
      },
      "source": [
        "#raise NotImplementedError   # TODO: implement\n",
        "\n",
        "song_title_per_artist = 3\n",
        "artist_name = ['Eminem', 'Lady Gaga', 'Beyoncé', 'Maroon 5']\n",
        "\n",
        "def generate_title(artist_list, num_titles):\n",
        "    for artist in artist_list:\n",
        "        print(f\"Generating {num_titles} for {artist}\")\n",
        "        for i in range(num_titles):\n",
        "            primer = EOT_TOKEN + ' song artist: ' + artist  + ' song title: ' \n",
        "            ex = generate(primer, 18)\n",
        "            \n",
        "            print(f\"The title generated is: {ex}\")\n",
        "\n",
        "generate_title(artist_name, song_title_per_artist)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 3 for Eminem\n",
            "The title generated is: <|endoftext|> song artist: Eminem song title:     dr  and  shady    he  must as  far  as  is  there\n",
            "The title generated is: <|endoftext|> song artist: Eminem song title:   mark  my  daughter  j im  ready  to  take  it  i 'm ma\n",
            "The title generated is: <|endoftext|> song artist: Eminem song title:   hook  you  see  my  love  ' cause  that  boy  it 's  all\n",
            "Generating 3 for Lady Gaga\n",
            "The title generated is: <|endoftext|> song artist: Lady Gaga song title:   Rap !  You  gotta  call  ya    don 't  buy  me\n",
            "The title generated is: <|endoftext|> song artist: Lady Gaga song title:   The  Sun �  Night  and  the  air  you  get  i  i\n",
            "The title generated is: <|endoftext|> song artist: Lady Gaga song title:   With  My - oker  dat  hey  k r igg ar is\n",
            "Generating 3 for Beyoncé\n",
            "The title generated is: <|endoftext|> song artist: Beyoncé song title:   speech  o ow  so  many  we  r y ork  how  there  americ\n",
            "The title generated is: <|endoftext|> song artist: Beyoncé song title:   rom j as y ' l  shady    l us en  to\n",
            "The title generated is: <|endoftext|> song artist: Beyoncé song title:   be tin �  wo oho ou  there  is  there  is  not  long\n",
            "Generating 3 for Maroon 5\n",
            "The title generated is: <|endoftext|> song artist: Maroon 5 song title:   Rap  On  each  other  i �  promise  i  love    '\n",
            "The title generated is: <|endoftext|> song artist: Maroon 5 song title:  � �  I 'm  crying  when  we    there  yeah  uh\n",
            "The title generated is: <|endoftext|> song artist: Maroon 5 song title:   ale ill  hold in  all  my  daddy  im  talking  from  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33LlxraI-Stv"
      },
      "source": [
        "We can see that the results in the title generation are not the best and it is probably related to the fact that there might be some data leakage in our training pipeline that we could not fix. We can see that for songs from Eminem, we can see titles with words that appear more frequently in that author songs, as well as with Lady Gaga. For Beyonce, every time we ran this cell, we found words related to love, baby, rlationships. The hardest to predict song titles were from Maroon 5. For some reason the model had a harder time predicting the title from this band."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FTboYjO1JGu"
      },
      "source": [
        "<div class='q_pink'><b>2.10 Full Song Generation [5 points]</b>\n",
        "\n",
        "Now condition on artist names and *new* song titles of your creation. Write a few sentences evaluating the resulting lyrics. How realistic are they given your titles? How reasonable are they for a song in general? How does their style compare to other lyrics by that artist?  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jec94Zfl1JfS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6445fb0-6819-4d87-98f6-19c5080f1150"
      },
      "source": [
        "#raise NotImplementedError   # TODO: implement\n",
        "\n",
        "artist_name = ['Eminem', 'Lady Gaga', 'Beyoncé', 'Maroon 5']\n",
        "song_titles = {'Eminem': ['8 mile', 'Da dude', 'WTF your momma'],\n",
        "               'Lady Gaga': ['Rockester', 'Monster city', 'Crazy Rose'],\n",
        "               'Beyoncé': ['Why you leave me?', 'Wanna be my lover', 'Remember us'],\n",
        "               'Maroon 5': ['Sugar', 'Leave me', 'Lets get party']}\n",
        "\n",
        "def generate_lyric(artist_list, song_titles):\n",
        "    for artist in artist_list:\n",
        "        print(f\"Generating songs for {artist}\")\n",
        "        for title in song_titles[artist]:\n",
        "            primer = EOT_TOKEN + ' song artist: ' + artist  + ' song title: ' + title + ' song lyrics: '\n",
        "            ex = generate(primer, 350)\n",
        "            print(f\"The song generated is: {ex}\")\n",
        "\n",
        "generate_lyric(artist_name, song_titles)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating songs for Eminem\n",
            "The song generated is: <|endoftext|> song artist: Eminem song title: 8 mile song lyrics:   i 'm  back  up  j ork  and  i  want  ' cause  if  you  i  want    '  cause  i        so  baby  wait  and  i  don 't  know  baby  you  just  gotta  go  and    the  man  im  so  alright  but  now  it 's  only  that  i  say    em inem  im a  girl  i  said  he 's  too  late  i  dont  try  this  time    em  i    feel  so  good  to  be  happy  as  long  as    and  yeah  i  don  give    and  im  perfect    you  as  long  as  cause  you    im  a it    im  in  my  middle  finger  up  j  on  our  home    i    just  like    just  like    i   'm  yeah    im  a  i    a  guy    it  just  gotta    me  and  i    ' cause  i      and      ' cause  ' yon a  y or go y d aw g i  like  this  and  yeah    im  on  i  n ah  ' round    em  in em    it  was  too  much  it 's  so  i  just    you  baby  ' cause  it  ' inem ack      a ight    you  so  girl    i      yo  i      em in ac ou ie  your  dad 's  in  rap  an    this    you    he  '  gonna  hear  the    i    n ah  i  love  this  it  was  y a est an  y  when  you  you  and  im  in  y  first  two    ' cause    you    in  yourself    you    in ana      im  back    i 'm  like    yeah        and  i    im  '  better    myself    huh  but  somebody    i    y  her    e ot  his  and  n  ah    the  new  50  from  em      it  just  what    me    yeah    i  dont  tell    em  into  me inem    em  ' cause    it  is  just\n",
            "The song generated is: <|endoftext|> song artist: Eminem song title: Da dude song lyrics:   hook  let  it  feels  all  time  it  sucks  oh oh  this  one  girl    he  don 't  know  tell  your  body  it 's  sweet  ' b y et h  yeah    when  i  do  it    so  i  can 't  take  it  ' cause  i  i  ain 't  ' tin  one  man  she d re  ' cause  if  i    tell  im    everything  these  are  my  heart ster ic  in  love    my  brain  im b  and  this  when  i    do  it  it  ' cause  i    it  feels    i  have  the  i  wanna  get  ' cause  it  is  i  don 's  man  is  a  girl    i  wanna  hear    mom  and  i  im  an  one  yo    so  what  is    it  feels      i  i 'm a  baby      when    i    do  it    this  ' cause    i    im  a  man    its  when  hail in  her  life  it  ain    so  now  you  dont  have  it  from  you  you  until  that  man  oh  yeah  o ooo ah    my  ass    ' cause  hey  y ' cause  wo �  i  feel  this  when  oh  my  ass  in  your  wife  right  down    this  ass\n",
            "The song generated is: <|endoftext|> song artist: Eminem song title: WTF your momma song lyrics:   dr  mic  d ar s  a  d  n a    k  y uh  o oh  oh  yeah    you  were  in  my  house  no  longer  mine  and  if  there  has  so  far  from  this  is  it 's  not  right  that  it 's  you  but  no  baby  im ma  but  there  is    the  greatest  of  mine  with    they  just  come  in  here  yeah      the  type  of  mine  that  means  a    b  h  o oh  oh    you  in    my  mind  and  i  cannot  treat  the  worst  you  you    you  yeah  uh    my  life  my  ass  once  with  this    and  i  just  as  once  it 's  as    in  the  baby  ' round  it  in  myself  but    i  could o  change  baby  from  a    go  with  all  you  wanna  go    when  is  there  or  it  im  back    and  this  one  n  another    im  name  for  the  words    im  name  for  n  y  yeah    my  daddy    yeah  uh    somebody  who  i  know  there    i  do      yo  but  y  �    em inem  on  shady  and  im  right  i  want  a  a  im a  type  m  tomorrow  yeah    i      but  it  ' til  tonight  well    o h hh    is  a    c    and  i    don 't  listen    o  i  want    the    oh  wo ah  wo ah  ' til  y my  words  is    oh oh  no  no  yeah    i    this    in    and  i    you    uh oh    my  ex  n  my  ass  and  o oho oh oh oh oh  oh    so  not  happy      o oh oh  oh    oh  uh  oh    r  r  oh  oh  u  oh    r  o oh  oh  oh  oh    the  ones  again  uh  want    o  uh oh  wo  � oh  oh    o oo  o oho  b    yo    uh  oh  yeah  huh  \n",
            "Generating songs for Lady Gaga\n",
            "The song generated is: <|endoftext|> song artist: Lady Gaga song title: Rockester song lyrics:   inter lude os  u ll ir  d re a  u  honey  oh h oh  he    k ar o  se    l ee  t  la  o  inter  de  l ing    l es    ne  t  hey  uh uh  honey  oh  he    oh  he  s  him  a  oh    k  ne h  oh  uh  oh  h  oh      se  b  b  e  it    ne  k    k  ne    u  ah      u  h  ah  well  it 's  all  right  there  oh  ah  there  now    o  yes    am ik  se    se    adam  s    c ork    ah he  b  im  sorry    im  so  well  well  yes  r  u  r  o    k  b  oh  hey    se    k  ne  h    i    l  e  oh  h  4  o  o    t    la      l  e    j    de    e  black  huh  y  alright    et  i    b  ha    u  oh  oh    h      i    h    he  woo      y  ay    oh  yeah  well    se      j  y  o oh  well    yes  4    se    y  s  na  o  he  n uh  y  4    o\n",
            "The song generated is: <|endoftext|> song artist: Lady Gaga song title: Monster city song lyrics:   you  don 't  give  ' cause  i  i  just  wanna  hit  tonight  and  i    i  say  it 's  just    say  that    do  it  just  do  it  yeah  do  it  yeah  oh  oh    do  i    just    you  yeah  babe  hey  baby  baby  do  y all  night  hey    oh  baby  i    yeah  huh  o o oh  baby  let 's  know    o  u hh h oh  oh    yeah    uh  baby  she ye h l  living  in  the  bur pe  where  the  summer  ways  oh  oh    and  y    you  and  baby  oh  and  baby    it  ain 't  even  when  i    just  what  when  you  love    oh  oh  tonight    and    i    now    be  your  tonight    oh    oh  baby  oh  hey      yeah  yeah  tonight      i    i    so  now  believe  i  know  oh  hey  i  is  baby    oh  hey  \n",
            "The song generated is: <|endoftext|> song artist: Lady Gaga song title: Crazy Rose song lyrics:   you  know  and  that  should  know  and  oh h oo oo  o o uu    it  seems  so  hard  to  be  that  feels  so  far  home  babe  when  i  take  here  on  my  l ymes �  mother m op ab at ı  baby  i  like  we  i  can 't  i 'd  diss  and    oh  uh  uh  yeah  i  just  want  it  ' cause  it  won 't  make  no  matter  what  tonight  and  o oh h  o o oo    there  isn 't  girl  when    i  o go  go  it  and  it  don 't    be  so  far  baby  o  o o o oo  o  oh h    and  and  i  am  i  a  mother m  pop ab  at   aha  uh  o ow h  i    be    many    and  baby  im  alright  na  se  na  i  gotta    take    i  never  c ay    and  im  a  huh    on  the  o oh  he    and  is    you  the  mother  m  ac at  i  and    i  oh  i  want    o oh h  o oh  h  ay    i  am  i  without  americ as a  boy  yeah  and  who os  n ah  yeah  a  �  americ  as  a  hey    yeah  you    o  yeah    ay    oh  she �  alright    uh    it 's  all  around  the  babe  like    us  ' cause    it  ' cause  and  is    you  it  ain play ie  n  �    you  and    my    oh  uh      and  ay  n    oh  i  wanna    take    yeah  yeah  4  u uh  yeah\n",
            "Generating songs for Beyoncé\n",
            "The song generated is: <|endoftext|> song artist: Beyoncé song title: Why you leave me? song lyrics:   and  i  know  you  shake  this  and  i    they  know    there  you  go  there  and  they  yeah  you  go  right  there  you  do  the  time  goes  oh  oh  wo ah  oh  oh  the  since  you  know  this  could  not  know  so  when  i  go  right  again  well  when  you    oh oh    so  that  i      the    if  the  9    you    go  \n",
            "The song generated is: <|endoftext|> song artist: Beyoncé song title: Wanna be my lover song lyrics:   it 's all  about  being  out ta  stay  ' cause  this  is  is  my  lover  the  art  of  being    that 's  my  baby  i  don 't  think  there  isn 't  mine  with  me  tonight  do  ya  ' cause  i  know  it  ain 't  nobody 's  well  again  yeah    from  where and  how  by  me    tonight  for  the  next  night    and  when    and  are  we  right  tomorrow  tonight    for  me    tonight    for  oh h oooo oho oh  h  o oo oh  uh oh  g  yeah  ' cause    that  ' til  baby  now    for  the  baby  cause  oh  there    not  at  both    my    tonight    i  it  ' til  i  i  ' cause  you  ' r n y  y    i 'm ma  ' cause  it  is    is    my  baby    my    for  i    '  cause  yeah      from  i    tonight    it  i  babe  yeah    from  g    my  girl  you  oh  yeah      you  yeah  i  just  do    o oho oh  \n",
            "The song generated is: <|endoftext|> song artist: Beyoncé song title: Remember us song lyrics:   you  are  i  love  it  seems  like  that  i    you  let 's  get  in  it  isn 't  nothing  is  like  all  that  love  like    and  now  all  that  you    all  of  us  do  it    like  a  baby    lets  see  there    i  like      all  of    three    it  does  so  lets  go    i    when  we    love  this  time  yeah      a    tonight    it    isn  ' til  every  life  by    together  with  our  fame   �  a  tonight  no    i    yeah    when    i    so  we  gotta    all  we    all  i    all    us  the  tonight    it    lets  you    just  so  i  is  j  babe  who os    you    all  that  i    like  and    now    i      you  and    now    all  from  you  well  it 's  all  and  this  was    in  it  will  come    my  ' round  from  us  before  i    when    the  people  i    and    now  it    uh  yeah    in    all    all  the    and  o oh  oh  i  o oh  all  and  o  oh  all  i  i  o  oh  and  i  se    it  it      you    all  \n",
            "Generating songs for Maroon 5\n",
            "The song generated is: <|endoftext|> song artist: Maroon 5 song title: Sugar song lyrics:       o oh  oh  i  i  but  you  should  check  this  ain 't �  never  let  ' round    and  you  you  know    and  yeah  o oh  yeah  o  o  and  oh  it 's  how  these  ones  when  you  h ush  oh  yes  ah oh  oh  you  and    the  dawn    h  mile  each ha go    it  gets  more  anymore  when  i    oh  yeah    o h  do  what    o  uh    h  oh  yeah  anymore  i  hey    and    n    all  of  those  hairs  tonight  o ooo  yeah  yeah  hey  �  wo ah  oh  uh   �      i  o oh  hey    i  i  o  yeah    americ a h  yeah  you  like    you    so  one  long  yeah  yeah oh oh  yeah    so  yeah  hey    o oo  o o oh  and  oh  \n",
            "The song generated is: <|endoftext|> song artist: Maroon 5 song title: Leave me song lyrics:     so  happy  now  be yon le y o le  y  when  is  woo    i 'm  sorry  for  i      oh  oh  oh  alright  baby    i  been  bur ted  y est k y    i 'm    just  i    i  just  and  i  in  y on  my  name  is  alright  with  '  heart man  it 's  alright  baby  right  there  oh  yeah  baby  i  oh  yeah    oh  huh  yo  g iv  just  alright  i  be  oh  like  we  i  love  myself  from  they  as  na  oh  baby  yeah  yeah    and  i  o oo    im  on  y r  sweet  a my  life  it 's  so  nice  oh  again  with    i  not  well  y as    st ans  okay    i  hey    i  was ma a  girl  well  but  baby    i  i  in  yeah  alright  okay  4  like  j my  mother  from    adam  the  god  ale ale  na  k  my    i  in  j  mom  and  yeah  y et  k  alright    alright    oh  and    yo    i    just  again    i    it  isn 't  a  babe  what  she h town  she  se aaaa  babe  there  d ah  huh    i    o o  uh  oh  yeah    i  do  y  n ü inem inem  in on y  r  r    y or  oh  oh    i �  dont  listen    yo    you �  the  la  y  r  i  uh    oh  yeah    oh    so  \n",
            "The song generated is: <|endoftext|> song artist: Maroon 5 song title: Lets get party song lyrics:   m am am ap ma  d  d      im  a  haha  a  ah  baby  cause  oh  yeah  i  just  show  the  lights      i  can  smell  your  whole  skin  oh  i    yeah    what  did  you  and  i  im  a    i  it  can  im  i  love  ya  b  like  im  on  top  of  me  i    like  well  baby  now  yeah  we  know    yeah  hey  let 's  get    come  by  i  i  i  it    can  i      i  dont a  babe    in  here  i    o o o  ay  on  my  name    i  i    just  on    i  with  i  im  the  l oon  h i  will    oh  tonight      just    hey  tonight  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2mc3XMg-bwL"
      },
      "source": [
        "In general our predicted songs are not ery good. They are not very realistic given the titles. In aprticular, many of the predicted songs are random words, very few of the lyrics words are related to the song title. At least, some of the lyrics words seem to be plausible they would appear in that artist songs. The style that some of the lyrics seem to posses, in terms of the words that are used, do align with the kind oflyrics that that artist usually sings, especially this was the case for many Eminem and Lady Gaga songs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKprf1726Z8"
      },
      "source": [
        "<div class='header_green'>\n",
        "    \n",
        "# 3. RESEARCH PAPER 1 [16 points]\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7udniogw26Z8"
      },
      "source": [
        "**NOTE:** Since I want everyone to dedicate the vast majority of the remaining semester to the research project, I highly suggest selecting papers that are related to your project. And, for this final homework, we will ask you to read two papers, not just one.\n",
        "\n",
        "\n",
        "-----------\n",
        "\n",
        "As we build a foundation in NLP, it's also important to also see what the latest, cutting-edge work (research) looks like. It's incredibly worthwhile to learn about the types of problems people work on, their methodology and approach to the problem, the datasets they work on, the issues they raise, and the solutions they posit. The field moves incredibly fast, but the __approach__ to ML/NLP research is relatively stable -- different types of papers are accepted as the years progress, but that's a different story.\n",
        "\n",
        "We want to help you get practice reading research papers, which mostly entails thinking critically about the work, being able to discern the main takeaways/conclusions, and to reflect on the work in a meaningful way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iix7n20d26Z8"
      },
      "source": [
        "<div class='q_green'><b>3.1: Read an NLP research paper [0 points]</b>\n",
        "    \n",
        "Select and read a paper that was published in ACL, NAACL, EMNLP, or COLING in 2020 or 2021. You can find a list of such published papers by searching Google, a la \"ACL 2020 accepted papers\". For this assignment, you are allowed to pick either a short paper (4-5 pages) or long paper (8-9 pages), **but you must not select a workshop paper**. List below the name of the paper, authors, venue, and year published.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcS6Ofz626Z8"
      },
      "source": [
        "The title of the paper i am reading is \"GNN is a counter? Revisiting GNN for question answering\" by Kuan Wang, Yuyu Zhang, Diyi Yang, Le Song, Tao Qin. The paper was posted in Arxiv in October 2021, not yet published but done in Microsoft Research and quite relevant to our final project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj_MUNS226Z8"
      },
      "source": [
        "<div class='q_green'><b>3.2: Problem? [2 point]</b> What is the problem that it is trying to address? In other words, what is it trying to solve? (2-3 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14riVqdG26Z8"
      },
      "source": [
        "THe paper is trying to answer the questions: Are GNN under- or over-complicating networks for QA? What is the essential role of the graph architecture in reasoning over knowledge?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTcro8Vr26Z8"
      },
      "source": [
        "<div class='q_green'><b>3.3: Solution? [2 point]</b> At a very high-level, what was their solution? (2-3 sentences). Here, you don't have room to go into the small details (e.g., about the model), so you'll need to summarize the most important elements that comprised the solution.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyx2hKIW26Z9"
      },
      "source": [
        "The authors used Sparse Variational Dropout as a tool to disssec the existing graph network architecture. This tools was used to recognize which pieces of the QA-GNN were dispesable and which ones were actually cotnributing to reasoning. Based on these results of the dissection, the authors came up with a simplified GNN model that shows great results on the CommonSenseQA and OpenBookQA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Zp-euc26Z9"
      },
      "source": [
        "<div class='q_green'><b>3.4: Data? [2 points]</b> What dataset(s) did they use? Are they freely available? What's the size of the data? (2-3 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyON8YG826Z9"
      },
      "source": [
        "The authors used CommonSenseQA and OpenBookQA datasets which are datasets commonly used as benchmark for question answering tasks. Both of the datasets are freely available. CommonSenseQA is a multiple choice question dataset with 12,102 questions and OpenBookQA is a multiple choicd question dataset with 5,957 multiple choice questions. They also use ConceptNet as a general commonsense graph. ConceptNet is also freely available to use and has over 13 million of edges. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lOnObMl26Z9"
      },
      "source": [
        "<div class='q_green'><b>3.5: Model [2 points]</b> Very related to the 'solutions' question, describe here any models that they used, and what made it effective (2-3 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk65LXRH26Z9"
      },
      "source": [
        "With the SParse Variational Dropout, the authors found that GNN are over-parameterized, some layers can be pruned substantially and the initial node embeddings are dispensable. After this discover, they developed Graph Soft Counter (GSC) an extremely simple graph neural network that basically serves as a counter of the edges of the knowledge graph. This much simpler model, with less than 1% of trainable parameters than the used GNN architectures in other papers, outperforms the previous solutions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9zV05oA26Z9"
      },
      "source": [
        "<div class='q_green'><b>3.6: Results? [2 points]</b> What are their main results? (~2 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SOPdvD726Z9"
      },
      "source": [
        "The GSC network outperforms the results of other GNN based networks in both benchmark datasets. In particular, GSC outperforms the previous best model with a mean of 2.57% test accuracy in the CommonSenseQA dataset. Similarly, GSC outperforms the previous best model with a mean of 2.53% test accuracy in the OpenBookQA dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npXsqVeg26Z9"
      },
      "source": [
        "<div class='q_green'><b>3.7: Strengths? [2 points]</b> List 2-3 strengths of the paper\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNlcj9eN26Z9"
      },
      "source": [
        "- The use of SVD (SParse Variational Dropout) is a great idea to show really how much the weights/layers of the GNN are helping with reasoning.\n",
        "- The simple structure of GSC and great results in test set strongly show that there is an overparameterization of the GNN models when applied to this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5sAFaZK26Z9"
      },
      "source": [
        "<div class='q_green'><b>3.8: Weaknesses? [2 points]</b> Although you may be new to this problem and all of its content, try to list 2-3 weaknesses of the paper (anything that you think could strengthen the paper is sufficient).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BJCZ8gg26Z-"
      },
      "source": [
        "- Even though GSC has a clear and intuitive explanation, the detailed explanation in the paper does not seem very clear to me. I think there could be more work in the explanation here.\n",
        "- Although it was a great idea to include the SVD, it would have been helpful to provide more background on how it was used on the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwXuKbK_26Z-"
      },
      "source": [
        "<div class='q_green'><b>3.9: Evaluation [2 points]</b>\n",
        "    \n",
        "How would you evaluate this paper in terms of:\n",
        "- scientific contribution\n",
        "- effectiveness to solve the problem\n",
        "- how convincing it was.\n",
        "    \n",
        "Give each of these elements a score from 1-10 (10 is best). No word explanation necessary.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jptg2YdK26Z-"
      },
      "source": [
        "- scientific contribution: 9\n",
        "- effectiveness to solve the problem: 9\n",
        "- how convincing it was : 9\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZuM18Uj26Z-"
      },
      "source": [
        "<div class='q_green'><b>3.10: Research Ideas [4 points]</b>\n",
        "    \n",
        "Think of 1-2 research ideas that you have based on this paper. It doesn't have to be grand; most research is very incremental. Specifically, your research idea should have a concrete question that you're aiming to answer. List it below. \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUYnm6526Z-"
      },
      "source": [
        "- The idea to show that GNN is overparameterized is really cool. However, I think it would be interesting to see if such overparameterization holds for other type of question answering, i.e. question answering for non-multiple choice questions. I think this could provide insights into how effective the regular GNN's are for this more general questions and if GSC is enough for this. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqqeDtxY26Z-"
      },
      "source": [
        "<div class='header_green'>\n",
        "\n",
        "    \n",
        "# 4. RESEARCH PAPER 2 [16 points]\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIThgepH26Z-"
      },
      "source": [
        "And another one! -- DJ Khaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERBoNH-v26Z-"
      },
      "source": [
        "<div class='q_green'><b>4.1: Read an NLP research paper [0 points]</b>\n",
        "    \n",
        "Select and read a paper that was published in ACL, NAACL, EMNLP, or COLING in 2020 or 2021. You can find a list of such published papers by searching Google, a la \"ACL 2020 accepted papers\". For this assignment, you are allowed to pick either a short paper (4-5 pages) or long paper (8-9 pages), **but you must not select a workshop paper**. List below the name of the paper, authors, venue, and year published.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHdw0DpV26Z-"
      },
      "source": [
        "The title of the paper is \"Enhancing multiple choice question answering with causal knowledge\" written by Dhairya Dalal, Mihael Arcan, and Paul Buitelaar. The paper was published in ACL in 2021."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ENVmm0N26Z-"
      },
      "source": [
        "<div class='q_green'><b>4.2: Problem? [2 point]</b> What is the problem that it is trying to address? In other words, what is it trying to solve? (2-3 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQoGXaSx26Z_"
      },
      "source": [
        "The paper explores the effectiveness of augmenting pretrained language models with external causal knowledge for multiple-choice causal question answering. In particular, the authors prsent new strategies for representing the causal knowledge and augenting pretrained models with external causal knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4jzPwQh26Z_"
      },
      "source": [
        "\n",
        "<div class='q_green'><b>4.3: Solution? [2 point]</b> At a very high-level, what was their solution? (2-3 sentences). Here, you don't have room to go into the small details (e.g., about the model), so you'll need to summarize the most important elements that comprised the solution.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XISW8z2926Z_"
      },
      "source": [
        "The paper extracts causal facts from CauseNet dataset, it creates tuples by extracting a lits of tokens $T$ from the input question/premise. Then the authors generate a list of potential causal fact candidates that are filtered once. Finally, they create a table that can be queried to return all candidate facts where both cause and effects exist as tokens. Then the authors present ways to do causal knowledge representations: Distributed Causal Embeddings and Causal Knowledge Graph Embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwCY5em626Z_"
      },
      "source": [
        "<div class='q_green'><b>4.4: Data? [2 points]</b> What dataset(s) did they use? Are they freely available? What's the size of the data? (2-3 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJU8YoOP26Z_"
      },
      "source": [
        "The authors use CauseNet that is a causality graph extracted from the web, publicly available with more than 11 million causal relations. Additionally, the authors use the COPA (Choice of Plausible Alternatives) dataset which has more than multiple choice 1000 questions and is publicly available. And, the authors also use the WIQA (What-IF Question Answering) dataset whih has 39706 questions containing a perturbation and a possible effect in the context of a paragraph, it is also publicly available. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwjyNmgN26Z_"
      },
      "source": [
        "<div class='q_green'><b>4.5: Model [2 points]</b> Very related to the 'solutions' question, describe here any models that they used, and what made it effective (2-3 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNp_Df9q26Z_"
      },
      "source": [
        "The authors present causal knowledge representation models to solve their problem. In particular, they describe a method for modelling causality using a distrbituional similarity model. Two embeddings are learned fo cause and effect concepts respectively, where their model learns embeddings fo reach single token representation of caus and effect concepts and finally get one single representation using mean pool between cause and effect vectors. The authors also present a mehtod to represent causal knowledge as a knowledge graph embedding. The authors represent entities and relations in a lower-dimensional space such that the head, the relation and tail satisfy the dimensionality: $h + r = t$. Once we get an embedding for the head, relation and tail, we get a single vector by mean pooling these vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYTrI2M826Z_"
      },
      "source": [
        "<div class='q_green'><b>4.6: Results? [2 points]</b> What are their main results? (~2 sentences)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038qPIjD26aA"
      },
      "source": [
        "The authors used RoBERTa as a baseline and they get a 4% increase in the COPA dataset using the distributional similarity model and a 6.20% using the causal knowledge graph representation. In the WIQA dataset, the authors also get a 7% incrase using causal knowledge graph represenation but not better results using distributional similarity model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHHkGExb26aA"
      },
      "source": [
        "<div class='q_green'><b>4.7: Strengths? [2 points]</b> List 2-3 strengths of the paper\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJzX5xTP26aA"
      },
      "source": [
        "- The paper makes a strong case for leveraging causal knowledge sources for causal question answering.\n",
        "- I think it is a strong suit of the paper to consider different ways of using the causal knowledge representations. This provide substantial footing to use causal knowledge graphs and GNN in the future.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmrbpKr726aA"
      },
      "source": [
        "<div class='q_green'><b>4.8: Weaknesses? [2 points]</b> Although you may be new to this problem and all of its content, try to list 2-3 weaknesses of the paper (anything that you think could strengthen the paper is sufficient).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE9obbLb26aA"
      },
      "source": [
        "- It is not quite clear, I had to spend quit a bit of time to figure it out, how the authors are forming causal relationships without using the knowledge graph. That could have been explained better.\n",
        "- From my understanding in the field, and given the recency of the paper, the authors could have used other knowledge graphs that seem to be more established, e.g. ConceptNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKWdM2-Q26aA"
      },
      "source": [
        "<div class='q_green'><b>4.9: Evaluation [2 points]</b>\n",
        "    \n",
        "How would you evaluate this paper in terms of:\n",
        "- scientific contribution\n",
        "- effectiveness to solve the problem\n",
        "- how convincing it was.\n",
        "    \n",
        "Give each of these elements a score from 1-10 (10 is best). No word explanation necessary.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0vK85O26aA"
      },
      "source": [
        "- scientific contribution: 8\n",
        "- effectiveness to solve the problem: 7\n",
        "- how convincing it was: 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTq2qqQW26aA"
      },
      "source": [
        "<div class='q_green'><b>4.10: Research Ideas [4 points]</b>\n",
        "    \n",
        "Think of 1-2 research ideas that you have based on this paper. It doesn't have to be grand; most research is very incremental. Specifically, your research idea should have a concrete question that you're aiming to answer. List it below. \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYIQVejf26aA"
      },
      "source": [
        "- We could try to test how it affects the source of knowledge graph in answering the questions in a same dataset. IN particular, for COPA and WIQA, we can change the knowledge base fro CauseNet to ConceptNet and see how the results change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_3pIlEh26aA"
      },
      "source": [
        "<div class='q_green'><b>BONUS POINTS [5 points]</b> I mention the full details in the syllabus on the course website. However, in short, these bonus points cannot bring one's grade to exceed 100. That is, if someone received a 97 on this homework, doing this bonus could allow their grade to reach 100 points. If the person had an 83 on the homework, then the most they could achieve is an 88.\n",
        "    \n",
        "The task: read another research paper -- meaning, a total of 3 papers. It's allowed to be a Short Paper. Write answers to the same questions again. Please copy and paste all of the questions below.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2air5RT26aB"
      },
      "source": [
        "<div class='q_green'><b>E.1: Read an NLP research paper [0 points]</b>\n",
        "\n",
        "The last paper I am reading is: \"Multi-Scale Progressive Attention Network for Vieo Question Answering\" by Zhicheng Guo, Jiaxuan Zhao, Licheng Jiao, Xu Liu, Lingling Li, it was published in ACL in 2021.\n",
        "\n",
        "<div class='q_green'><b>E.2: Problem? [2 point]</b> What is the problem that it is trying to address? In other words, what is it trying to solve? (2-3 sentences)\n",
        "</div>\n",
        "\n",
        "The authros are trying to solve the problem of answering questions from videos. IN particular, they want to answer how can we use video information to answer specific questions related to the video.\n",
        "\n",
        "<div class='q_green'><b>E.3: Solution? [2 point]</b> At a very high-level, what was their solution? (2-3 sentences). Here, you don't have room to go into the small details (e.g., about the model), so you'll need to summarize the most important elements that comprised the solution.\n",
        "</div>\n",
        "\n",
        "The authors proposed a new network Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information. In particular, the authors generate clip-level features from the video oand generate a graph from each time step of clips. Then apply GNN to extract features used for answering the question.\n",
        "\n",
        "<div class='q_green'><b>E.4: Data? [2 points]</b> What dataset(s) did they use? Are they freely available? What's the size of the data? (2-3 sentences)\n",
        "</div>\n",
        "\n",
        "The authors used the datasets TGIF_QA which is a large-scale benchmark dataset for VideoQA. This dataset is publicly availabe and contains 165000 QA pairs for animated GIFs. Also, the authors used the MSVD-QA and MSRVTT-QA tasks which are generated from video descriptions.  \n",
        "\n",
        "<div class='q_green'><b>E.5: Model [2 points]</b> Very related to the 'solutions' question, describe here any models that they used, and what made it effective (2-3 sentences)\n",
        "</div>\n",
        "\n",
        "The proposed model has two parts. First the authors obtain video representations using ResNet networks to extract features from the video and to extract video motion features. Also, they generate a question embedding representation using a BiLSTM network. Afterwards, the authors use max-pool layers with different kernel sizes to generate multi-scale graphs. On top of each graphs, the authors run a GCN and combine the results with the question embedding. Finally, they use a decoder to get the answer to the question.\n",
        "\n",
        "<div class='q_green'><b>E.6: Results? [2 points]</b> What are their main results? (~2 sentences)\n",
        "</div>\n",
        "\n",
        "MSPAn produces state of the art methods in terms of action questions, count questions, FrameQA and transitions quesitons on the TGIF_QA dataset. Similarly the gate SOTA resutls fro most groups of questions for the MSVD-QA and MSRVT-QA datasets.\n",
        "\n",
        "<div class='q_green'><b>E.7: Strengths? [2 points]</b> List 2-3 strengths of the paper\n",
        "</div>\n",
        "\n",
        "- The explanation of the network is very clear and although it could be complicated the authors made a great job in putting that clear.\n",
        "- The authors were able to effectively use GNNs for their network. I think looking at the video scales through the lens of GNNs could open the door for future research in this direction.\n",
        "\n",
        "<div class='q_green'><b>E.8: Weaknesses? [2 points]</b> Although you may be new to this problem and all of its content, try to list 2-3 weaknesses of the paper (anything that you think could strengthen the paper is sufficient).\n",
        "</div>\n",
        "\n",
        "- Although the authors had a good explanation of how to use GNN's in their particular application. It was hard for me to understand why or how they got to the conclusion that they can use these networks.\n",
        "- The authors could have put an appendix to explain their model in detail. Is a short fault, but I think there could have been space for writing a few more of the math and details about how the model was designed.\n",
        "\n",
        "<div class='q_green'><b>E.9: Evaluation [2 points]</b>\n",
        "\n",
        "- scientific contribution: 8\n",
        "- effectiveness to solve the problem: 8\n",
        "- how convincing it was: 9\n",
        "\n",
        "<div class='q_green'><b>E.10: Research Ideas [4 points]</b>\n",
        "    \n",
        "Think of 1-2 research ideas that you have based on this paper. It doesn't have to be grand; most research is very incremental. Specifically, your research idea should have a concrete question that you're aiming to answer. List it below. \n",
        "</div>\n",
        "\n",
        "- It would be interesting to see if we can extract richer information and surass the results if we use other type of graph neural network architectures. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b71GzMD26aB"
      },
      "source": [
        "<div class='header_blue'>\n",
        "    \n",
        "# 5. SELF-REFLECTION [0 points]\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BksMn5Gv26aB"
      },
      "source": [
        "<div class='q_blue'><b>5.1: Self-reflection and Feedback [0 points]</b>\n",
        "\n",
        "Are you thriving in the course? Are there elements that are particularly confusing to you? I want everyone to be and feel fully supported. Toward this, I strongly urge you all to think critically about your own learning and efforts. Please provide us with feedback about how you're doing in the course and if there's anything further or different we can do to better assist your learning. I want everyone to give their earnest account, so the form is completely anonymous.\n",
        "\n",
        "</div>\n",
        "\n",
        "[Anonymous Self-Reflection and Feedback Form](https://forms.gle/3LT6UfhtCtqp2G7X9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uDSJ5zaN26aB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}