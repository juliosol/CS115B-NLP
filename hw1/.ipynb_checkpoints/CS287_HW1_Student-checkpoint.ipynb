{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "\n",
       "hr {\n",
       "    height: 1px;\n",
       "    background-color: black;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "div.quote {\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12px;\n",
       "\talign-items: center;\n",
       "\tmax-width: 80%;\n",
       "\ttext-align: center;\n",
       "}\n",
       "\n",
       "div.header_purple {\n",
       "\tbackground-color: #D0C7FF; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_purple {\n",
       "\tbackground-color: #9183D9;\n",
       "\tborder-color: #FF8484;\n",
       "\tborder-left: 5px solid #FF8484; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_pink {\n",
       "\tbackground-color: #FFC8C8;\n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_pink {\n",
       "\tbackground-color: #FFC8C8;\n",
       "\tborder-color: #FF8484;\n",
       "\tborder-left: 5px solid #FF8484; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_yellow {\n",
       "\tbackground-color: #FDFFB6; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_yellow {\n",
       "\tbackground-color: #FDFFD0;\n",
       "\tborder-color: #FFD6A5;\n",
       "\tborder-left: 5px solid #FFD6A5; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_orange {\n",
       "\tbackground-color: #FFD6A5; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_orange {\n",
       "\tbackground-color: #FFD6A5;\n",
       "\tborder-color: #D6562C;\n",
       "\tborder-left: 5px solid #D6562C; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_green {\n",
       "\tbackground-color: #CAFFBF; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_green {\n",
       "\tbackground-color: #CAFFBF;\n",
       "\tborder-color: #98C98E;\n",
       "\tborder-left: 5px solid #98C98E; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "div.header_blue {\n",
       "\tbackground-color: #C9FAFF; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_blue {\n",
       "\tbackground-color: #C9FAFF;\n",
       "\tborder-color: #A0C4FF;\n",
       "\tborder-left: 5px solid #A0C4FF; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "h1 {\n",
       "    text-align: left; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 { \n",
       "    text-align: left; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "#        RUN THIS CELL\n",
    "################################\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"style.css\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "<div class='header_orange'>\n",
    "\n",
    "# <img style=\"float: left; padding-right: 10px; width: 60px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295/CS287/E-115B: Deep Learning for NLP\n",
    "\n",
    "<br/>\n",
    "<hr color=black>\n",
    "\n",
    "## Homework 1: Language Representations and Modelling\n",
    "### THE ORANGE BOOK\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructor**: Chris Tanner<br/>\n",
    "**Release Date**: September 7 (Tues)<br/>\n",
    "<font color=\"red\">**Due Date**: Sept 20 (Mon) @ 11:59pm (EST)</font>\n",
    "\n",
    "<hr color=black>\n",
    "<center>\n",
    "<div class='quote'>\n",
    "\n",
    "_\"A couple of years ago, on Headland and Delowe, was the start of something good.\"_\n",
    "\n",
    "    André Benjamin (July 9, 1996)\n",
    "</div>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='header_orange'>\n",
    "    \n",
    "# OVERVIEW\n",
    "\n",
    "</div>\n",
    "<br/>\n",
    "This assignment spans the content covered in the following lectures:\n",
    "\n",
    "- **Lecture 1:** Course Overview + Intro to NLP\n",
    "- **Lecture 2:** Representations: BoW and TF-IDF \n",
    "- **Lecture 3:** Language Models: n-grams\n",
    "- **Lecture 4:** word2vec\n",
    "\n",
    "You will gain experience with some of the simple yet highly powerful, foundational techniques that dominated NLP before the recent Deep Learning revolution. All of the core concepts behind these earlier approaches (e.g., `BoW`, `TF-IDF`, and `n-grams`) are still relevant and strongly intertwined within the modern, deep learning advances. For example, learning how to (a) _represent_ language on a fine-grain scale (e.g., words) and coarse-grain (e.g., documents); and (b) _model_ language are still the crux of all NLP systems -- but using deep neural networks to do so currently yield the best results to date. This material serves as the backbone for all future work in the course. Moreover, `word2vec` (2013) provides a segue to deep learning, as it was the first, revolutionary neural model in NLP that yielded profound results. It was the catalyst for the deep learning era, and it opened everyone's eyes to the incredible potential of using neural networks for language... again.\n",
    "\n",
    "**Disclaimer:** The dataset that we use for this assignment is a random subset of news articles. These articles may include content that is offensive, disturbing, or insensitive. We apologize if the nature of any of these documents is unsettling to you. Our intent is purely to use real-world data for pedagogical purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='header_orange'>\n",
    "    \n",
    "# LEARNING OBJECTIVES\n",
    "\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "The purpose of this homework is to help you:\n",
    "\n",
    "- understand how language can be represented (on a coarse-grained document level and fine-grained word level)\n",
    "- understand the strengths and weaknesses of bag-of-words representations\n",
    "- develop a strong foundation in language _modelling,_ via `n-grams`\n",
    "- discover the profound power of distributed, contextualized word representations (i.e., word embeddings)\n",
    "- deeply understand the previous point by writing from scratch the revolutionary `word2vec` model\n",
    "- gain some initial experience with NLP research by critically reading and summarizing a recent research paper from a top NLP conference.\n",
    "\n",
    "To assist you reach these learning objectives, this homework is structured into three parts:\n",
    "- <span style=\"background-color: #FDFFB6\"><b>Foundation (concepts):</b></span> demonstrate an understanding of the core concepts taught in lectures\n",
    "- <span style=\"background-color: #FFC8C8\"><b>Application (programming):</b></span> gain experience putting that knowledge into practice \n",
    "- <span style=\"background-color: #CAFFBF\"><b>Research (creating new knowledge):</b></span> use your current NLP knowledge and skills to go beyond the course material, to grasp cutting-edge results and to critically accept or challenge that information. This serves as practice for you to research your own NLP interests and to be well-equipped to continuously learn the latest, greatest NLP work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='header_orange'>\n",
    "    \n",
    "## SUPPORT\n",
    "\n",
    "</div>\n",
    "\n",
    "- **Supplemental Resources:** See the list of [supplemental resources](https://harvard-iacs.github.io/CS287/supplemental) for a wealth of rich information concerning Machine Learning, NLP, and Math. Some of the courses listed concern the exact topics covered in this homework and lectures.\n",
    "- **Sanity Check cells:** We provide several 'sanity check' cells which allow you to see our expected outputs. You should ensure your code produces the same. <span style=\"background-color: #FDFFB6\"><b>**NOTE:** We are not claiming that passing the sanity check cells indicates that you have _fully_ implemented everything correctly; rather, they provide simple checks to help inform you if you are on the right track.</span>\n",
    "- **Ed**: If you are stuck on anything conceptual (not code) about the content from lectures, please post a question on Ed. This is your community, and please contribute and help each other out. If your questions concern the homework, you can post these on Ed, too, but make sure you are not posting any of your code or solutions in general. If you think you've spotted a bug in our homework questions, or something that needs clarifying, please let us know on Ed! We want to correct these issues ASAP.\n",
    "- **OH:** After having given a wholehearted attempt, if you are having trouble with the homework, please come to Office Hours.\n",
    "- **Classmates:** We have a strict policy about the homeworks being individual. You are free to discuss _concepts_ with one another, to help each other learn the material. However, no student shall ever discuss their solutions or see another student's solutions to any problem. Once you see someone's coding solution, it's nearly impossible to harness that information in a way that you can write your own unique solution. You've been robbed of a learning opportunity and will likely just regurgitate someone else's work. As a reminder, if you want to take a shortcut on any problem by looking online for already-existing solutions, that's permissible, but you must cite your sources. Otherwise, it constitutes cheating. Posting any pieces of this homework online for others to see if a flagrant violation of our academic policy.\n",
    "- **Other:** I want everyone to be and feel fully supported. If there's anything else we, as a teaching staff, can do to further assist in your learning, please let us know. Related, at the end of this homework assignment, you are expected to complete an anonymous feedback form. I urge you to critically and earnestly think about your own learning, communicate to us your thoughts, and to optionally tell us possible adjustments we could make so that you meet our learning expectations and you achieve your own learning goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='header_yellow'>\n",
    "    \n",
    "# 1. FOUNDATION (CONCEPTS) [10 points]\n",
    "\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_yellow'><b>1.1. BoW [2 points]</b>\n",
    "\n",
    "Let's say we are building a spam classifier, which simply predicts if each email is either spam or not spam. It doesn't matter what our exact model is, but our data is represented by **boolean, bag-of-words (BoW) vector representations**. Specifically, each email is represented as a boolean vector whose length is the size of our vocabulary. Each index of the vector corresponds to a particular word, and it has a value of 1 if the word is present in the input data, and 0 if the word is not present. Our model is able to achieve an accuracy of $S_{1}$.\n",
    "\n",
    "Now, if we change our data's representation such that we simply first remove all stopwords (e.g., the top 1% of the most frequent words, such as 'the', 'and', 'for'), our input vectors will naturally be 1% shorter, as their vocabulary is slightly smaller. Using this data on the aforementioned model yields an accuracy of $S_{2}$.\n",
    "\n",
    "What can you say about the scores $S_{1}$ and $S_{2}$? Which one should be higher (better)? Why?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_yellow'><b>1.2. TF-IDF [2 points]</b>\n",
    "\n",
    "Let's say we have a corpus of 100,000 documents (academic research papers), and we're hoping to build an app that allows researchers to quickly see which papers are most related to others. (This could be used to help inform which papers one ought to cite when writing a paper). The 'relatedness' score is calculated by representing each document as a TF-IDF embedding, and then we compute the cosine similarity score between it and every other document vector. The app thus has a score for every pairwise combination of documents, and these can be visualized on the screen as a fully-connected graph with edges' lengths being shorter for documents that are more similar to one another.\n",
    "\n",
    "Unfortunately, one of the new employees who works on the app's parsing and data ingestion made a mistake: a random, 50% of the time, the system can't correctly parse any text after the Abstract section. So, our corpus has actually been reduced to having 50% of the research papers in its original, long form. Yet, the other 50% of papers only have the Abstract section and no other text.\n",
    "\n",
    "- How would this affect the app's results, in terms of how related it thinks the documents are (visualized as a graph)?\n",
    "- Do you think long documents would have more accurate document \"neighbors\" or would the Abstract-only documents? Explain.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_yellow'><b>1.3. n-grams [2 points]</b>\n",
    "\n",
    "**True/False** \n",
    "    \n",
    "Let $w$ and $w'$ represent word types and $d$ is an end-of-sentence padded corpus. If $n_{w}(d) = 1$, then there exists exactly one $w'$ such that $n_{w,w'}(d) = 1$.\n",
    "\n",
    "Remember, the notation $n_{w}(d)$ represents the number of occurrences of word $w$ in corpus $d$. Similarly, $n_{w,w'}(d)$ represents the number of times the bigram $w,w'$ appeared in corpus $d$.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_yellow'><b>1.4. CBOW [2 points]</b>\n",
    "\n",
    "The CBOW model has two weight matrices:\n",
    "- input layer to the projection layer (V x N)\n",
    "- project layer to the output layer (N X V)\n",
    "\n",
    "What would happen to the results if we tied the weights (i.e., the 1st weight layer and 2nd weight layer are forced to always have the same weights)?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_yellow'><b>1.5. CBOW-Expanded [2 points]</b>\n",
    "\n",
    "In the typical CBOW architecture, the input words are the context words appearing to the left and right of the target word. If content window size = $N$, then the input vector would be of length $2N$ and **not** include the target word. What would happen to the results if we provided the target word in the input, too. That is, the input would be of size $2N+1$, and the center word would be the target word.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR RESPONSE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='header_pink'>\n",
    "    \n",
    "# 2. APPLICATION (PROGRAMMING) [70 points]\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "# Mini-Google™ aka Mooogle\n",
    "In this programming exercise, we will build a very simple, make-shift \"search engine\" called Mini-Google, or Mooogle for short (cows love it). I'm putting \"search engine\" in quotes because clearly a fully-functioning, effective search engine requires a full host of computer science solutions; although NLP and Information Retrieval (IR) serve as the crux for any powerful search engine, one must also make heavy use of databases, distributed systems, security, data structures and algorithms, human-computer interaction (HCI), etc. Nonetheless, here, you will get a flavor of the work by building a barebones system that illustrates the power and limitations of document-level representations (e.g., `BoW` and `TF-IDF`) and basic word-level representations (e.g., `word2vec` embeddings).\n",
    "\n",
    "### DATA\n",
    "\n",
    "For Mooogle, we have created two datasets for you to use:\n",
    "- `CS287_news_full` which contains approximately 1000 documents from June 2016 and serves as the full corpus. The documents come from one of three news sources:\n",
    "    - New York Times articles (politically liberal)\n",
    "    - Reuters (politically neutral)\n",
    "    - Fox \"News\" (politically conservative)\n",
    "- `CS287_news_sample` which contains just 3 documents sampled from the full corpus. This serves as a toy example for you to do sanity checks. The documents consist of just 1 document from each of NYT, APNews, and Fox.\n",
    "\n",
    "### PANDAS\n",
    "In all homework assignments, including this one, you are free to use `Pandas`. You are not required to use it at all, but it has some highly useful functionality, e.g., its `read_csv()` function, `DataFrame` and `Series` data structures, and its ability to quickly filter/sort/edit data (which is particularly helpful when experimenting and exploring your data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.1 Tokenization [4 points total]**\n",
    "\n",
    "</div>\n",
    "\n",
    "The first step in any NLP pipeline is **tokenization**. Remember, tokenization is the process of taking a string of text (e.g. sentence, paragraph, document) and splitting it up into meaningful sub-units (e.g., words). This will allow models to process and understand text in the way that humans do -- i.e. one word at a time.\n",
    "\n",
    "Many popular libraries (e.g. [NLTK](https://www.nltk.org/api/nltk.tokenize.html), [SpaCy](https://spacy.io/api/tokenizer), [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)) have their own tokenization function, which we encourage you to use in future problem sets, as well as your research project. However, for this first problem set, we find it most instructive to write the tokenizer from scratch. This will allow us to develop an appreciation of what takes place \"under the hood\" of these great libraries.\n",
    "\n",
    "Our tokenizer will involve the following three steps:\n",
    "- **Lowercasing**: converts all text to lowercase (e.g. `\"The cat ate the fish.\"` -> `\"the cat ate the fish.\"`).  It will allow models to recognize that the first `\"The\"` in the sentence and the second `\"the\"` in the sentence are actually the same word.  \n",
    "- **Punctuation removal**: deletes punctuation (e.g. `\"the cat ate the fish.\"` -> `\"the cat ate the fish\"`) so that characters such as `\".\"`, `\"!\"`, `\"?\"`, etc. not recognized as part of any word. \n",
    "- **Whitespace-splitting**: takes a string of text and segments out the words as a list of strings based on whitespace (e.g. `\"the cat ate the fish\"` -> `[\"the\", \"cat\", \"ate\", \"the\", \"fish\"]`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start with **lowercasing**. We will use this as an example to show how to complete problems in the programming section of this notebook. In general, when we ask you to write code, you will see a cell like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def lowercase(text: str) -> str:\n",
    "    raise NotImplementedError() # Replace with code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This cell asks you to implement a function named `lowercase()` that takes as input a *string* named `text` and outputs a *string* of lowercased text. If you're not already familiar, you can learn about _typing_ in Python [here](https://docs.python.org/3/library/typing.html). We will _type_ all of the functions we ask you to implement so that you know exactly what goes in and what needs to come out. Your job is to replace `raise NotImplementedError()` with code that accomplishes the desired task. For example, one solution may re-write the above cell as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def lowercase(text: str) -> str:\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we can test our function by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat ate the fish.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation cell\n",
    "lowercase('The cat ate the fish.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.1a Tokenization: Punctuation Removal [1 point]**\n",
    "\n",
    "Now, implement the **punctuation removal** function below. The function accepts two inputs:\n",
    "- the input string `text`, which is the data from which you will remove punctuation\n",
    "- a `punc` string, which contains punctuation characters all concatenated together, each of which we want to remove from `text` (**Hint:** you might find Python's built-in functions [`str.replace`](https://www.w3schools.com/python/ref_string_replace.asp), [`str.translate`](https://www.w3schools.com/python/ref_string_translate.asp), or [`ord`](https://www.w3schools.com/python/ref_func_ord.asp) to be helpful)\n",
    "\n",
    "Your function should return `text` with the punctuation removed. **NOTE:** This function has nothing to do with lowercasing, so please do not change the casing of your input text. And, as you can see from the sanity check, you should be _removing_ punctuation, not _replacing_ it with whitespaces.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_punc(text: str, punc: str) -> str:\n",
    "    for ele in punc:\n",
    "        text = text.replace(ele, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As a sanity check, run the test cell below, which should output 'the cat ate the fish'. **Remember:** we are not claiming that passing the following test indicates that you have _fully_ implemented everything correctly; rather, it's a simple check to help inform you if you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the cat ate the fish that's wild\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check cell\n",
    "punc = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~—“”'\n",
    "\n",
    "# should return 'the cat ate the fish that's wild'\n",
    "remove_punc('the cat ate the fish?!! that\\'s wild!', punc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that your code is correct, simply run the evaluation cell below to remove the punctuation from a more difficult string of text. **NOTE: DO NOT EDIT THE CELL BELOW.** You will be evaluated on the correctness of the cell's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Passwords they say must include a special character like  or  but I think  is not allowed right'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation cell\n",
    "punc = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~—“”'\n",
    "remove_punc('Passwords, they say, must include a special character like !#$ or & but I think * is not allowed, right?!', punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.1b Tokenization: Whitespace Splitting [1 point]**\n",
    "    \n",
    "OK, now let's try **whitespace-splitting**. Implement the following `split_on_whitespace()` function which simply takes as input a `text` string and outputs a list of words that have been delimited by _whitespace_. Note, this should handle any whitespace, including tabs, carriage returns, newline characters, and spaces.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def split_on_whitespace(text: str) -> List[str]:\n",
    "    text = text.lstrip()\n",
    "    text = text.rstrip()\n",
    "    if \"\\n \" in text:\n",
    "        text = text.replace(\"\\n \", \" \")\n",
    "    else:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "    if \"\\n \" in text:\n",
    "        text = text.replace(\"\\t \", \" \")\n",
    "    else:\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "    if \"\\n \" in text:\n",
    "        text = text.replace(\"\\r \", \" \")\n",
    "    else:\n",
    "        text = text.replace(\"\\r\", \" \")\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    text = text.split(\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As a sanity check, run the test cell below, which should output `['the', 'cat', 'ate', 'the', 'fish']`. **Remember:** we are not claiming that passing the following test indicates that you have _fully_ implemented everything correctly; rather, it's a simple check to help inform you if you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'ate', 'the', 'fish.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check cell\n",
    "split_on_whitespace(' the\\tcat\\n ate\\rthe     fish.  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that your code is correct, simply run the evaluation cell below to create a vocabulary for the full corpus. **NOTE: DO NOT EDIT THE CELL BELOW.** You will be evaluated on the correctness of the cell's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'goes',\n",
       " 'without',\n",
       " 'saying',\n",
       " 'that',\n",
       " 'regular',\n",
       " 'maintenance',\n",
       " 'is',\n",
       " 'key',\n",
       " 'to',\n",
       " 'the',\n",
       " 'good',\n",
       " 'looks',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Old',\n",
       " 'Yard’s',\n",
       " 'most',\n",
       " 'popular',\n",
       " 'resident',\n",
       " 'who',\n",
       " 'is',\n",
       " 'nearing',\n",
       " '140',\n",
       " 'and',\n",
       " 'has',\n",
       " 'endured',\n",
       " 'much',\n",
       " 'at',\n",
       " 'the',\n",
       " 'hands',\n",
       " 'of',\n",
       " 'visitors',\n",
       " 'and',\n",
       " 'vandals.',\n",
       " 'The',\n",
       " 'iconic',\n",
       " 'John',\n",
       " 'Harvard',\n",
       " 'Statue,',\n",
       " 'modeled',\n",
       " 'after',\n",
       " 'a',\n",
       " 'student',\n",
       " 'from',\n",
       " 'the',\n",
       " '19th',\n",
       " 'century',\n",
       " '(not',\n",
       " 'the',\n",
       " '17th-century',\n",
       " 'English',\n",
       " 'minister',\n",
       " 'and',\n",
       " 'generous',\n",
       " 'College',\n",
       " 'benefactor,',\n",
       " 'John',\n",
       " 'Harvard),',\n",
       " 'receives',\n",
       " 'occasional',\n",
       " 'washings',\n",
       " 'by',\n",
       " 'Harvard’s',\n",
       " 'Landscape',\n",
       " 'Services',\n",
       " 'team,',\n",
       " 'but',\n",
       " 'last',\n",
       " 'month',\n",
       " 'he',\n",
       " 'had',\n",
       " 'a',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'statuary',\n",
       " 'spa',\n",
       " 'treatment.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation cell\n",
    "tmp_text = \"It goes without saying that regular maintenance is key to the good looks of the Old Yard’s most popular resident who is nearing 140 and has endured much at the hands of visitors and vandals.  \\\n",
    "\\\n",
    "The iconic John Harvard Statue, modeled after a student from the 19th century (not the 17th-century English minister and generous College benefactor, John Harvard), receives occasional washings by Harvard’s Landscape Services team, but last month he had a kind of statuary spa treatment.\"\n",
    "split_on_whitespace(tmp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.1c Tokenization: The full process [1 point]**\n",
    "\n",
    "Implement the following `tokenize()` function by chaining together the aforementioned three steps -- lowercase, punctuation removal, and whitespace-splitting -- and returns the list of string tokens. You can make use of the functions you've already written, if you wish. Then, test the function to ensure that it works.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "punc = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~—“”'\n",
    "def tokenize(text: str, punc: str) -> List[str]:\n",
    "    text = lowercase(text)\n",
    "    text = remove_punc(text, punc)\n",
    "    text = split_on_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As a sanity check, run the test cell below, which should output `['the', 'cat', 'ate', 'the', 'fish']`. \n",
    "\n",
    "**Remember:** we are not claiming that passing the following test indicates that you have _fully_ implemented everything correctly; rather, it's a simple check to help inform you if you are on the right track. <span style=\"background-color: #FDFFB6\">So as to minimize the annoyance and extraneous text, I will refrain from reminding you of this disclaimer on the subsequent \"sanity check\" cells.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'ate', 'the', 'fish']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check cell\n",
    "tokenize('The cat\\t\\tate the fish?!', punc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Run the evaluation cell below to test your `tokenize()` function. **NOTE: DO NOT EDIT THE CELL BELOW.** You will be evaluated on the correctness of the cell's outputs. <span style=\"background-color: #FDFFB6\">Similarly, all \"evaluation cells\" serve this purpose, and I will now refrain from describing such on the subsequent \"evaluation\" cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['it', 'goes', 'without', 'saying', 'that', 'regular', 'maintenance', 'is', 'key', 'to', 'the', 'good', 'looks', 'of', 'the', 'old', 'yard’s', 'most', 'popular', 'resident', 'who', 'is', 'nearing', '140', 'and', 'has', 'endured', 'much', 'at', 'the', 'hands', 'of', 'visitors', 'and', 'vandals', 'the', 'iconic', 'john', 'harvard', 'statue', 'modeled', 'after', 'a', 'student', 'from', 'the', '19th', 'century', 'not', 'the', '17thcentury', 'english', 'minister', 'and', 'generous', 'college', 'benefactor', 'john', 'harvard', 'receives', 'occasional', 'washings', 'by', 'harvard’s', 'landscape', 'services', 'team', 'but', 'last', 'month', 'he', 'had', 'a', 'kind', 'of', 'statuary', 'spa', 'treatment']\n",
      "# tokens: 78\n",
      "contains check #3: True\n",
      "contains check #1: True\n",
      "contains check #2: False\n",
      "contains check #4: False\n",
      "contains check #5: False\n"
     ]
    }
   ],
   "source": [
    "tmp_text = \"It goes without saying that regular maintenance is key to the good looks of the Old Yard’s most popular resident who is nearing 140 and has endured much at the hands of visitors and vandals.  \\\n",
    "\\\n",
    "The iconic John Harvard Statue, modeled after a student from the 19th century (not the 17th-century English minister and generous College benefactor, John Harvard), receives occasional washings by Harvard’s Landscape Services team, but last month he had a kind of statuary spa treatment.\"\n",
    "\n",
    "tokens = tokenize(tmp_text, punc)\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"# tokens:\", len(tokens))\n",
    "print(\"contains check #3:\", \"yard’s\" in tokens)\n",
    "print(\"contains check #1:\", \"17thcentury\" in tokens)\n",
    "print(\"contains check #2:\", \"17th\" in tokens)\n",
    "print(\"contains check #4:\", \"It\" in tokens)\n",
    "print(\"contains check #5:\", \" statue,\" in tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Above, you should see that `tmp_text` contains a funky apostrophe, `’` that we did not include in our punctuation list. This was deliberate, as it illustrates how it can be difficult to anticipate all of the nuanced characters and situations that one may encounter in raw text. This is exacerbated by having multiple character encodings. Thus, in practice, it can take a long time to write from scratch a parser that perfectly parses (e.g., tokenizes) your desired input data. As mentioned, fortunately many popular libraries (e.g. [NLTK](https://www.nltk.org/api/nltk.tokenize.html), [SpaCy](https://spacy.io/api/tokenizer), [Keras](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)) have their own tokenization functions, which often works sufficiently well for one's data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.1d Load Corpus [1 point]**\n",
    "\n",
    "Now, you will use your `tokenize()` function on some real data. Implement the following `load_corpus()` function, which: takes as input (a) the name of a file containing news articles, and (b) a string of punctuation symbols; loads the file's contents into memory; and returns as output a dictionary of tokenized documents. The _key_ of the dictionary should be the document id and the _value_ will be the tokenized list of words in the document.\n",
    "\n",
    "**NOTE:** Below, we import `pandas` for you. You are not required to use `pandas`, but in this homework you might find it highly useful (e.g., for its `read_csv()` function, `DataFrame` and `Series` data structures, etc).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_data_df = pd.read_csv(\"CS287_news_sample.csv\")\n",
    "full_data_df = pd.read_csv(\"CS287_news_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20153</td>\n",
       "      <td>New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20189</td>\n",
       "      <td>New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20240</td>\n",
       "      <td>New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20283</td>\n",
       "      <td>New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20285</td>\n",
       "      <td>New York Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>189773</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>189775</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>189777</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>189778</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>189782</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     publication\n",
       "0      20153  New York Times\n",
       "1      20189  New York Times\n",
       "2      20240  New York Times\n",
       "3      20283  New York Times\n",
       "4      20285  New York Times\n",
       "...      ...             ...\n",
       "1134  189773         Reuters\n",
       "1135  189775         Reuters\n",
       "1136  189777         Reuters\n",
       "1137  189778         Reuters\n",
       "1138  189782         Reuters\n",
       "\n",
       "[1139 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data_df[['id', 'publication']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def load_corpus(filename: str, punc: str) -> Dict[int, List[str]]:\n",
    "    df_documents = pd.read_csv(filename)\n",
    "    token_dict: Dict[float, list] = {}\n",
    "    for row in df_documents.itertuples(index=False):\n",
    "        token_dict[row[0]] = tokenize(row[8], punc)\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As a sanity check, run the cell below to load the `CS287_news_sample` corpus. It should return a dictionary that has three keys: `20621, 84549, 189782`. The value for the key `20621` should be a list of strings, which starts as `['rome', 'among', 'the', 'bureaucrats', ...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "editable": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rome', 'among', 'the', 'bureaucrats', 'of', 'the', 'european', 'union', 'it', 'is', 'an', 'article', 'of', 'faith', 'that', 'the', 'bloc', 'always', 'emerges', 'stronger']\n"
     ]
    }
   ],
   "source": [
    "# sanity check cell\n",
    "samp_tokenized = load_corpus('CS287_news_sample.csv', punc)\n",
    "print(samp_tokenized[20621][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{20621: ['rome', 'among', 'the', 'bureaucrats', 'of', 'the', 'european', 'union', 'it', 'is', 'an', 'article', 'of', 'faith', 'that', 'the', 'bloc', 'always', 'emerges', 'stronger', 'from', 'a', 'crisis', 'the', 'idealistic', 'founders', 'who', 'six', 'decades', 'ago', 'dreamed', 'of', 'stitching', 'warring', 'nations', 'into', 'a', 'peaceful', 'whole', 'knew', 'the', 'path', 'would', 'be', 'bumpy', 'but', 'always', 'the', 'union', 'wobbled', 'forward', 'now', 'the', 'dream', 'of', 'an', 'integrated', 'and', 'europe', 'could', 'sink', 'into', 'the', 'english', 'channel', 'on', 'thursday', 'when', 'british', 'voters', 'decide', 'whether', 'to', 'abandon', 'the', 'bloc', 'to', 'the', 'establishment', 'this', 'latest', 'crisis', 'is', 'considered', 'a', 'peculiarly', 'british', 'affair', 'in', 'which', 'the', 'villains', 'are', 'opportunistic', 'politicians', 'steering', 'voters', 'toward', 'a', 'delusional', 'mistake', 'that', 'may', 'be', 'but', 'if', 'britain', 'does', 'leave', 'the', 'european', 'union', 'can', 'also', 'blame', 'its', 'own', 'handling', 'of', 'the', 'crises', 'of', 'the', 'past', 'decade', 'the', 'tribulations', 'of', 'the', 'euro', 'the', 'debt', 'standoff', 'with', 'greece', 'and', 'a', 'flawed', 'approach', 'to', 'migration', 'each', 'time', 'the', 'bloc', 'rammed', 'through', 'ugly', 'fixes', 'that', 'only', 'inflamed', 'the', 'angry', 'nationalism', 'now', 'spreading', 'across', 'the', 'continent', 'and', 'britain', 'the', 'result', 'was', 'almost', 'a', 'decade', 'of', 'ad', 'hoc', 'crisis', 'management', 'that', 'even', 'many', 'admirers', 'agree', 'has', 'left', 'the', 'european', 'union', 'badly', 'wounded', 'and', 'its', 'reputation', 'badly', 'damaged', 'idealism', 'has', 'given', 'way', 'to', 'disillusionment', 'the', 'bloc’s', 'elite', 'technocrats', 'are', 'often', 'perceived', 'as', 'out', 'of', 'touch', 'while', 'european', 'institutions', 'are', 'not', 'fully', 'equipped', 'to', 'address', 'problems', 'like', 'unemployment', 'and', 'economic', 'stagnation', 'political', 'solidarity', 'is', 'dissolving', 'into', 'regional', 'divisions', 'of', 'east', 'and', 'west', 'north', 'and', 'south', 'the', 'economic', 'implications', 'of', 'a', 'british', 'exit', 'the', 'brexit', 'are', 'potentially', 'staggering', 'but', 'many', 'experts', 'agree', 'that', 'regardless', 'of', 'how', 'the', 'british', 'vote', 'politics', 'across', 'europe', 'must', 'change', 'the', 'structure', 'of', 'the', 'euro', 'currency', 'zone', 'is', 'still', 'considered', 'fragile', 'the', 'bloc’s', 'economic', 'policy', 'has', 'meant', 'nearly', 'a', 'decade', 'lost', 'in', 'much', 'of', 'southern', 'europe', 'which', 'is', 'still', 'struggling', 'to', 'recover', 'from', 'its', 'economic', 'crisis', 'we', 'cannot', 'continue', 'with', 'the', 'status', 'quo', 'said', 'enrico', 'letta', 'a', 'former', 'italian', 'prime', 'minister', 'we', 'have', 'to', 'move', 'forward', 'politics', 'in', 'europe', 'as', 'in', 'the', 'united', 'states', 'have', 'gotten', 'ugly', 'and', 'mean', 'parties', 'are', 'gaining', 'strength', 'in', 'poland', 'hungary', 'austria', 'france', 'and', 'germany', 'that', 'same', 'nasty', 'tenor', 'has', 'infused', 'the', 'british', 'campaign', 'with', 'hostility', 'and', 'xenophobia', 'toward', 'immigrants', 'the', 'killing', 'on', 'thursday', 'of', 'jo', 'cox', 'a', 'member', 'of', 'parliament', 'who', 'had', 'campaigned', 'for', 'remaining', 'in', 'the', 'union', 'shocked', 'all', 'of', 'britain', 'it', 'is', 'not', 'very', 'easy', 'being', 'english', 'at', 'the', 'moment', 'said', 'simon', 'tilford', 'deputy', 'director', 'of', 'the', 'center', 'for', 'european', 'reform', 'in', 'london', 'grim', 'stuff', 'mr', 'tilford', 'falls', 'into', 'an', 'interesting', 'camp', 'he', 'has', 'long', 'been', 'an', 'outspoken', 'critic', 'of', 'the', 'european', 'union’s', 'handling', 'of', 'its', 'currency', 'woes', 'yet', 'he', 'strongly', 'supports', 'britain’s', 'remaining', 'in', 'the', 'bloc', 'the', 'benefits', 'far', 'outweigh', 'the', 'disadvantages', 'he', 'argues', 'even', 'as', 'he', 'realizes', 'that', 'policy', 'failures', 'by', 'the', 'european', 'union', 'have', 'helped', 'legitimize', 'the', 'arguments', 'of', 'some', 'who', 'want', 'to', 'leave', 'it', 'has', 'made', 'it', 'easier', 'for', 'them', 'to', 'portray', 'the', 'e', 'u', 'as', 'a', 'failure', 'he', 'said', 'lots', 'of', 'people', 'have', 'become', 'euroskeptics', 'in', 'britain', 'because', 'they', 'are', 'so', 'angry', 'at', 'what', 'has', 'happened', 'in', 'the', 'eurozone', 'in', 'recent', 'years', 'during', 'the', '1990s', 'britain', 'was', 'already', 'a', 'member', 'of', 'the', 'european', 'union', 'and', 'was', 'considering', 'whether', 'to', 'drop', 'its', 'currency', 'the', 'pound', 'and', 'join', 'the', 'zone', 'of', 'countries', 'adopting', 'the', 'bloc’s', 'new', 'currency', 'the', 'euro', 'today', '19', 'of', 'the', '28', 'countries', 'in', 'the', 'european', 'union', 'share', 'the', 'euro', 'but', 'a', 'crisis', 'in', 'the', 'financial', 'markets', 'in', '1992', 'effectively', 'settled', 'the', 'matter', 'as', 'britain', 'decided', 'not', 'to', 'join', 'other', 'countries', 'changing', 'to', 'the', 'euro', 'those', 'skeptical', 'of', 'using', 'a', 'common', 'currency', 'to', 'drive', 'closer', 'integration', 'in', 'europe', 'have', 'always', 'argued', 'that', 'joining', 'the', 'euro', 'system', 'would', 'limit', 'policy', 'flexibility', 'such', 'as', 'the', 'ability', 'to', 'devalue', 'the', 'national', 'currency', 'during', 'economic', 'downturns', 'or', 'use', 'deficit', 'spending', 'to', 'encourage', 'growth', 'and', 'they', 'said', 'problems', 'would', 'inevitably', 'arise', 'because', 'of', 'the', 'stark', 'economic', 'differences', 'among', 'the', 'countries', 'sharing', 'the', 'currency', 'each', 'proved', 'true', 'the', 'economic', 'crisis', 'in', 'plunged', 'the', 'bloc', 'into', 'a', 'cycle', 'of', 'crises', 'from', 'which', 'it', 'still', 'has', 'not', 'recovered', 'the', 'disparities', 'among', 'eurozone', 'countries', 'were', 'exposed', 'and', 'to', 'save', 'the', 'currency', 'northern', 'countries', 'led', 'by', 'germany', 'bailed', 'out', 'their', 'desperate', 'southern', 'counterparts', 'the', 'eurozone', 'became', 'divided', 'between', 'debtors', 'and', 'creditors', 'rather', 'than', 'equal', 'partners', 'the', 'solution', 'of', 'austerity', 'economics', 'inflicted', 'heavy', 'punishment', 'on', 'countries', 'like', 'greece', 'and', 'britain', 'outside', 'the', 'euro', 'recovered', 'more', 'quickly', 'from', 'the', '2008', 'crisis', 'than', 'did', 'most', 'members', 'of', 'the', 'currency', 'group', 'the', 'politics', 'also', 'shifted', 'germany', 'the', 'bloc’s', 'economic', 'powerhouse', 'steadily', 'accrued', 'more', 'political', 'power', 'inside', 'european', 'union', 'institutions', 'resentment', 'gradually', 'followed', 'especially', 'in', 'greece', 'the', 'weakest', 'and', 'most', 'indebted', 'member', 'of', 'the', 'eurozone', 'populist', 'anger', 'erupted', 'in', 'january', '2015', 'when', 'greek', 'voters', 'swept', 'aside', 'the', 'country’s', 'political', 'establishment', 'and', 'elected', 'as', 'prime', 'minister', 'a', 'radical', 'leftist', 'alexis', 'tsipras', 'mr', 'tsipras', 'promised', 'to', 'end', 'austerity', 'write', 'down', 'greek', 'debt', 'and', 'change', 'europe', 'by', 'leading', 'a', 'clash', 'of', 'ideas', 'against', 'the', 'consensus', 'it', 'was', 'political', 'theater', 'and', 'the', 'greeks', 'would', 'badly', 'misplay', 'their', 'hand', 'but', 'what', 'followed', 'was', 'a', 'clash', 'of', 'cultures', 'not', 'ideas', 'union', 'officials', 'refused', 'to', 'budge', 'on', 'greece’s', 'debt', 'obligations', 'and', 'after', 'months', 'of', 'negotiation', 'and', 'brinkmanship', 'greece', 'nearly', 'collapsed', 'into', 'bankruptcy', 'before', 'acceding', 'to', 'demands', 'from', 'brussels', 'the', 'greek', 'standoff', 'was', 'a', 'demonstration', 'not', 'just', 'of', 'european', 'union', 'power', 'politics', 'but', 'also', 'of', 'the', 'bloc’s', 'penchant', 'for', 'muddling', 'through', 'the', 'bloc', 'agreed', 'on', 'a', 'new', 'bailout', 'package', 'for', 'greece', 'that', 'most', 'analysts', 'regard', 'as', 'a', 'stopgap', 'solution', 'greece’s', 'debt', 'is', 'now', 'higher', 'than', 'before', 'and', 'analysts', 'warn', 'that', 'another', 'euro', 'crisis', 'could', 'still', 'occur', 'there', 'is', 'a', 'lot', 'of', 'criticism', 'of', 'the', 'german', 'way', 'of', 'handling', 'the', 'eurozone', 'said', 'daniela', 'schwarzer', 'the', 'director', 'of', 'the', 'europe', 'program', 'for', 'the', 'german', 'marshall', 'fund', 'in', 'berlin', 'that', 'bleeds', 'into', 'the', 'perception', 'that', 'the', 'european', 'union', 'is', 'not', 'functioning', 'properly', 'it', 'is', 'this', 'perceived', 'ineffectiveness', 'of', 'mainstream', 'political', 'parties', 'at', 'the', 'european', 'and', 'national', 'levels', 'that', 'has', 'emboldened', 'populist', 'or', 'parties', 'from', 'the', 'left', 'and', 'the', 'right', 'last', 'year', 'parties', 'seized', 'on', 'the', 'migration', 'crisis', 'with', 'its', 'images', 'of', 'hundreds', 'of', 'thousands', 'of', 'refugees', 'pouring', 'through', 'europe', 'to', 'provoke', 'public', 'anxiety', 'led', 'by', 'hungary', 'some', 'countries', 'began', 'erecting', 'fences', 'to', 'block', 'migrants', 'despite', 'the', 'european', 'union', 'system', 'of', 'open', 'borders', 'european', 'leaders', 'struggled', 'to', 'mount', 'a', 'coherent', 'response', 'and', 'chancellor', 'angela', 'merkel’s', 'popularity', 'plummeted', 'after', 'she', 'opened', 'germany', 'to', 'syrian', 'refugees', 'she', 'has', 'since', 'tightened', 'restrictions', 'and', 'championed', 'a', 'controversial', 'deal', 'with', 'turkey', 'that', 'has', 'sharply', 'reduced', 'the', 'migrant', 'flow', 'into', 'the', 'continent', 'in', 'the', 'british', 'referendum', 'forces', 'have', 'sought', 'to', 'drive', 'support', 'for', 'the', 'campaign', 'to', 'leave', 'the', 'union', 'by', 'depicting', 'the', 'continent', 'as', 'being', 'under', 'invasion', 'from', 'migrants', 'a', 'campaign', 'poster', 'unveiled', 'last', 'week', 'by', 'nigel', 'farage', 'the', 'u', 'k', 'independence', 'party', 'leader', 'showed', 'a', 'parade', 'of', 'migrants', 'it', 'was', 'pilloried', 'as', 'blatant', 'xenophobia', 'but', 'it', 'is', 'little', 'different', 'from', 'the', 'propaganda', 'of', 'politicians', 'in', 'hungary', 'or', 'poland', 'should', 'it', 'remain', 'in', 'the', 'bloc', 'britain', 'could', 'emerge', 'as', 'a', 'powerful', 'force', 'with', 'more', 'clout', 'to', 'force', 'the', 'changes', 'that', 'most', 'analysts', 'believe', 'are', 'necessary', 'even', 'among', 'top', 'officials', 'there', 'is', 'a', 'growing', 'recognition', 'that', 'europe’s', 'political', 'mainstream', 'has', 'misjudged', 'the', 'public', 'appetite', 'for', 'rapid', 'european', 'integration', 'the', 'specter', 'of', 'a', 'breakup', 'is', 'haunting', 'europe', 'warned', 'donald', 'tusk', 'the', 'president', 'of', 'the', 'european', 'council', 'which', 'comprises', 'the', 'heads', 'of', 'state', 'of', 'the', 'bloc’s', 'members', 'we', 'need', 'to', 'understand', 'the', 'necessity', 'of', 'the', 'historical', 'moment', 'it', 'is', 'easy', 'to', 'forget', 'that', 'the', 'european', 'union', 'is', 'an', 'audacious', 'political', 'experiment', 'to', 'wash', 'away', 'the', 'antagonisms', 'of', 'world', 'war', 'ii', 'and', 'build', 'a', 'new', 'europe', 'unity', 'required', 'putting', 'aside', 'the', 'ancient', 'rivalry', 'between', 'france', 'and', 'germany', 'and', 'binding', 'together', 'countries', 'with', 'different', 'languages', 'cultures', 'and', 'economies', 'generations', 'growing', 'up', 'between', 'the', '1980s', 'and', 'the', '2000s', 'saw', 'how', 'an', 'expanding', 'europe', 'brought', 'tangible', 'benefits', 'borderless', 'travel', 'job', 'and', 'educational', 'mobility', 'within', 'the', 'bloc', 'rising', 'prosperity', 'poorly', 'governed', 'countries', 'came', 'under', 'pressure', 'from', 'brussels', 'to', 'improve', 'but', 'as', 'the', 'bloc', 'expanded', 'also', 'became', 'more', 'unwieldy', 'frictions', 'inevitably', 'arose', 'and', 'old', 'resentments', 'between', 'member', 'states', 'were', 'never', 'fully', 'scrubbed', 'away', 'tensions', 'also', 'increased', 'between', 'european', 'institutions', 'and', 'national', 'governments', 'over', 'sovereignty', 'this', 'has', 'meant', 'little', 'political', 'space', 'for', 'the', 'deep', 'reforms', 'and', 'possible', 'further', 'integration', 'of', 'political', 'powers', 'that', 'some', 'advocates', 'say', 'are', 'needed', 'in', 'brussels', 'many', 'analysts', 'say', 'that', 'even', 'if', 'britain', 'remains', 'in', 'the', 'bloc', 'the', 'likelihood', 'of', 'the', 'union’s', 'undertaking', 'significant', 'political', 'reforms', 'is', 'slim', 'until', 'after', 'the', 'national', 'elections', 'in', 'france', 'and', 'germany', 'in', '2017', 'doing', 'something', 'bold', 'in', 'brussels', 'before', 'then', 'could', 'spur', 'an', 'electoral', 'backlash', 'so', 'the', 'european', 'union', 'will', 'wait', 'to', 'see', 'what', 'britain', 'does', 'and', 'then', 'possibly', 'wait', 'some', 'more', 'what', 'has', 'happened', 'to', 'you', 'the', 'europe', 'of', 'humanism', 'the', 'champion', 'of', 'human', 'rights', 'democracy', 'and', 'freedom', 'pope', 'francis', 'asked', 'last', 'month', 'during', 'his', 'acceptance', 'speech', 'for', 'the', 'charlemagne', 'prize', 'awarded', 'for', 'service', 'toward', 'european', 'unification', 'he', 'added', 'what', 'has', 'happened', 'to', 'you', 'europe', 'the', 'mother', 'of', 'peoples', 'and', 'nations', 'the', 'mother', 'of', 'great', 'men', 'and', 'women', 'who', 'upheld', 'and', 'even', 'sacrificed', 'their', 'lives', 'for', 'the', 'dignity', 'of', 'their', 'brothers', 'and', 'sisters'], 84549: ['house', 'speaker', 'paul', 'ryan', 'said', 'thursday', 'he', 'would', 'vote', 'for', 'presumptive', 'republican', 'presidential', 'nominee', 'donald', 'trump', 'an', 'announcement', 'his', 'office', 'said', 'amounts', 'to', 'an', 'endorsement', 'ryan', 'made', 'the', 'announcement', 'in', 'a', 'column', 'for', 'the', 'janesville', 'gazette', 'the', 'speaker’s', 'hometown', 'paper', 'and', 'also', 'tweeted', 'out', 'his', 'voting', 'preference', 'i’ll', 'be', 'voting', 'for', 'realdonaldtrump', 'this', 'fall', 'i’m', 'confident', 'he', 'will', 'help', 'turn', 'the', 'house', 'gop’s', 'agenda', 'into', 'laws', 'https', 'ryan’s', 'support', 'for', 'trump', 'ends', 'any', 'speculation', 'about', 'whether', 'or', 'not', 'ryan', 'would', 'back', 'the', 'likely', 'republican', 'nominee', 'ryan', 'had', 'held', 'off', 'issuing', 'a', 'formal', 'endorsement', 'or', 'saying', 'he', 'would', 'vote', 'for', 'the', 'business', 'magnate', 'even', 'after', 'trump', 'had', 'appeared', 'to', 'gain', 'the', 'number', 'of', 'delegates', 'needed', 'to', 'win', 'the', 'republican', 'nomination', 'and', 'all', 'of', 'his', 'primary', 'opponents', 'had', 'suspended', 'their', 'campaigns', 'though', 'ryan', 'on', 'thursday', 'seemed', 'to', 'stop', 'short', 'of', 'the', 'endorsement', 'his', 'office', 'clarified', 'that', 'he', 'was', 'in', 'fact', 'getting', 'behind', 'trump', 'we’re', 'not', 'playing', 'word', 'games', 'feel', 'free', 'to', 'call', 'it', 'an', 'endorsement', 'ryan’s', 'chief', 'communications', 'adviser', 'brendan', 'buck', 'tweeted', 'we’re', 'not', 'playing', 'word', 'games', 'feel', 'free', 'to', 'call', 'it', 'an', 'endorsement', 'ryan', 'wrote', 'that', 'conversations', 'he', 'had', 'with', 'trump', 'on', 'various', 'issues', 'including', 'whether', 'a', 'president', 'trump', 'would', 'help', 'enact', 'the', 'house', 'gop', 'agenda', 'helped', 'seal', 'his', 'support', 'the', 'latest', 'headlines', 'on', 'the', '2016', 'elections', 'from', 'the', 'biggest', 'name', 'in', 'politics', 'see', 'latest', 'coverage', '→', 'through', 'these', 'conversations', 'i', 'feel', 'confident', 'he', 'would', 'help', 'us', 'turn', 'the', 'ideas', 'in', 'this', 'agenda', 'into', 'laws', 'to', 'help', 'improve', 'people’s', 'lives', 'ryan', 'wrote', 'that’s', 'why', 'i’ll', 'be', 'voting', 'for', 'him', 'this', 'fall', 'the', 'wisconsin', 'representative', 'acknowledged', 'that', 'he', 'and', 'trump', 'have', 'been', 'at', 'odds', 'in', 'the', 'past', 'it’s', 'no', 'secret', 'that', 'he', 'and', 'i', 'have', 'our', 'differences', 'ryan', 'wrote', 'i', 'won’t', 'pretend', 'otherwise', 'and', 'when', 'i', 'feel', 'the', 'need', 'to', 'i’ll', 'continue', 'to', 'speak', 'my', 'mind', 'but', 'the', 'reality', 'is', 'on', 'the', 'issues', 'that', 'make', 'up', 'our', 'agenda', 'we', 'have', 'more', 'common', 'ground', 'than', 'disagreement', 'ryan', 'had', 'criticized', 'trump', 'during', 'the', 'campaign', 'on', 'a', 'range', 'of', 'statements', 'made', 'by', 'the', 'new', 'york', 'billionaire', 'including', 'a', 'proposed', 'ban', 'on', 'muslims', 'entering', 'the', 'u', 's', 'the', 'two', 'men', 'held', 'a', 'private', 'summit', 'in', 'may', 'in', 'washington', 'but', 'ryan', 'stopped', 'short', 'of', 'backing', 'trump', 'after', 'the', 'meeting', 'but', 'ryan’s', 'support', 'for', 'trump', 'was', 'inevitable', 'fox', 'news', 'was', 'told', 'weeks', 'ago', 'the', 'news', 'comes', 'as', 'ryan', 'begins', 'rolling', 'out', 'a', 'series', 'of', 'six', 'policy', 'initiatives', 'during', 'the', 'next', 'several', 'weeks', 'none', 'of', 'which', 'are', 'expected', 'to', 'be', 'considered', 'in', 'the', 'current', 'congress', 'ryan', 'made', 'his', 'announcement', 'just', 'as', 'likely', 'democratic', 'presidential', 'nominee', 'hillary', 'clinton', 'was', 'about', 'to', 'deliver', 'a', 'foreign', 'policy', 'speech', 'but', 'buck', 'said', 'the', 'timing', 'was', 'merely', 'happenstance', 'and', 'no', 'while', 'a', 'fun', 'coincidence', 'it', 'wasn’t', 'timed', 'to', 'a', 'hillary', 'clinton', 'speech', 'buck', 'tweeted', 'believe', 'it', 'or', 'not', 'we', 'don’t', 'follow', 'her', 'sched', 'that', 'closely', 'an', 'aide', 'to', 'ryan', 'tweeted', 'that', 'a', 'video', 'would', 'be', 'forthcoming', 'of', 'ryan', 'talking', 'about', 'his', 'support', 'for', 'trump', 'fox', 'news’', 'chad', 'pergram', 'contributed', 'to', 'this', 'report'], 189782: ['in', 'the', 'meantime', 'health', 'officials', 'have', 'warned', 'couples', 'to', 'refrain', 'from', 'unprotected', 'sex', 'for', 'six', 'months', 'after', 'a', 'male', 'partner', 'is', 'infected', 'the', 'extraordinary', 'recommendation', 'based', 'on', 'a', 'single', 'report', 'of', 'zika', 'surviving', '62', 'days', 'in', 'semen', 'could', 'affect', 'millions', 'the', 'grave', 'risks', 'associated', 'with', 'zika', 'along', 'with', 'its', 'potential', 'reach', 'are', 'driving', 'u', 's', 'health', 'authorities', 'to', 'pursue', 'research', 'even', 'though', 'funding', 'is', 'mired', 'in', 'congressional', 'gridlock', 'a', 'study', 'of', 'sexual', 'transmission', 'risk', 'is', 'one', 'example', 'of', 'science', 'that', 'health', 'officials', 'said', 'can’t', 'wait', 'for', 'politics', 'borrowing', 'money', 'earmarked', 'for', 'other', 'programs', 'the', 'u', 's', 'national', 'institute', 'of', 'allergy', 'and', 'infectious', 'diseases', 'has', 'started', 'enrolling', 'men', 'infected', 'with', 'zika', 'in', 'brazil', 'and', 'colombia', 'in', 'the', 'study', 'to', 'determine', 'how', 'long', 'the', 'virus', 'remains', 'transmittable', 'in', 'semen', 'the', 'study', 'could', 'take', 'years', 'to', 'complete', 'but', 'interim', 'results', 'could', 'help', 'public', 'health', 'officials', 'their', 'recommendations', 'on', 'sex', 'we', 'are', 'going', 'out', 'on', 'a', 'limb', 'but', 'we', 'have', 'to', 'dr', 'anthony', 'fauci', 'director', 'of', 'the', 'u', 's', 'institute', 'said', 'in', 'an', 'interview', 'we', 'can’t', 'say', 'we’re', 'going', 'to', 'wait', 'until', 'we', 'get', 'all', 'the', 'money', 'public', 'health', 'officials', 'are', 'alarmed', 'by', 'zika’s', 'transmission', 'versatility', 'which', 'has', 'the', 'potential', 'to', 'expand', 'its', 'reach', 'it', 'is', 'primarily', 'spread', 'by', 'aedes', 'aegypti', 'mosquitoes', 'as', 'are', 'the', 'dengue', 'and', 'chikungunya', 'viruses', 'but', 'at', 'least', '10', 'countries', 'including', 'the', 'united', 'states', 'and', 'france', 'have', 'reported', 'zika', 'infections', 'in', 'people', 'who', 'had', 'not', 'traveled', 'to', 'an', 'outbreak', 'area', 'but', 'whose', 'sexual', 'partners', 'had', 'this', 'ability', 'to', 'spread', 'through', 'sex', 'could', 'help', 'zika', 'gain', 'a', 'further', 'foothold', 'outside', 'the', 'warm', 'habitats', 'of', 'its', 'most', 'effective', 'agent', 'the', 'mosquito', 'caution', 'in', 'lieu', 'of', 'answers', 'to', 'protect', 'women', 'who', 'are', 'pregnant', 'or', 'trying', 'to', 'conceive', 'the', 'u', 's', 'centers', 'for', 'disease', 'control', 'and', 'prevention', 'recommended', 'couples', 'refrain', 'from', 'unprotected', 'sex', 'for', 'six', 'months', 'triple', 'the', '62', 'days', 'the', 'virus', 'survived', 'in', 'the', 'semen', 'in', 'one', 'british', 'case', 'study', 'the', 'world', 'health', 'organization', 'recently', 'issued', 'similar', 'guidance', 'but', 'such', 'strict', 'advice', 'is', 'not', 'ideal', 'dr', 'anne', 'schuchat', 'a', 'cdc', 'deputy', 'director', 'said', 'in', 'an', 'interview', 'to', 'tell', 'people', 'not', 'to', 'have', 'sex', 'until', 'we', 'get', 'back', 'to', 'you', 'is', 'not', 'a', 'very', 'satisfying', 'recommendation', 'she', 'said', 'we', 'would', 'like', 'to', 'have', 'some', 'more', 'understanding', 'of', 'the', 'sexual', 'risk', 'in', 'the', 'u', 's', 'territory', 'of', 'puerto', 'rico', 'where', 'more', 'than', '2', '100', 'cases', 'of', 'infection', 'have', 'been', 'reported', 'since', 'the', 'start', 'of', 'the', 'year', 'health', 'officials', 'are', 'passing', 'out', 'zika', 'protection', 'kits', 'that', 'include', 'bug', 'spray', 'and', 'condoms', 'along', 'with', 'the', 'recommendation', 'but', 'the', 'warning', 'against', 'unprotected', 'sex', 'isn’t', 'going', 'over', 'very', 'well', 'said', 'dr', 'chris', 'prue', 'a', 'cdc', 'behavioral', 'scientist', 'who', 'has', 'studied', 'the', 'response', 'condoms', 'are', 'not', 'popular', 'in', 'a', 'lot', 'places', 'she', 'said', 'there’s', 'religious', 'and', 'personal', 'preferences', 'and', 'lots', 'of', 'personal', 'factors', 'in', 'that', 'u', 's', 'lawmakers', 'deadlocked', 'over', 'funding', 'to', 'fight', 'the', 'zika', 'virus', 'on', 'tuesday', 'as', 'senate', 'democrats', 'blocked', 'a', 'republican', 'proposal', 'they', 'said', 'fell', 'short', 'of', 'the', 'challenge', 'posed', 'by', 'the', 'virus', 'and', 'hurt', 'other', 'health', 'priorities', 'it', 'was', 'unclear', 'when', 'congress', 'would', 'revisit', 'the', 'request', 'by', 'president', 'barack', 'obama', 'for', '1', '9', 'billion', 'funding', 'priorities', 'in', 'the', 'meantime', 'the', 'white', 'house', 'has', 'diverted', 'more', 'than', '500', 'million', 'earmarked', 'for', 'other', 'projects', 'for', 'urgent', 'zika', 'initiatives', 'including', 'those', 'where', 'scientific', 'opportunities', 'will', 'be', 'lost', 'if', 'not', 'acted', 'upon', 'immediately', 'one', 'such', 'study', 'will', 'follow', 'children', 'born', 'to', 'women', 'infected', 'with', 'zika', 'to', 'identify', 'the', 'development', 'of', 'any', 'disabilities', 'not', 'detected', 'at', 'birth', 'other', 'projects', 'on', 'the', 'priority', 'list', 'include', 'vaccine', 'development', 'and', 'mosquito', 'eradication', 'one', 'study', 'underway', 'will', 'assess', 'whether', 'the', 'risk', 'of', 'transmission', 'is', 'greater', 'from', 'men', 'who', 'experience', 'zika', 'infection', 'symptoms', 'such', 'as', 'fever', 'and', 'rash', 'than', 'from', 'those', 'who', 'don’t', 'this', 'information', 'is', 'considered', 'vital', 'since', 'most', 'people', 'experience', 'no', 'symptoms', 'the', 'study', 'of', 'infected', 'men', 'in', 'brazil', 'and', 'colombia', 'will', 'test', 'semen', 'from', 'thousands', 'of', 'men', 'over', 'time', 'to', 'determine', 'how', 'long', 'zika', 'poses', 'a', 'risk', 'to', 'sexual', 'partners', 'as', 'long', 'the', 'virus', 'can', 'be', 'grown', 'in', 'a', 'laboratory', 'from', 'semen', 'cell', 'samples', 'infectious', 'disease', 'experts', 'believe', 'it', 'is', 'potentially', 'contagious', 'zika', 'typically', 'clears', 'the', 'bloodstream', 'about', 'a', 'week', 'after', 'infection', 'but', 'it', 'has', 'been', 'detected', 'in', 'urine', 'for', 'at', 'least', 'twice', 'as', 'long', 'its', 'persistence', 'in', 'semen', 'in', 'the', 'british', 'case', 'study', 'has', 'caused', 'some', 'researchers', 'to', 'draw', 'comparisons', 'to', 'other', 'viruses', 'hiv', 'can', 'last', 'in', 'blood', 'and', 'semen', 'indefinitely', 'and', 'the', 'west', 'nile', 'virus', 'can', 'reside', 'in', 'the', 'kidneys', 'and', 'urine', 'for', 'years', 'researchers', 'said', 'one', 'patient', 'who', 'survived', 'the', 'deadly', 'ebola', 'outbreak', 'had', 'evidence', 'of', 'that', 'virus', 'in', 'his', 'semen', 'for', '18', 'months', 'we', 'got', 'very', 'surprised', 'by', 'ebola', 'that', 'it', 'was', 'hanging', 'around', 'for', 'so', 'long', 'said', 'dr', 'peter', 'hotez', 'dean', 'of', 'the', 'national', 'school', 'of', 'tropical', 'medicine', 'at', 'baylor', 'college', 'of', 'medicine', 'one', 'of', 'the', 'big', 'questions', 'we', 'have', 'to', 'ask', 'is', 'does', 'zika', 'also', 'cause', 'a', 'similar', 'type', 'of', 'latency', 'additional', 'reporting', 'by', 'julie', 'steenhuysen', 'editing', 'by', 'michele', 'gershberg', 'and', 'lisa', 'girion', 'chicago', 'the', 'fate', 'of', 'a', 'spending', 'plan', 'and', 'tax', 'hike', 'aimed', 'at', 'ending', 'illinois’', 'unprecedented', 'budget', 'impasse', 'moved', 'on', 'tuesday', 'to', 'the', 'house', 'of', 'representatives', 'which', 'will', 'seek', 'to', 'enact', 'the', 'legislation', 'by', 'overriding', 'the', 'republican', 'governor’s', 'vetoes', 'new', 'jersey', 'and', 'maine', 'ended', 'partial', 'government', 'shutdowns', 'just', 'in', 'time', 'for', 'the', 'fourth', 'of', 'july', 'holiday', 'on', 'tuesday', 'helping', 'new', 'jersey', 'governor', 'chris', 'christie', 'move', 'past', 'the', 'embarrassment', 'of', 'being', 'photographed', 'on', 'a', 'beach', 'that', 'had', 'been', 'closed', 'to', 'the', 'public']}\n"
     ]
    }
   ],
   "source": [
    "print(samp_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that your code is correct, simply run (i.e., **DO NOT EDIT**) the cell below to parse the full corpus and output particular samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "editable": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n",
      "# docs: 1139\n",
      "length of doc 20516: 274\n",
      "['nairobi', 'kenya', 'kenyan', 'consumers', 'have', 'long', 'suspected', 'a', 'little', 'monkey', 'business', 'when', 'it', 'comes', 'to', 'their', 'power', 'bills', 'but', 'the']\n",
      "['don’t', 'have', 'much', 'going', 'for', 'yourself', 'whether', 'it', 'be', 'work', 'whether', 'it', 'be', 'school', 'your', 'options', 'are', 'on', 'the', 'block']\n"
     ]
    }
   ],
   "source": [
    "# evaluation cell\n",
    "full_tokenized = load_corpus('CS287_news_full.csv', punc)\n",
    "print(type(full_tokenized))\n",
    "print(type(full_tokenized[20516]))\n",
    "print(\"# docs:\", len(full_tokenized.keys()))\n",
    "print(\"length of doc 20516:\", len(full_tokenized[20516]))\n",
    "print(full_tokenized[20516][:20])\n",
    "print(full_tokenized[23230][-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.2 Bag-of-words [6 points total]**\n",
    "\n",
    "</div>\n",
    "\n",
    "Now, we analyze our corpus by creating *vector representations* of each document. These vector representations will allow us to work with documents as numerical data, compare them to one another, and hopefully design a useful information retrieval system.  \n",
    "\n",
    "The first vector representation we will examine is **bag-of-words** (BoW). Here, in BoW, the dimension of each vector should represents the number of times a particular word occurs in a document.\n",
    "\n",
    "\n",
    "<div class='q_pink'>\n",
    "    \n",
    "**2.2a Word Mapping [1 point]**\n",
    "\n",
    "To start, we need to create a **vocabulary** that maps words to the indices of a vector. Implement the following `create_vocab()` function, which takes as input our `load_corpus()`'s output dictionary and returns a dictionary that maps each unique word in the corpus to a unique integer *index*. The returned dictionary should satisfy the following properties:\n",
    "- Each word is mapped to a number between $0$ and $V - 1$, inclusive, where $V$ is the number of unique words in the corpus.\n",
    "- Words with higher frequency across the corpus should have smaller indices; i.e. the most frequent word should have index $0$ and the least frequent word should have index $V - 1$.  Feel free to break ties arbitrarily.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def create_vocab(tokenized_texts: Dict[int, List[str]]) -> Dict[str, int]:\n",
    "    vocab_dict: Dict[float, str] = {}\n",
    "    for id_doc, content_list in tokenized_texts.items():\n",
    "        for word in content_list:\n",
    "            if word == '':\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "            if word in vocab_dict.keys():\n",
    "                vocab_dict[word] += 1\n",
    "            else:\n",
    "                vocab_dict[word] = 1\n",
    "    num_unique_words = len(list(vocab_dict.keys()))\n",
    "    sorted_vocab_dict = {k[0]:idx for k, idx in zip(sorted(vocab_dict.items(), key=lambda item: item[1], reverse=True), range(num_unique_words))}\n",
    "    return sorted_vocab_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As a sanity check, run the test cell below, which should output a dictionary with 1147 distinct keys (0 through 1146)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "editable": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'of': 1, 'to': 2, 'and': 3, 'in': 4, 'a': 5, 'for': 6, 'that': 7, 'is': 8, 'european': 9, 'has': 10, 'it': 11, 'on': 12, 'but': 13, 'as': 14, 'not': 15, 'said': 16, 'union': 17, 'from': 18, 'are': 19, 'have': 20, 'an': 21, 'by': 22, 'was': 23, 'he': 24, 'ryan': 25, 'europe': 26, 'with': 27, 'we': 28, 'zika': 29, 'would': 30, 'countries': 31, 'trump': 32, 'bloc': 33, 'who': 34, 'into': 35, 'britain': 36, 'its': 37, 'political': 38, 'had': 39, 'crisis': 40, 'this': 41, 'currency': 42, 'at': 43, 'u': 44, 'more': 45, 'his': 46, 'be': 47, 'could': 48, 'british': 49, 'euro': 50, 'economic': 51, 'health': 52, 'semen': 53, 'study': 54, 'which': 55, 'also': 56, 'even': 57, 'or': 58, 'than': 59, 'officials': 60, 'after': 61, 'will': 62, 's': 63, 'virus': 64, 'bloc’s': 65, 'out': 66, 'politics': 67, 'germany': 68, 'long': 69, 'some': 70, 'new': 71, 'other': 72, 'national': 73, 'their': 74, 'most': 75, 'men': 76, 'help': 77, 'sex': 78, 'one': 79, 'whether': 80, 'greece': 81, 'through': 82, 'policy': 83, 'been': 84, 'what': 85, 'eurozone': 86, 'during': 87, 'between': 88, 'public': 89, 'support': 90, 'house': 91, 'republican': 92, 'endorsement': 93, 'among': 94, 'six': 95, 'thursday': 96, 'when': 97, 'considered': 98, 'can': 99, 'debt': 100, 'how': 101, 'still': 102, 'parties': 103, 'france': 104, 'campaign': 105, 'member': 106, 'very': 107, 'director': 108, 'made': 109, 'people': 110, 'such': 111, 'months': 112, 'brussels': 113, 'analysts': 114, 'last': 115, 'she': 116, 'over': 117, 'wait': 118, 'tweeted': 119, 'agenda': 120, 'about': 121, 'feel': 122, 'including': 123, 'i': 124, 'infected': 125, 'sexual': 126, 'risk': 127, 'dr': 128, 'always': 129, 'now': 130, 'voters': 131, 'latest': 132, 'toward': 133, 'if': 134, 'does': 135, 'leave': 136, 'handling': 137, 'past': 138, 'decade': 139, 'time': 140, 'continent': 141, 'many': 142, 'badly': 143, 'institutions': 144, 'like': 145, 'vote': 146, 'states': 147, 'hungary': 148, 'all': 149, 'being': 150, 'they': 151, 'so': 152, 'happened': 153, 'years': 154, 'those': 155, 'integration': 156, 'partners': 157, 'greek': 158, 'ideas': 159, 'before': 160, 'just': 161, 'migrants': 162, 'since': 163, 'believe': 164, 'president': 165, 'say': 166, 'until': 167, 'you': 168, 'speech': 169, 'women': 170, 'nominee': 171, 'announcement': 172, 'voting': 173, 'i’ll': 174, 'ryan’s': 175, 'short': 176, 'we’re': 177, 'buck': 178, 'wrote': 179, 'no': 180, 'unprotected': 181, 'recommendation': 182, 'funding': 183, 'transmission': 184, 'going': 185, 'infection': 186, 'tuesday': 187, 'ago': 188, 'nations': 189, 'forward': 190, 'english': 191, 'establishment': 192, 'politicians': 193, 'may': 194, 'crises': 195, 'standoff': 196, 'migration': 197, 'each': 198, 'ugly': 199, 'angry': 200, 'across': 201, 'agree': 202, 'left': 203, 'way': 204, 'perceived': 205, 'while': 206, 'fully': 207, 'problems': 208, 'west': 209, 'potentially': 210, 'experts': 211, 'change': 212, 'zone': 213, 'meant': 214, 'nearly': 215, 'lost': 216, 'southern': 217, 'continue': 218, 'prime': 219, 'minister': 220, 'move': 221, 'united': 222, 'poland': 223, 'xenophobia': 224, 'remaining': 225, 'easy': 226, 'moment': 227, 'tilford': 228, 'deputy': 229, 'mr': 230, 'union’s': 231, 'benefits': 232, 'helped': 233, 'lots': 234, 'because': 235, 'join': 236, 'common': 237, 'drive': 238, 'system': 239, 'ability': 240, 'spending': 241, 'inevitably': 242, 'differences': 243, 'recovered': 244, 'were': 245, 'led': 246, 'became': 247, 'solution': 248, 'austerity': 249, 'outside': 250, 'members': 251, 'power': 252, 'followed': 253, 'populist': 254, 'aside': 255, 'tsipras': 256, 'clash': 257, 'against': 258, 'cultures': 259, 'greece’s': 260, 'there': 261, 'lot': 262, 'german': 263, 'mainstream': 264, 'year': 265, 'thousands': 266, 'refugees': 267, 'response': 268, 'under': 269, 'week': 270, 'little': 271, 'different': 272, 'force': 273, 'growing': 274, 'warned': 275, 'donald': 276, 'need': 277, 'away': 278, 'world': 279, 'up': 280, 'improve': 281, 'reforms': 282, 'further': 283, 'needed': 284, 'remains': 285, 'elections': 286, 'then': 287, 'see': 288, 'mother': 289, 'lives': 290, 'presidential': 291, 'office': 292, 'fall': 293, 'confident': 294, 'turn': 295, 'laws': 296, 'any': 297, 'back': 298, 'likely': 299, 'held': 300, 'gain': 301, 'though': 302, 'playing': 303, 'word': 304, 'games': 305, 'free': 306, 'call': 307, 'conversations': 308, 'issues': 309, 'enact': 310, 'our': 311, 'fox': 312, 'news': 313, 'weeks': 314, 'initiatives': 315, 'congress': 316, 'hillary': 317, 'clinton': 318, 'don’t': 319, 'follow': 320, 'report': 321, 'meantime': 322, 'couples': 323, 'refrain': 324, '62': 325, 'days': 326, 'along': 327, 'potential': 328, 'reach': 329, 'can’t': 330, 'money': 331, 'earmarked': 332, 'institute': 333, 'infectious': 334, 'brazil': 335, 'colombia': 336, 'determine': 337, 'interview': 338, 'get': 339, 'spread': 340, 'viruses': 341, 'least': 342, 'reported': 343, 'outbreak': 344, 'mosquito': 345, 'disease': 346, 'survived': 347, 'case': 348, 'similar': 349, 'cdc': 350, 'where': 351, 'include': 352, 'condoms': 353, 'chris': 354, 'personal': 355, 'priorities': 356, 'projects': 357, 'development': 358, 'detected': 359, 'experience': 360, 'symptoms': 361, 'urine': 362, 'researchers': 363, 'ebola': 364, 'medicine': 365, 'jersey': 366, 'rome': 367, 'bureaucrats': 368, 'article': 369, 'faith': 370, 'emerges': 371, 'stronger': 372, 'idealistic': 373, 'founders': 374, 'decades': 375, 'dreamed': 376, 'stitching': 377, 'warring': 378, 'peaceful': 379, 'whole': 380, 'knew': 381, 'path': 382, 'bumpy': 383, 'wobbled': 384, 'dream': 385, 'integrated': 386, 'sink': 387, 'channel': 388, 'decide': 389, 'abandon': 390, 'peculiarly': 391, 'affair': 392, 'villains': 393, 'opportunistic': 394, 'steering': 395, 'delusional': 396, 'mistake': 397, 'blame': 398, 'own': 399, 'tribulations': 400, 'flawed': 401, 'approach': 402, 'rammed': 403, 'fixes': 404, 'only': 405, 'inflamed': 406, 'nationalism': 407, 'spreading': 408, 'result': 409, 'almost': 410, 'ad': 411, 'hoc': 412, 'management': 413, 'admirers': 414, 'wounded': 415, 'reputation': 416, 'damaged': 417, 'idealism': 418, 'given': 419, 'disillusionment': 420, 'elite': 421, 'technocrats': 422, 'often': 423, 'touch': 424, 'equipped': 425, 'address': 426, 'unemployment': 427, 'stagnation': 428, 'solidarity': 429, 'dissolving': 430, 'regional': 431, 'divisions': 432, 'east': 433, 'north': 434, 'south': 435, 'implications': 436, 'exit': 437, 'brexit': 438, 'staggering': 439, 'regardless': 440, 'must': 441, 'structure': 442, 'fragile': 443, 'much': 444, 'struggling': 445, 'recover': 446, 'cannot': 447, 'status': 448, 'quo': 449, 'enrico': 450, 'letta': 451, 'former': 452, 'italian': 453, 'gotten': 454, 'mean': 455, 'gaining': 456, 'strength': 457, 'austria': 458, 'same': 459, 'nasty': 460, 'tenor': 461, 'infused': 462, 'hostility': 463, 'immigrants': 464, 'killing': 465, 'jo': 466, 'cox': 467, 'parliament': 468, 'campaigned': 469, 'shocked': 470, 'simon': 471, 'center': 472, 'reform': 473, 'london': 474, 'grim': 475, 'stuff': 476, 'falls': 477, 'interesting': 478, 'camp': 479, 'outspoken': 480, 'critic': 481, 'woes': 482, 'yet': 483, 'strongly': 484, 'supports': 485, 'britain’s': 486, 'far': 487, 'outweigh': 488, 'disadvantages': 489, 'argues': 490, 'realizes': 491, 'failures': 492, 'legitimize': 493, 'arguments': 494, 'want': 495, 'easier': 496, 'them': 497, 'portray': 498, 'e': 499, 'failure': 500, 'become': 501, 'euroskeptics': 502, 'recent': 503, '1990s': 504, 'already': 505, 'considering': 506, 'drop': 507, 'pound': 508, 'adopting': 509, 'today': 510, '19': 511, '28': 512, 'share': 513, 'financial': 514, 'markets': 515, '1992': 516, 'effectively': 517, 'settled': 518, 'matter': 519, 'decided': 520, 'changing': 521, 'skeptical': 522, 'using': 523, 'closer': 524, 'argued': 525, 'joining': 526, 'limit': 527, 'flexibility': 528, 'devalue': 529, 'downturns': 530, 'use': 531, 'deficit': 532, 'encourage': 533, 'growth': 534, 'arise': 535, 'stark': 536, 'sharing': 537, 'proved': 538, 'true': 539, 'plunged': 540, 'cycle': 541, 'disparities': 542, 'exposed': 543, 'save': 544, 'northern': 545, 'bailed': 546, 'desperate': 547, 'counterparts': 548, 'divided': 549, 'debtors': 550, 'creditors': 551, 'rather': 552, 'equal': 553, 'economics': 554, 'inflicted': 555, 'heavy': 556, 'punishment': 557, 'quickly': 558, '2008': 559, 'did': 560, 'group': 561, 'shifted': 562, 'powerhouse': 563, 'steadily': 564, 'accrued': 565, 'inside': 566, 'resentment': 567, 'gradually': 568, 'especially': 569, 'weakest': 570, 'indebted': 571, 'anger': 572, 'erupted': 573, 'january': 574, '2015': 575, 'swept': 576, 'country’s': 577, 'elected': 578, 'radical': 579, 'leftist': 580, 'alexis': 581, 'promised': 582, 'end': 583, 'write': 584, 'down': 585, 'leading': 586, 'consensus': 587, 'theater': 588, 'greeks': 589, 'misplay': 590, 'hand': 591, 'refused': 592, 'budge': 593, 'obligations': 594, 'negotiation': 595, 'brinkmanship': 596, 'collapsed': 597, 'bankruptcy': 598, 'acceding': 599, 'demands': 600, 'demonstration': 601, 'penchant': 602, 'muddling': 603, 'agreed': 604, 'bailout': 605, 'package': 606, 'regard': 607, 'stopgap': 608, 'higher': 609, 'warn': 610, 'another': 611, 'occur': 612, 'criticism': 613, 'daniela': 614, 'schwarzer': 615, 'program': 616, 'marshall': 617, 'fund': 618, 'berlin': 619, 'bleeds': 620, 'perception': 621, 'functioning': 622, 'properly': 623, 'ineffectiveness': 624, 'levels': 625, 'emboldened': 626, 'right': 627, 'seized': 628, 'images': 629, 'hundreds': 630, 'pouring': 631, 'provoke': 632, 'anxiety': 633, 'began': 634, 'erecting': 635, 'fences': 636, 'block': 637, 'despite': 638, 'open': 639, 'borders': 640, 'leaders': 641, 'struggled': 642, 'mount': 643, 'coherent': 644, 'chancellor': 645, 'angela': 646, 'merkel’s': 647, 'popularity': 648, 'plummeted': 649, 'opened': 650, 'syrian': 651, 'tightened': 652, 'restrictions': 653, 'championed': 654, 'controversial': 655, 'deal': 656, 'turkey': 657, 'sharply': 658, 'reduced': 659, 'migrant': 660, 'flow': 661, 'referendum': 662, 'forces': 663, 'sought': 664, 'depicting': 665, 'invasion': 666, 'poster': 667, 'unveiled': 668, 'nigel': 669, 'farage': 670, 'k': 671, 'independence': 672, 'party': 673, 'leader': 674, 'showed': 675, 'parade': 676, 'pilloried': 677, 'blatant': 678, 'propaganda': 679, 'should': 680, 'remain': 681, 'emerge': 682, 'powerful': 683, 'clout': 684, 'changes': 685, 'necessary': 686, 'top': 687, 'recognition': 688, 'europe’s': 689, 'misjudged': 690, 'appetite': 691, 'rapid': 692, 'specter': 693, 'breakup': 694, 'haunting': 695, 'tusk': 696, 'council': 697, 'comprises': 698, 'heads': 699, 'state': 700, 'understand': 701, 'necessity': 702, 'historical': 703, 'forget': 704, 'audacious': 705, 'experiment': 706, 'wash': 707, 'antagonisms': 708, 'war': 709, 'ii': 710, 'build': 711, 'unity': 712, 'required': 713, 'putting': 714, 'ancient': 715, 'rivalry': 716, 'binding': 717, 'together': 718, 'languages': 719, 'economies': 720, 'generations': 721, '1980s': 722, '2000s': 723, 'saw': 724, 'expanding': 725, 'brought': 726, 'tangible': 727, 'borderless': 728, 'travel': 729, 'job': 730, 'educational': 731, 'mobility': 732, 'within': 733, 'rising': 734, 'prosperity': 735, 'poorly': 736, 'governed': 737, 'came': 738, 'pressure': 739, 'expanded': 740, 'unwieldy': 741, 'frictions': 742, 'arose': 743, 'old': 744, 'resentments': 745, 'never': 746, 'scrubbed': 747, 'tensions': 748, 'increased': 749, 'governments': 750, 'sovereignty': 751, 'space': 752, 'deep': 753, 'possible': 754, 'powers': 755, 'advocates': 756, 'likelihood': 757, 'undertaking': 758, 'significant': 759, 'slim': 760, '2017': 761, 'doing': 762, 'something': 763, 'bold': 764, 'spur': 765, 'electoral': 766, 'backlash': 767, 'possibly': 768, 'humanism': 769, 'champion': 770, 'human': 771, 'rights': 772, 'democracy': 773, 'freedom': 774, 'pope': 775, 'francis': 776, 'asked': 777, 'month': 778, 'acceptance': 779, 'charlemagne': 780, 'prize': 781, 'awarded': 782, 'service': 783, 'unification': 784, 'added': 785, 'peoples': 786, 'great': 787, 'upheld': 788, 'sacrificed': 789, 'dignity': 790, 'brothers': 791, 'sisters': 792, 'speaker': 793, 'paul': 794, 'presumptive': 795, 'amounts': 796, 'column': 797, 'janesville': 798, 'gazette': 799, 'speaker’s': 800, 'hometown': 801, 'paper': 802, 'preference': 803, 'realdonaldtrump': 804, 'i’m': 805, 'gop’s': 806, 'https': 807, 'ends': 808, 'speculation': 809, 'off': 810, 'issuing': 811, 'formal': 812, 'saying': 813, 'business': 814, 'magnate': 815, 'appeared': 816, 'number': 817, 'delegates': 818, 'win': 819, 'nomination': 820, 'primary': 821, 'opponents': 822, 'suspended': 823, 'campaigns': 824, 'seemed': 825, 'stop': 826, 'clarified': 827, 'fact': 828, 'getting': 829, 'behind': 830, 'chief': 831, 'communications': 832, 'adviser': 833, 'brendan': 834, 'various': 835, 'gop': 836, 'seal': 837, 'headlines': 838, '2016': 839, 'biggest': 840, 'name': 841, 'coverage': 842, '→': 843, 'these': 844, 'us': 845, 'people’s': 846, 'that’s': 847, 'why': 848, 'him': 849, 'wisconsin': 850, 'representative': 851, 'acknowledged': 852, 'odds': 853, 'it’s': 854, 'secret': 855, 'won’t': 856, 'pretend': 857, 'otherwise': 858, 'speak': 859, 'my': 860, 'mind': 861, 'reality': 862, 'make': 863, 'ground': 864, 'disagreement': 865, 'criticized': 866, 'range': 867, 'statements': 868, 'york': 869, 'billionaire': 870, 'proposed': 871, 'ban': 872, 'muslims': 873, 'entering': 874, 'two': 875, 'private': 876, 'summit': 877, 'washington': 878, 'stopped': 879, 'backing': 880, 'meeting': 881, 'inevitable': 882, 'told': 883, 'comes': 884, 'begins': 885, 'rolling': 886, 'series': 887, 'next': 888, 'several': 889, 'none': 890, 'expected': 891, 'current': 892, 'democratic': 893, 'deliver': 894, 'foreign': 895, 'timing': 896, 'merely': 897, 'happenstance': 898, 'fun': 899, 'coincidence': 900, 'wasn’t': 901, 'timed': 902, 'her': 903, 'sched': 904, 'closely': 905, 'aide': 906, 'video': 907, 'forthcoming': 908, 'talking': 909, 'news’': 910, 'chad': 911, 'pergram': 912, 'contributed': 913, 'male': 914, 'partner': 915, 'extraordinary': 916, 'based': 917, 'single': 918, 'surviving': 919, 'affect': 920, 'millions': 921, 'grave': 922, 'risks': 923, 'associated': 924, 'driving': 925, 'authorities': 926, 'pursue': 927, 'research': 928, 'mired': 929, 'congressional': 930, 'gridlock': 931, 'example': 932, 'science': 933, 'borrowing': 934, 'programs': 935, 'allergy': 936, 'diseases': 937, 'started': 938, 'enrolling': 939, 'transmittable': 940, 'take': 941, 'complete': 942, 'interim': 943, 'results': 944, 'recommendations': 945, 'limb': 946, 'anthony': 947, 'fauci': 948, 'alarmed': 949, 'zika’s': 950, 'versatility': 951, 'expand': 952, 'primarily': 953, 'aedes': 954, 'aegypti': 955, 'mosquitoes': 956, 'dengue': 957, 'chikungunya': 958, '10': 959, 'infections': 960, 'traveled': 961, 'area': 962, 'whose': 963, 'foothold': 964, 'warm': 965, 'habitats': 966, 'effective': 967, 'agent': 968, 'caution': 969, 'lieu': 970, 'answers': 971, 'protect': 972, 'pregnant': 973, 'trying': 974, 'conceive': 975, 'centers': 976, 'control': 977, 'prevention': 978, 'recommended': 979, 'triple': 980, 'organization': 981, 'recently': 982, 'issued': 983, 'guidance': 984, 'strict': 985, 'advice': 986, 'ideal': 987, 'anne': 988, 'schuchat': 989, 'tell': 990, 'satisfying': 991, 'understanding': 992, 'territory': 993, 'puerto': 994, 'rico': 995, '2': 996, '100': 997, 'cases': 998, 'start': 999, 'passing': 1000, 'protection': 1001, 'kits': 1002, 'bug': 1003, 'spray': 1004, 'warning': 1005, 'isn’t': 1006, 'well': 1007, 'prue': 1008, 'behavioral': 1009, 'scientist': 1010, 'studied': 1011, 'popular': 1012, 'places': 1013, 'there’s': 1014, 'religious': 1015, 'preferences': 1016, 'factors': 1017, 'lawmakers': 1018, 'deadlocked': 1019, 'fight': 1020, 'senate': 1021, 'democrats': 1022, 'blocked': 1023, 'proposal': 1024, 'fell': 1025, 'challenge': 1026, 'posed': 1027, 'hurt': 1028, 'unclear': 1029, 'revisit': 1030, 'request': 1031, 'barack': 1032, 'obama': 1033, '1': 1034, '9': 1035, 'billion': 1036, 'white': 1037, 'diverted': 1038, '500': 1039, 'million': 1040, 'urgent': 1041, 'scientific': 1042, 'opportunities': 1043, 'acted': 1044, 'upon': 1045, 'immediately': 1046, 'children': 1047, 'born': 1048, 'identify': 1049, 'disabilities': 1050, 'birth': 1051, 'priority': 1052, 'list': 1053, 'vaccine': 1054, 'eradication': 1055, 'underway': 1056, 'assess': 1057, 'greater': 1058, 'fever': 1059, 'rash': 1060, 'information': 1061, 'vital': 1062, 'test': 1063, 'poses': 1064, 'grown': 1065, 'laboratory': 1066, 'cell': 1067, 'samples': 1068, 'contagious': 1069, 'typically': 1070, 'clears': 1071, 'bloodstream': 1072, 'twice': 1073, 'persistence': 1074, 'caused': 1075, 'draw': 1076, 'comparisons': 1077, 'hiv': 1078, 'blood': 1079, 'indefinitely': 1080, 'nile': 1081, 'reside': 1082, 'kidneys': 1083, 'patient': 1084, 'deadly': 1085, 'evidence': 1086, '18': 1087, 'got': 1088, 'surprised': 1089, 'hanging': 1090, 'around': 1091, 'peter': 1092, 'hotez': 1093, 'dean': 1094, 'school': 1095, 'tropical': 1096, 'baylor': 1097, 'college': 1098, 'big': 1099, 'questions': 1100, 'ask': 1101, 'cause': 1102, 'type': 1103, 'latency': 1104, 'additional': 1105, 'reporting': 1106, 'julie': 1107, 'steenhuysen': 1108, 'editing': 1109, 'michele': 1110, 'gershberg': 1111, 'lisa': 1112, 'girion': 1113, 'chicago': 1114, 'fate': 1115, 'plan': 1116, 'tax': 1117, 'hike': 1118, 'aimed': 1119, 'ending': 1120, 'illinois’': 1121, 'unprecedented': 1122, 'budget': 1123, 'impasse': 1124, 'moved': 1125, 'representatives': 1126, 'seek': 1127, 'legislation': 1128, 'overriding': 1129, 'governor’s': 1130, 'vetoes': 1131, 'maine': 1132, 'ended': 1133, 'partial': 1134, 'government': 1135, 'shutdowns': 1136, 'fourth': 1137, 'july': 1138, 'holiday': 1139, 'helping': 1140, 'governor': 1141, 'christie': 1142, 'embarrassment': 1143, 'photographed': 1144, 'beach': 1145, 'closed': 1146}\n"
     ]
    }
   ],
   "source": [
    "# sanity check cell\n",
    "samp_vocab = create_vocab(samp_tokenized)\n",
    "print(samp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that your code is correct, simply run (i.e., **DO NOT EDIT**) the cell below to create a vocabulary for the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "editable": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full vocab size: 35599\n",
      "'said' index: 9\n",
      "'last' index: 69\n",
      "'obama' index: 112\n"
     ]
    }
   ],
   "source": [
    "# evaluation cell\n",
    "full_vocab = create_vocab(full_tokenized)\n",
    "\n",
    "print(\"full vocab size:\", len(full_vocab))\n",
    "print(\"'said' index:\", full_vocab['said'])\n",
    "print(\"'last' index:\", full_vocab['last'])\n",
    "print(\"'obama' index:\", full_vocab['obama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_full_vocab = {idx: word for word, idx in full_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.2b BoW Representations [1 point]**\n",
    "\n",
    "Now, let's create the actual BoW representations. Write the function `create_bow()` below, as follows:\n",
    "- accept two inputs: the output from `load_corpus()` (e.g., `samp_tokenized` or `full_tokenized`) and the output from `create_vocab()` (e.g., `samp_vocab` or `full_vocab`)\n",
    "- it creates BoW representations for each document (not the titles) and returns it\n",
    "- specifically, the output needs to be a dictionary mapping each document id to a NumPy array $\\vec x \\in \\mathbb{R}^V$, where $x_i$ is the number of times the word, whose index into the vocabulary is $i$, appears in the document. For example, if $i=0$ corresponds to \"the\", and \"the\" appears 17 times in the given document, then the 1st index of vector $\\vec x$ should be 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_bow(tokenized_texts: Dict[int, List[str]], vocab: Dict[str, int]) -> Dict[int, np.ndarray]:\n",
    "    bow_dict: Dict[int, np.array] = {}\n",
    "    for doc_id, tokens in tokenized_texts.items():\n",
    "        doc_list = [0] * len(vocab)\n",
    "        for idx, word in enumerate(vocab):\n",
    "            num_occurr = tokens.count(word)\n",
    "            doc_list[idx] = num_occurr\n",
    "        bow_dict[doc_id] = np.array(doc_list)\n",
    "    return bow_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As a sanity check, run the cell below on the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "editable": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys([20621, 84549, 189782])\n",
      "vocab size: 1147\n",
      "sample slice from doc 20621's vector: [130  60  33  43  27  30  12  19]\n"
     ]
    }
   ],
   "source": [
    "# sanity check cell\n",
    "samp_bow = create_bow(samp_tokenized, samp_vocab)\n",
    "\n",
    "# should yield keys: 20621, 84549, 189782\n",
    "print(\"keys:\", samp_bow.keys()) \n",
    "\n",
    "# should yield 1147\n",
    "print(\"vocab size:\", len(samp_bow[20621]))\n",
    "\n",
    "# should yield [130.  60.  33.  43.  27.  30.  12.  19.]\n",
    "print(\"sample slice from doc 20621's vector:\", samp_bow[20621][0:8]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that your code is correct, simply run (i.e., **DO NOT EDIT**) the cell below to create a BoW representation for the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# keys: 1139\n",
      "first 5 sorted keys: [20153, 20189, 20240, 20283, 20285]\n",
      "last 5 sorted keys: [189773, 189775, 189777, 189778, 189782]\n",
      "vocab size: 35599\n",
      "[56 34 34 30 27]\n",
      "[53 14 26 21 18]\n"
     ]
    }
   ],
   "source": [
    "# evaluation cell\n",
    "full_bow = create_bow(full_tokenized, full_vocab)\n",
    "print(\"# keys:\", len(full_bow.keys()))\n",
    "print(\"first 5 sorted keys:\", sorted(full_bow.keys())[:5])\n",
    "print(\"last 5 sorted keys:\", sorted(full_bow.keys())[-5:])\n",
    "print(\"vocab size:\", len(full_bow[20337]))\n",
    "print(full_bow[20337][:5])\n",
    "print(full_bow[20442][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.2c Using BoW [4 points]**\n",
    "\n",
    "Great! Now that we have vector representations of all of our _documents_ (not titles of documents), let's see what we can do with them! To quantify the distance between two documents, we will calculate the **cosine similarity** between their vector representations. Imagine that someone is using Mooogle and really likes a particular document $d$. One easy way for Mooogle to recommend documents that are similar to $d$ is to find $k$ documents in the rest of the corpus that have the largest cosine similarity with $d$. Below, write code however you wish so that you can answer the following questions about the full `CS287_news_full.csv` corpus:\n",
    "1. What is the cosine similarity between the following pairs of documents: 20153 and 23299?\n",
    "2. What are the *titles* of the $k = 5$ documents most similar to document 23266? Do these results seem reasonable?\n",
    "3. Explore the corpus a bit.  Are there documents that are close to many different documents? Are there documents that are far from all other documents? **NOTE:** We are not asking/requiring you to make visualizations here; you can if you want, as that would be highly insightful, but we are simply asking for you to explore your results a bit and report your thoughts.\n",
    "4. Can you find a document whose $k = 5$ closest recommendations don't make sense?  Why do you think that's the case?\n",
    "\n",
    "For clarity, we will explicitly ask you each of these questions in separate prompts below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a, vec_b):\n",
    "    result = np.dot(vec_a, vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b))\n",
    "    return result\n",
    "\n",
    "def most_similar_docs(doc1, curr_bow, as_dict=True):\n",
    "    doc1_bow = curr_bow[doc1]\n",
    "    doc1_sim_dict: Dict[int, float] = {}\n",
    "    for doc in curr_bow.keys():\n",
    "        doc1_sim_dict[doc] = cosine_similarity(curr_bow[doc1], curr_bow[doc])\n",
    "    if as_dict:\n",
    "        sorted_vocab_dict = {k: v for k,v in sorted(doc1_sim_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "        return sorted_vocab_dict\n",
    "    else:\n",
    "        return doc_1_sim_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "1. What is the cosine similarity between the following pairs of documents: 20153 and 23299? Your cell below should output your answer (e.g., 0.42)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between documents 20153 and 23299 is : 0.8158961839604788\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(full_bow[20153], full_bow[23299])\n",
    "print(\"Cosine similarity between documents 20153 and 23299 is : %s\" % (cos_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "2. What are the titles of the $k=5$ documents most similar to document 23266 (which is titled \"Muhammad Ali Remembered, by Those Who Knew Him as Cassius - The New York Times\")? Your cell below should output your answer. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is sorted_similarity_dict  {23266: 1.0000000000000002, 20522: 0.9060596165772996, 189341: 0.8965078729119949, 25773: 0.8915644451910685, 84573: 0.8876387537786607, 20695: 0.8725416721687299, 25769: 0.8715543550470715, 20620: 0.8707921851418821, 20666: 0.8618595155485458, 26203: 0.8599318678601845, 23361: 0.8548749368396225, 20577: 0.8538205008351765, 23299: 0.8523629091069594, 189443: 0.849634398965712, 23351: 0.8496101927114946, 26206: 0.8488883873818409, 20446: 0.8448217335036934, 20547: 0.8442322129765402, 84761: 0.8438132971937593, 23394: 0.843192067053954, 23319: 0.8425362861169622, 23306: 0.842457470350241, 24948: 0.8412907123711978, 23293: 0.8406811409104796, 24880: 0.8405591357623448, 23254: 0.8390410147818708, 20438: 0.8387036577515671, 189513: 0.8364943745904988, 20622: 0.8352859594650773, 25804: 0.8352799679050119, 20478: 0.8350764985549355, 23345: 0.8343216240230846, 23400: 0.8338236556228282, 24913: 0.8336248056893755, 23247: 0.833527129349665, 23253: 0.8334128770261174, 20716: 0.8333917193698285, 24909: 0.8332103369244552, 20479: 0.8328063805268161, 20559: 0.8317949438107582, 23308: 0.8317631912031632, 20507: 0.8303431018697776, 20457: 0.8303196008102164, 25786: 0.8296751874999597, 84853: 0.829338570532035, 24931: 0.8290082970912798, 23326: 0.828708674098205, 20575: 0.8283087940218968, 24956: 0.8280655323986003, 20486: 0.8279650242266138, 23329: 0.827922089177264, 20498: 0.8277571441872762, 23276: 0.8276686969306838, 20485: 0.8267235636462711, 20488: 0.826552738871072, 26204: 0.8263679632776816, 189496: 0.826263243860242, 25741: 0.824778113159703, 189498: 0.8246620440229798, 20506: 0.8245057444764274, 23333: 0.8243199502740459, 189582: 0.8237915502305126, 84682: 0.8236767932344916, 20601: 0.8228103289827841, 23297: 0.8220488507581731, 20642: 0.8217293186628453, 20512: 0.8215484160323417, 20564: 0.8212151697484327, 24915: 0.8205519417030795, 20474: 0.8200886642170593, 189592: 0.8196039383044136, 23322: 0.8192846028980233, 20550: 0.819188350390858, 23335: 0.8187859142924018, 20738: 0.8182776224600948, 84796: 0.8182182443607755, 84785: 0.8173425642523094, 84696: 0.817192549389199, 24908: 0.8171766825358088, 20355: 0.8170388114189773, 23320: 0.8170343870588659, 84855: 0.8165319109062065, 26411: 0.8159709433820505, 84778: 0.8153146320674743, 84854: 0.8150792652739532, 189742: 0.8148408260932067, 25800: 0.814666387688466, 25801: 0.8146435814130724, 84764: 0.8141246661004018, 25774: 0.8140928709332544, 23307: 0.813606371547895, 84692: 0.813487357738086, 189392: 0.813364544800345, 20451: 0.8133241126494354, 20574: 0.8130597281355035, 84720: 0.8128387912339572, 23397: 0.8122858466076437, 24919: 0.8119084151847218, 20677: 0.8115499168441433, 189387: 0.8108951365729496, 23311: 0.8107005170463218, 20652: 0.810500687076051, 20510: 0.8102973445306471, 24963: 0.8098036909988291, 25787: 0.8096863161251879, 189768: 0.8090915624287627, 24947: 0.8089260364344155, 20509: 0.8088533452633843, 20399: 0.8088348064012503, 20553: 0.8087471270272804, 84569: 0.8081620639112582, 84708: 0.8080169467890906, 25789: 0.8079802317704594, 20538: 0.807832045200265, 189664: 0.807775922849429, 84572: 0.8076041228642699, 20681: 0.807535873380485, 24942: 0.8073175872743231, 84679: 0.8070749302117286, 26197: 0.8068487091278054, 84706: 0.8055882518486605, 23285: 0.8055781907547782, 20491: 0.8051537120067807, 84718: 0.8051017391208058, 189469: 0.80505106971031, 23239: 0.8049476280554008, 24904: 0.804939915305449, 24933: 0.804589169669159, 20496: 0.8042995587071433, 23296: 0.804247165951971, 24981: 0.8041597328971308, 23250: 0.8036085768374295, 20718: 0.803491735746931, 26193: 0.8032804846146888, 25766: 0.802574750240405, 84890: 0.8024285087222125, 24971: 0.802099272542657, 26515: 0.8014285480571088, 189647: 0.8013029373766826, 20653: 0.8012584806122143, 20672: 0.8011214873712663, 25758: 0.801089855454905, 23241: 0.8008733468458779, 23327: 0.8006590061633937, 23225: 0.8004650464513349, 23349: 0.8003136412211167, 189551: 0.8002610983127678, 189524: 0.8000405815061711, 20452: 0.7996824936295882, 84916: 0.7992811271746184, 84930: 0.798650735560737, 84698: 0.7985495420154697, 23402: 0.7984804379078744, 24989: 0.7983791386195994, 189611: 0.7982265293949458, 20609: 0.7980938935713517, 23395: 0.7978710838716228, 26198: 0.7967553829700494, 20643: 0.7961206032798306, 20475: 0.7959875980295169, 20767: 0.7957247319300041, 23244: 0.7955508913137543, 84612: 0.7948527955108846, 24892: 0.7947862467557484, 20624: 0.7947572108976778, 20645: 0.7941702155753804, 23367: 0.7937746309890547, 20732: 0.7937083357366114, 20454: 0.7935679666870745, 23416: 0.793492488064746, 20490: 0.7931956081778956, 20688: 0.7927968688110469, 20740: 0.792713412593767, 20443: 0.792617289243841, 23412: 0.7918708132948582, 20646: 0.7918291365921005, 20240: 0.7917377083007209, 84637: 0.7916140098713027, 20667: 0.7912335722524817, 20447: 0.7909820167924864, 23341: 0.7908401246909371, 20500: 0.7907206063650365, 23226: 0.7906873681334368, 189591: 0.7906758893028396, 84540: 0.7905778862774018, 189313: 0.7904966421106842, 23270: 0.7904205347742213, 189431: 0.7902228276008526, 189734: 0.7901449649517076, 84914: 0.7901150070493069, 84927: 0.790086732933675, 20590: 0.789232029627903, 84596: 0.7889135430501406, 189528: 0.7888990519290942, 189691: 0.7888977532490407, 23265: 0.7887931253155647, 20761: 0.7887037150109568, 84669: 0.788694088879303, 84848: 0.7885733471161789, 23282: 0.7885466970496048, 24918: 0.7885309864166814, 26201: 0.7880753966562303, 189745: 0.7879648181764838, 26414: 0.7877462807365435, 20567: 0.787710796449583, 20565: 0.787680644565731, 23283: 0.7874606349096656, 20542: 0.7874011601770112, 20337: 0.7871585468636692, 20189: 0.7871286533397648, 84559: 0.7871077137512691, 23193: 0.786596305931355, 24968: 0.7861846743709731, 24944: 0.7860777239879312, 24941: 0.78585561016209, 20753: 0.7854080713396336, 20588: 0.7846012768574405, 23362: 0.7845183902967426, 20465: 0.7843385765963109, 189308: 0.7843238727963582, 20712: 0.7842624448848354, 20640: 0.7840902017928197, 189384: 0.7839977948997706, 23318: 0.7839712318768445, 20701: 0.7837113707412535, 24991: 0.7833669099323809, 84843: 0.7832622574106015, 84804: 0.7830270495829674, 84566: 0.7829958490284914, 189720: 0.7827716328947271, 189505: 0.7820173382462775, 20439: 0.781928288099583, 20433: 0.781908259887846, 20662: 0.7818878324338829, 84610: 0.7816866493090269, 24979: 0.7816305362710955, 24983: 0.7814562675957156, 24914: 0.7813830202181254, 26209: 0.7812969891161289, 84734: 0.7811394498669914, 84561: 0.7810822373110772, 20678: 0.7807903840297369, 24938: 0.780495932922417, 23304: 0.7804628817236875, 20420: 0.7797827664305689, 20417: 0.779709914234227, 20647: 0.7794612114042019, 20611: 0.778905870919468, 189401: 0.7788497790849626, 24962: 0.7787421101529821, 189648: 0.7786437495071455, 20502: 0.7785672666904849, 23373: 0.7782938302176854, 23274: 0.7782502629922181, 20476: 0.7781134667982695, 189603: 0.7779192232840719, 20458: 0.7775617880770032, 20489: 0.7775486014874669, 20518: 0.7775425648910593, 84945: 0.7775407563988109, 23204: 0.7773268465532654, 23392: 0.7772627656668507, 84800: 0.7771747118359015, 23279: 0.7768117087126966, 20546: 0.7767096072549535, 20524: 0.7766819491718738, 20708: 0.7763693438340478, 23370: 0.7763068026392108, 20424: 0.7762979251827575, 20448: 0.7761447958889124, 20698: 0.7761336649431505, 189754: 0.776022785891936, 189537: 0.7759444935998651, 189529: 0.7758620737181195, 84621: 0.7757461756133639, 84846: 0.7756350122030738, 84835: 0.775574121841076, 23313: 0.7755311832277423, 20571: 0.7755046009036145, 84595: 0.775388881090579, 23245: 0.7753250444613783, 20636: 0.7751707888798167, 189680: 0.7750836113108045, 20517: 0.7749209086975246, 20419: 0.7748633827395497, 189746: 0.7748346034763299, 20513: 0.7747808836154532, 20608: 0.7747446793736973, 20468: 0.774636881902215, 84824: 0.7745962604681109, 25794: 0.7743978515953779, 24920: 0.7743601506286564, 189448: 0.7743469297203249, 23294: 0.7743006415256222, 20519: 0.7741036264450831, 84860: 0.7737651261989026, 189342: 0.7736589979160952, 20675: 0.773413421741203, 189545: 0.7733896481511757, 84809: 0.7733382314174418, 20587: 0.773282405159032, 84865: 0.7730582241196573, 26410: 0.7730074397277449, 20456: 0.7729231230854895, 24984: 0.7728516547052251, 23284: 0.7727961380151865, 189365: 0.7725658168063345, 84747: 0.7724678274621937, 20551: 0.7722224237006942, 25790: 0.7721378102440392, 84823: 0.7719311122124741, 20597: 0.771851553987116, 20445: 0.771811069073811, 189506: 0.7716799858897093, 84888: 0.7716639994207893, 84701: 0.7715573766165942, 25775: 0.7714138344875888, 23230: 0.7713442134074142, 189616: 0.7710917383611516, 189497: 0.7710238322968387, 84651: 0.7709368023812837, 20541: 0.7708963685426014, 25791: 0.7705505252434306, 20442: 0.7705240550069967, 84784: 0.7704849239064112, 189503: 0.7704110620235791, 84825: 0.7702416354678541, 84833: 0.7701572956494271, 23340: 0.7697322551616935, 20665: 0.7696606634634539, 84564: 0.769635275706884, 23408: 0.7695595413794275, 189380: 0.7695447205502479, 189422: 0.7695134878391134, 189712: 0.7694396226906136, 84672: 0.7694101810688913, 84861: 0.7693835052089881, 23409: 0.7693372564535733, 84697: 0.7693077917448102, 189718: 0.7693008936255592, 189553: 0.7690116742056976, 84653: 0.7688909161307115, 84932: 0.7686379969861675, 189615: 0.7685056452006115, 84616: 0.7684923648752402, 189508: 0.768438932315495, 189501: 0.7684207773853075, 23418: 0.7679139830076637, 23305: 0.7677243693221275, 189697: 0.767650458475634, 189494: 0.7676405331093864, 84533: 0.7675548625924484, 20554: 0.7675400080350011, 189356: 0.7674816464439694, 23347: 0.7672211533669789, 23258: 0.7672125928478315, 20749: 0.7670029226922115, 20687: 0.7668309134589435, 84667: 0.7667534762331458, 84725: 0.7664224571695202, 23262: 0.7663044863081594, 20449: 0.7662317934045326, 26192: 0.7661158853212474, 24967: 0.7659804387400917, 20480: 0.7659122198855983, 23364: 0.7658726958705807, 23393: 0.7657587324425872, 84768: 0.7656058946385135, 24952: 0.7655602203987656, 189439: 0.7655086596365197, 24927: 0.7654467805084332, 20674: 0.7653816044122321, 20709: 0.7653218603769215, 20505: 0.7653156962846567, 20758: 0.7652617659328309, 84665: 0.7652115529943225, 84606: 0.7651548978375882, 23414: 0.7649237532026179, 189675: 0.7645585942668234, 23237: 0.7643230682041771, 20495: 0.764262097059377, 23267: 0.7642334907610887, 189609: 0.7640669413189954, 20428: 0.7639745574628042, 20421: 0.7638803452938248, 20735: 0.7638417358154043, 26408: 0.7638119446018738, 189634: 0.7635222446079079, 84722: 0.7634935033233838, 20481: 0.7632366164573278, 189460: 0.7630255866248931, 84707: 0.7629363587758947, 84631: 0.7628782701361236, 20679: 0.7628539990910277, 84683: 0.7627267722173064, 20637: 0.7626788572471365, 20724: 0.7626626409593968, 84749: 0.76265485733472, 26404: 0.7625090764046045, 84934: 0.7624692969670486, 23348: 0.7624535364167575, 84771: 0.7623148342727336, 189594: 0.7622695629816432, 23316: 0.7621252353117292, 189702: 0.762015479520001, 24911: 0.7619867396864233, 84650: 0.7619643379945052, 20527: 0.761941237864316, 189659: 0.761797847338186, 20525: 0.7617647150354541, 84847: 0.7616286414288482, 20638: 0.7612991702520276, 84615: 0.7612772047340062, 84803: 0.7612388058769005, 25765: 0.761099899888441, 20563: 0.7610317753964506, 84758: 0.7609157529762044, 20765: 0.7607670672185001, 84742: 0.7607181538281799, 20661: 0.7605637148956893, 189773: 0.7602709280692248, 189613: 0.7601594663525052, 189354: 0.760150430448603, 24917: 0.7598712768441402, 84602: 0.7598582605080715, 23289: 0.7591119474896402, 20707: 0.7590971732263149, 84777: 0.7589410070489034, 20670: 0.7588446755908133, 23112: 0.7588018649343592, 84859: 0.7585804025922755, 189602: 0.7584166430961562, 189685: 0.7581605710438186, 20684: 0.7580697174400095, 20649: 0.7580331335363186, 26199: 0.7579982389278304, 189640: 0.757950891532294, 84913: 0.7578874329969623, 189499: 0.757868163972395, 23359: 0.7577525749394363, 84900: 0.7576475066399941, 20592: 0.7575289578370206, 189753: 0.7574815440367165, 26409: 0.7574815357095095, 84608: 0.7574319522979297, 84906: 0.7572654845990436, 84741: 0.7571856272297244, 189770: 0.7571062480497318, 84647: 0.7570088165870991, 25780: 0.756946995729865, 84894: 0.75694567961582, 84818: 0.7567987951249115, 23336: 0.7563647974855684, 20584: 0.7562167188708341, 84817: 0.7562054038502134, 189393: 0.7557696772749695, 84901: 0.7555495791342972, 84864: 0.7553926678953796, 84600: 0.7552261040261491, 189512: 0.7550088575880791, 189421: 0.7548946998509071, 20566: 0.754889211176714, 189699: 0.75472511725279, 84837: 0.7545252632787588, 189423: 0.7542667984951764, 20686: 0.7542322223524776, 84858: 0.7539645396992909, 189525: 0.7537710271292293, 23324: 0.7537345448148352, 20530: 0.7536544466938239, 189456: 0.7535571468017053, 84766: 0.7535282760164081, 20429: 0.753472949990365, 23360: 0.7534698138042767, 20748: 0.7533526544339265, 84881: 0.7532075759863868, 84579: 0.7531828487895709, 84730: 0.7531827718397076, 26407: 0.7530948559191769, 189446: 0.7529647641595264, 25797: 0.7528466835341756, 84702: 0.7523637432282034, 84568: 0.7522654948556684, 189614: 0.7522199481591813, 20558: 0.7520724038924806, 84553: 0.7520392182154594, 189676: 0.7519867051466321, 84884: 0.7519515633745129, 189558: 0.751903922519317, 20423: 0.7518631195419937, 20663: 0.7506204444074012, 189711: 0.7505358859741535, 189651: 0.7505257913705065, 84780: 0.7502608515851994, 23399: 0.7501436026434927, 84724: 0.7499663220333692, 20598: 0.7499094220474359, 26520: 0.7498641408412309, 84852: 0.7494862798500334, 189555: 0.7492598445295234, 189710: 0.7491063083126183, 26211: 0.7490569975559515, 84821: 0.7487559400772994, 189606: 0.7483632026001709, 25771: 0.7481739539658516, 189298: 0.7480198227714873, 20473: 0.7471905450724822, 20153: 0.7471640851650234, 189715: 0.7467606726985285, 189483: 0.7466702806345692, 23379: 0.7466299866646884, 84574: 0.746240472137288, 23287: 0.7462369491422371, 84779: 0.7459997522811247, 20621: 0.7459667182423354, 84816: 0.7459143518993522, 189547: 0.7456687752923712, 24953: 0.7456288450930965, 189714: 0.7456119475384936, 20639: 0.7455727985604185, 23405: 0.7453904400132159, 84727: 0.7453870248594495, 84797: 0.7452913077777136, 84578: 0.7451487443614013, 189353: 0.7450986842751547, 25760: 0.7448191487992402, 25770: 0.7447901243880249, 189721: 0.7447366892163555, 23255: 0.7446962116607208, 189415: 0.7446667925608667, 189723: 0.7445903623079748, 20501: 0.744443470996331, 84641: 0.7443283689644258, 23243: 0.7442883273749357, 20710: 0.7442760149730037, 84763: 0.7442600208012378, 84548: 0.744170047711513, 20285: 0.7439266005132744, 189731: 0.7439078962109428, 84593: 0.7437726218351447, 84814: 0.7437432179044754, 189394: 0.7436688512283326, 20633: 0.7435585101635229, 24923: 0.7433113836511025, 84790: 0.7432969165149756, 189767: 0.7432842432417436, 189749: 0.7431509581379974, 26517: 0.7430396571670922, 24902: 0.7429650763707499, 84851: 0.742764165892144, 26210: 0.7426653249046047, 189661: 0.7426303105989029, 84617: 0.7425671091325227, 20455: 0.7424775915112157, 84759: 0.7422494997502286, 25781: 0.7421260484915198, 84874: 0.7419625629223976, 189747: 0.7419190152513592, 20713: 0.741852464196422, 84732: 0.741850090198712, 189709: 0.7417952458076656, 20764: 0.7417094368857485, 84581: 0.7414025152327811, 24965: 0.7413951182023916, 84834: 0.7413112038112056, 24960: 0.7409534891632228, 189330: 0.7408885980142531, 189624: 0.7407973101203489, 84813: 0.7406844144615373, 189713: 0.7405696922663576, 84902: 0.7404879942030664, 189778: 0.7404366683820278, 189692: 0.7402053604768853, 20283: 0.7401682915637963, 20733: 0.7401346415485721, 84831: 0.7399664313574116, 189500: 0.7396709528903421, 20418: 0.7396544161282237, 23300: 0.7396280917600437, 189663: 0.7393149388631871, 84856: 0.739307791032373, 20660: 0.7391613504711632, 189315: 0.7391331999262193, 189557: 0.739076275188036, 84658: 0.738890273844303, 20682: 0.7387788630086464, 23368: 0.7387548921741275, 25788: 0.7386856726961665, 189484: 0.7384147343264135, 84875: 0.7383734036469253, 20389: 0.7383668149813069, 84628: 0.7383551302093555, 189285: 0.7383260495562879, 189314: 0.7382233411947862, 189617: 0.7378914511833675, 189383: 0.7376516858632789, 24987: 0.737615980131029, 189677: 0.7376152751746596, 84723: 0.7374511317013541, 189457: 0.7373172466160604, 84599: 0.7372833353758482, 189729: 0.7372027097640461, 84926: 0.737168503544339, 20683: 0.7370602710417243, 84911: 0.7370291783043721, 189782: 0.7369066845420725, 20668: 0.7368172273778827, 84705: 0.7367322159621214, 84633: 0.7366804670357262, 84789: 0.736396878753628, 189461: 0.7362993125509644, 189441: 0.7362535585677066, 84935: 0.7362486596823552, 84598: 0.7362426086614257, 24903: 0.7362242972087878, 84538: 0.736154026410313, 189398: 0.7357248354197057, 84868: 0.7353906061380276, 189554: 0.7353147635628945, 20596: 0.735230567264749, 23369: 0.7351580424120248, 189531: 0.7351022233582593, 189756: 0.7350996801290951, 84670: 0.7350892029506602, 25785: 0.7346416704932561, 23413: 0.7344117129460659, 84866: 0.7343000848839876, 84862: 0.7341490004091641, 20436: 0.7338883338101572, 84551: 0.7338747060288022, 84736: 0.7338729151355482, 20288: 0.73387224218188, 20472: 0.7338459164491409, 189364: 0.7338265449224478, 84576: 0.7335589558442082, 189419: 0.7335570322812773, 84710: 0.7333524561594779, 189772: 0.7332781027012485, 84565: 0.7332570249938783, 84836: 0.7332058385950031, 189334: 0.7330932202498937, 23272: 0.7330805804098306, 23278: 0.7329465423271618, 84739: 0.7329119578178293, 20494: 0.7328451840158692, 189626: 0.7326758856733605, 189465: 0.7326396442153857, 84700: 0.7322076689127547, 20612: 0.732022845820515, 23275: 0.7319655355900914, 20493: 0.7319088961942828, 24977: 0.731831979100206, 189369: 0.7318134278963245, 24936: 0.7313494562057395, 84820: 0.7308877725613572, 189678: 0.7308834224689966, 189586: 0.7308035363817689, 84622: 0.7301764968787428, 84629: 0.7300564697259126, 189490: 0.7300087259719106, 189347: 0.7299921978341241, 84904: 0.7298950720027866, 20614: 0.7295791984500254, 84826: 0.7295581075047168, 84752: 0.7294517453185113, 23218: 0.7293413252282962, 84589: 0.729333558372218, 84830: 0.7292937814705994, 84841: 0.7292885190009532, 84541: 0.7290716661031507, 84924: 0.7290297882416946, 189643: 0.7289998114920782, 84703: 0.7289945470785949, 20763: 0.7288092446071721, 84607: 0.7287413524576734, 23271: 0.7287367957272438, 25772: 0.7286809651629347, 20482: 0.7286488149262418, 189655: 0.7286057471041297, 84731: 0.7285326817321525, 189350: 0.728514628488043, 189509: 0.7284373863203297, 84889: 0.7282838361548046, 189649: 0.7282280404255381, 189539: 0.7281578364724299, 24912: 0.7281342872771311, 84754: 0.7280373429575964, 84829: 0.727925591726066, 20657: 0.7276213953970908, 189445: 0.7272422259005177, 84614: 0.7272387347006464, 24980: 0.7269565437650211, 20437: 0.7267945136810837, 84620: 0.7263926480170383, 189447: 0.7262839283482181, 20573: 0.7262628635308999, 23384: 0.726221578606574, 84750: 0.7259521382403835, 20641: 0.7259196487263389, 20719: 0.7258436021241379, 24957: 0.7256697827565395, 189562: 0.7252852435889323, 24946: 0.7250528931327161, 84880: 0.7249820262664364, 189323: 0.7247987839082071, 84584: 0.7245741039654411, 20605: 0.724437107244476, 20511: 0.7243865375030968, 84769: 0.7243715807441352, 189761: 0.7243136894177067, 189309: 0.7243004038707077, 26207: 0.7242656428171449, 84575: 0.7241447995246093, 189635: 0.7239997086324864, 25778: 0.7239040963688925, 20656: 0.7237057784001131, 23317: 0.7235985929756291, 84666: 0.7234967810443825, 23352: 0.7231167955065504, 20629: 0.7229618455193189, 189381: 0.7228938650959728, 84756: 0.7227566209302505, 24958: 0.7224652842549625, 189673: 0.7224640645340781, 23353: 0.7223158071258866, 189361: 0.7221686721621272, 84915: 0.722164759080639, 20572: 0.7221478368089287, 24993: 0.7221030123814166, 189641: 0.722100971316348, 84755: 0.7218628039115971, 25805: 0.7218406964603431, 84663: 0.7217892144623618, 20759: 0.7216735079717678, 84661: 0.7216438009259181, 189436: 0.7215903342502419, 189667: 0.7215429456756843, 20430: 0.721127912803156, 189559: 0.7210390202923941, 84640: 0.7209135032499223, 189633: 0.7209097760570942, 189671: 0.7208705541233835, 189684: 0.720522707478195, 20560: 0.7203804380447791, 23396: 0.7203578367331553, 26217: 0.7197814565964613, 84603: 0.7196832671394767, 84587: 0.7195421747984098, 189652: 0.7195259664948533, 189338: 0.7192800651351072, 189442: 0.7189031302701849, 84558: 0.7188952230598409, 189777: 0.7186498512318381, 20736: 0.7185046650838353, 189638: 0.7183704082541585, 189360: 0.7183397451904195, 84757: 0.7178446194578535, 23371: 0.7177983245540758, 25768: 0.7175854697878008, 24964: 0.7173513692294408, 24924: 0.7169652590802698, 24929: 0.7166342272072161, 84869: 0.7165880346440103, 189689: 0.7165838184435851, 189738: 0.7163045366702453, 24921: 0.7161705612279542, 189486: 0.716052982876354, 23281: 0.7160448917814267, 189727: 0.7159042509088448, 84635: 0.7159031015193605, 84917: 0.7156563938932861, 23295: 0.7155511741435338, 189390: 0.7151928294253244, 189514: 0.7149554886128269, 20591: 0.7149093241834751, 24928: 0.7146616824953206, 20600: 0.7146098705961214, 84552: 0.7143462220274922, 189760: 0.7143388081119635, 84689: 0.7139184224475777, 20526: 0.7138858677590515, 189576: 0.7136833461886032, 26194: 0.7131185852289723, 20521: 0.7129815178972321, 20731: 0.7129328481637439, 84791: 0.712874783191977, 84788: 0.7128634742805141, 84844: 0.7127744009538901, 23257: 0.7127588034895993, 20441: 0.7124488644428201, 23290: 0.7123101382956095, 189653: 0.7122447552460327, 20528: 0.7120850024957047, 23358: 0.7114285391742068, 84828: 0.7113725316691598, 84802: 0.711316384680829, 84549: 0.7112681460910939, 26400: 0.711133436618642, 20722: 0.7111248537610788, 20615: 0.7108103863724987, 84712: 0.710687032289347, 189703: 0.7104934231909535, 189332: 0.7104388162294982, 20492: 0.7103790779496197, 189416: 0.7102898529898555, 84716: 0.7099781422569912, 84671: 0.7099130455819755, 84557: 0.7098358575909207, 189748: 0.7097831198786105, 84792: 0.7095224106886302, 84563: 0.7094041948021643, 189732: 0.709154634746721, 20529: 0.7089736077540967, 189757: 0.708870910291973, 189741: 0.7082506506812761, 189371: 0.708049959175846, 23240: 0.7079894888714667, 189750: 0.707920247233974, 84704: 0.7074129874209715, 189533: 0.7074033862609342, 20435: 0.7071847911212802, 84717: 0.7067468283380165, 20477: 0.7066226680103862, 189683: 0.7066126177405418, 26205: 0.7063622185542133, 189560: 0.7057536486341329, 189752: 0.7057352056108115, 20651: 0.705478286782885, 20632: 0.7054673335558438, 84832: 0.7053405761510614, 189389: 0.705121075673627, 84601: 0.7049996048291113, 20692: 0.7048915100710472, 189519: 0.7048884364617273, 84737: 0.7047565293823984, 84664: 0.7044920570784498, 20398: 0.7042491878908123, 84849: 0.7042154191843635, 84547: 0.7037461121005791, 23403: 0.7035967395888963, 84560: 0.7029963995515556, 23325: 0.7028983356233222, 189541: 0.7021810250067487, 189610: 0.701813903551321, 84925: 0.7017326207751858, 23268: 0.7014348765460147, 84594: 0.7012624159318855, 20729: 0.7012118638305772, 189670: 0.7011248877545372, 189540: 0.7009616366984672, 23332: 0.7008952978437153, 84567: 0.6998900648730025, 84613: 0.699825274443476, 84876: 0.6994800876784355, 24949: 0.6993508106709044, 189679: 0.6986352710080943, 189470: 0.6986319169952016, 189542: 0.6985923880355305, 84657: 0.6984325422868202, 23383: 0.6982756164406712, 84892: 0.6981189346341101, 189452: 0.6978715201467492, 189406: 0.6976292035160314, 189414: 0.6972714210307638, 84726: 0.6971528881624488, 84840: 0.697141191992462, 84728: 0.697034400702755, 189449: 0.6969705487746714, 189737: 0.6965335120194098, 84954: 0.6956829725435875, 84891: 0.6954225768406241, 189704: 0.6953079344636033, 189593: 0.6949616524112845, 84806: 0.6949105766707483, 189769: 0.694785184877402, 20453: 0.6947102828888637, 84808: 0.6946032430798245, 189717: 0.6945770833875986, 84694: 0.6945417014981561, 189404: 0.6944846294781392, 84819: 0.6944414061101142, 189340: 0.6942999713930853, 20514: 0.6941815963668126, 84782: 0.6940831241940629, 84753: 0.693968886215167, 23252: 0.6932513972087169, 189595: 0.6931979712029636, 84827: 0.693075280353829, 84562: 0.6930139811457804, 84535: 0.6929701023447994, 20594: 0.6929356422816281, 20387: 0.6928935555775997, 20569: 0.6923498923345474, 84625: 0.6923016948261129, 189464: 0.6919942677604155, 189424: 0.6919693931521035, 25756: 0.6919332595813703, 84711: 0.6917406264916023, 84713: 0.6915773795336521, 189620: 0.6913275091717512, 84655: 0.6912410652686974, 84588: 0.6904965778152119, 84897: 0.6903988246224242, 189534: 0.6903386220637362, 84767: 0.6901455813148006, 84885: 0.6901043198675948, 20685: 0.6900165252876934, 189584: 0.6899710372777222, 84845: 0.689935286278436, 20497: 0.6897281723508857, 84677: 0.6897249572711633, 20444: 0.6896750386914824, 84715: 0.6895381108450266, 26200: 0.6895229194149601, 189666: 0.6892525146547004, 189565: 0.6890381788768221, 20531: 0.6888449414774419, 189426: 0.68819649419019, 84545: 0.6881441748663789, 84648: 0.6879605749449323, 84577: 0.6876949185497963, 84923: 0.6873819367608939, 189612: 0.6873643885092632, 189319: 0.6872224150163754, 84659: 0.6872150670324166, 189425: 0.6871342515714784, 189563: 0.6871085058558947, 23387: 0.6868231629651816, 23375: 0.6868166494524234, 189385: 0.6868024399622131, 84583: 0.6865819339962703, 23286: 0.6865469412182139, 84872: 0.6859204433700739, 189437: 0.6854177760001389, 84586: 0.6851617472764016, 84571: 0.6850019971572425, 189639: 0.6849658202636356, 189708: 0.6849633355265294, 189495: 0.6842953042364213, 20537: 0.6839379399227135, 84654: 0.6838986816369073, 84605: 0.6838879778570286, 25779: 0.6836959204929468, 84619: 0.6831876830861442, 189637: 0.6829605375891539, 23328: 0.6828951233266144, 189491: 0.6826865639083941, 189660: 0.6819198051953078, 189585: 0.6810852857450814, 84709: 0.6808457910990319, 189764: 0.6806713210140284, 84675: 0.6806349069542409, 84801: 0.6806262400016906, 84684: 0.6805245078631215, 189355: 0.6800244351298419, 84863: 0.6796969716797436, 84611: 0.6795495342015697, 20425: 0.6794046374469842, 189444: 0.6787451788519103, 84921: 0.6785864617909321, 189621: 0.6773926232743748, 84644: 0.677300475915658, 189344: 0.677247679552378, 189758: 0.6768058335495118, 189650: 0.6763773942638877, 84783: 0.6762934296768173, 189569: 0.6756666798056226, 189619: 0.6754126699941693, 84604: 0.6751181107277141, 84798: 0.6736266975435757, 189324: 0.6735708491082762, 189451: 0.6734935449541978, 189623: 0.6734318154440605, 189674: 0.6730857894539032, 189775: 0.672732813437825, 189730: 0.6727292991319688, 84919: 0.6724958394256382, 189662: 0.6721949250399323, 189658: 0.6718377349857725, 20483: 0.6716167644102119, 23291: 0.6708247812913352, 84721: 0.6707302900629398, 189511: 0.6703726702142093, 189396: 0.6701336175750379, 189632: 0.6696626650888636, 84850: 0.6695958033779246, 189417: 0.6690124824821373, 26406: 0.6679074446306382, 84624: 0.6677935463793614, 189450: 0.6676727172144329, 20520: 0.6675076047850275, 84870: 0.6671747656230764, 23376: 0.667018764867239, 189707: 0.6667364400794323, 189548: 0.664465925601356, 84590: 0.6643413662364153, 189657: 0.6640182917696268, 84714: 0.6638629205894068, 84630: 0.6627827598141556, 84674: 0.6625920134330552, 189743: 0.6623146999828843, 84907: 0.6619787181167308, 24978: 0.6618049684641922, 189544: 0.6612380891402196, 189521: 0.6610666873583901, 84815: 0.6609734544801532, 84807: 0.6607660017602869, 189349: 0.6598218933595745, 84920: 0.6591878509819762, 189645: 0.6589596974827965, 189568: 0.6585478390820136, 189622: 0.6580162903117976, 189318: 0.6576239522229983, 189535: 0.6562594847021979, 84810: 0.6561500361581709, 189408: 0.6560746547008046, 84877: 0.6554327348366658, 84787: 0.6552112188313096, 24959: 0.655190220149105, 84744: 0.6551666036979827, 84676: 0.6550665462709834, 189596: 0.6547066731035478, 84632: 0.6537671915349839, 20516: 0.6536390851682936, 189672: 0.6517831512298291, 189590: 0.6512740980427353, 84811: 0.6509375971647487, 84539: 0.6506276190996508, 24943: 0.6504027160875326, 189310: 0.6492590775690921, 189771: 0.6492362202919815, 24907: 0.6480989872507238, 189462: 0.6480254207925575, 189476: 0.647005045853389, 189366: 0.6468360244742181, 84743: 0.6466260115151302, 84638: 0.6464051561350089, 189763: 0.6463304242999357, 20570: 0.6443340180251583, 84686: 0.6442156884083663, 189538: 0.6440725106919629, 189762: 0.642329299757303, 189716: 0.6420156469015694, 84634: 0.6406854746366052, 84893: 0.6402995818720059, 20630: 0.639488544380303, 23312: 0.6383170896259619, 189706: 0.6376162428703531, 25784: 0.6369345641839051, 24939: 0.6362302166938503, 189397: 0.6361835234542405, 84585: 0.6361671891224769, 189628: 0.633249820528616, 84627: 0.6329253107655436, 189337: 0.6327977216839539, 189378: 0.6308209171074104, 189543: 0.629152108566489, 189735: 0.629051586635519, 189377: 0.6286717901288057, 84760: 0.6246811329625016, 84786: 0.6242086122965054, 84799: 0.6240273102323584, 189580: 0.6238863840227441, 84660: 0.6237856755923881, 189589: 0.622082709468662, 84871: 0.6218709321112478, 84745: 0.6215132140419507, 84775: 0.621426178303546, 84912: 0.6208284132739832, 23357: 0.6203717285393062, 189427: 0.6201017170883999, 84556: 0.617199829025445, 189567: 0.6131718430372993, 84554: 0.612976275564808, 84685: 0.6120924476615128, 84695: 0.6104794903828719, 84781: 0.61040376596767, 84639: 0.6102171103875963, 84645: 0.6075212189989649, 84543: 0.6054641134786257, 84636: 0.6054462994321754, 84623: 0.6051574074791815, 84905: 0.6034372116946327, 20714: 0.6030262544996124, 84933: 0.6011227708724123, 189328: 0.5986500625324089, 189320: 0.5974433429766209, 84928: 0.5967792763217828, 84733: 0.5966675419486648, 189454: 0.5949117066579992, 189482: 0.5943889306286866, 189668: 0.5937615260508784, 84687: 0.5936539097495309, 26412: 0.5912941265655118, 84895: 0.5863504328626713, 84618: 0.5858054740189933, 189395: 0.584720432987762, 84656: 0.584442007457919, 84883: 0.5830220212913417, 84947: 0.5816107877472148, 84740: 0.5801832359334672, 20556: 0.5792127566160612, 189477: 0.57760238095527, 84580: 0.5758218399721343, 189351: 0.573764844960224, 189343: 0.5731639821366122, 23389: 0.5711439072892719, 84886: 0.56911804621544, 84887: 0.566899061543247, 23269: 0.5664248645402005, 84643: 0.5659693957745666, 84536: 0.5635811471248, 84746: 0.5626722126054906, 84795: 0.5617911098279931, 84642: 0.5610368364323546, 84910: 0.5609655520918475, 20734: 0.5607524952882295, 189550: 0.5584401036389179, 84776: 0.5476069733880732, 84878: 0.5364041709219979, 84805: 0.5299392244554849, 84896: 0.5298802020297623, 84908: 0.5233336634858193, 84729: 0.5212963553437742, 84839: 0.5191720750874529, 84898: 0.5170437369577956, 84693: 0.5126848873409356, 84929: 0.508883552267314, 20673: 0.5041071755297589, 84794: 0.5001877404772546, 84592: 0.4902632452002035, 84931: 0.4671911254631065, 84751: 0.45603858875927, 20727: 0.42673660547552883, 84899: 0.41659314683272325, 84903: 0.3602837437596066, 84882: 0.35062203053147617, 84681: 0.34269340493425005, 84570: 0.018981835794602785}\n"
     ]
    }
   ],
   "source": [
    "# Code to find most similar documents to document 23266\n",
    "print(\"This is sorted_similarity_dict \", most_similar_docs(23266, full_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, we can see that the 5 documents that are most similar to document 23266 are: 20522, 189341, 25773, 84573,20695. The title of document 23266 is: \"Muhammad Ali Rememberd, by Those whoKnew Him as Cassius- The New York Times\". The title of the 5 closest documents are 20522: \"Muhammad Ali was her first, and greatest, love - the New York Times\", 189341: \"MuhamadAli, 'the greatest', rememberd as boxer who transcended sports\", 25773: \"Muhammad Ali fans Pay Homage to Their Local Hero - The New York Times\", 84573: \"Juan Williams: Muhammad Ali was my hero who brought me to tears\" and 20695: \"Review: For Muhammad Ali, an Endless Round of Books\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "2 (continued). Do the results above seem reasonable? Please explain in 1-2 sentences.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution:\n",
    "\n",
    "The results above seems reasonable since all the 5 documents are talking about remembering Muhammad Ali for his passing away. These documents all share stories, memories and words from people who knew him and talk about his boxing record, his humanitarian/religious stances and how he was remembered even after many years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### TEXT SOLUTION:\n",
    "Yes, all 5 documents are clearly about Muhammad Ali. Several are about \"remembering\" him, so one can expect that the documents concern many shared words about remembering, paying homage, respect, and his legacy.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "3. Explore the corpus a bit. Are there documents that are close to many different documents? Are there documents that are far from all other documents? Please limit your response to 2-3 sentences. <b>NOTE:</b> We are not asking/requiring you to make visualizations here; you can if you want, as that would be highly insightful, but we are simply asking for you to explore your results a bit and report your thoughts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Code for computing matrix of cosine similarities\n",
    "def matrix_cos_sim(bow=full_bow):\n",
    "    docs_list = bow.keys()\n",
    "    list_similarities = []\n",
    "    for doc in docs_list:\n",
    "        similarity_list = list(most_similar_docs(doc, bow).values())\n",
    "        list_similarities.append(similarity_list)\n",
    "    matrix_similarities = np.array(list_similarities)\n",
    "    return pd.DataFrame(data=matrix_similarities, index=docs_list).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_similarity = matrix_cos_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>84570</th>\n",
       "      <th>84681</th>\n",
       "      <th>84882</th>\n",
       "      <th>84903</th>\n",
       "      <th>84899</th>\n",
       "      <th>84751</th>\n",
       "      <th>20727</th>\n",
       "      <th>20673</th>\n",
       "      <th>84886</th>\n",
       "      <th>84931</th>\n",
       "      <th>...</th>\n",
       "      <th>23402</th>\n",
       "      <th>23373</th>\n",
       "      <th>20547</th>\n",
       "      <th>23400</th>\n",
       "      <th>25801</th>\n",
       "      <th>20732</th>\n",
       "      <th>20510</th>\n",
       "      <th>25804</th>\n",
       "      <th>20485</th>\n",
       "      <th>20643</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.343970</td>\n",
       "      <td>0.363538</td>\n",
       "      <td>0.387613</td>\n",
       "      <td>0.398507</td>\n",
       "      <td>0.413384</td>\n",
       "      <td>0.467907</td>\n",
       "      <td>0.498276</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.507939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784472</td>\n",
       "      <td>0.784820</td>\n",
       "      <td>0.785090</td>\n",
       "      <td>0.785575</td>\n",
       "      <td>0.786399</td>\n",
       "      <td>0.788054</td>\n",
       "      <td>0.790278</td>\n",
       "      <td>0.790857</td>\n",
       "      <td>0.792311</td>\n",
       "      <td>0.794083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.031022</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>0.049562</td>\n",
       "      <td>0.063795</td>\n",
       "      <td>0.049881</td>\n",
       "      <td>0.046365</td>\n",
       "      <td>0.056533</td>\n",
       "      <td>0.057990</td>\n",
       "      <td>0.054484</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073739</td>\n",
       "      <td>0.075120</td>\n",
       "      <td>0.071663</td>\n",
       "      <td>0.073360</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>0.071670</td>\n",
       "      <td>0.074936</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>0.072515</td>\n",
       "      <td>0.076816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.009344</td>\n",
       "      <td>0.013114</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.028747</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.013454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.308918</td>\n",
       "      <td>0.336383</td>\n",
       "      <td>0.346474</td>\n",
       "      <td>0.374475</td>\n",
       "      <td>0.392112</td>\n",
       "      <td>0.437314</td>\n",
       "      <td>0.470894</td>\n",
       "      <td>0.477831</td>\n",
       "      <td>0.473892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753869</td>\n",
       "      <td>0.750656</td>\n",
       "      <td>0.754850</td>\n",
       "      <td>0.754363</td>\n",
       "      <td>0.757840</td>\n",
       "      <td>0.759076</td>\n",
       "      <td>0.757513</td>\n",
       "      <td>0.761268</td>\n",
       "      <td>0.761715</td>\n",
       "      <td>0.758362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.012213</td>\n",
       "      <td>0.348828</td>\n",
       "      <td>0.366497</td>\n",
       "      <td>0.393110</td>\n",
       "      <td>0.403393</td>\n",
       "      <td>0.417438</td>\n",
       "      <td>0.474769</td>\n",
       "      <td>0.505651</td>\n",
       "      <td>0.511963</td>\n",
       "      <td>0.515634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797340</td>\n",
       "      <td>0.796991</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.799318</td>\n",
       "      <td>0.800262</td>\n",
       "      <td>0.800568</td>\n",
       "      <td>0.804457</td>\n",
       "      <td>0.804153</td>\n",
       "      <td>0.807712</td>\n",
       "      <td>0.807217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.018851</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.395242</td>\n",
       "      <td>0.431007</td>\n",
       "      <td>0.427896</td>\n",
       "      <td>0.440445</td>\n",
       "      <td>0.505662</td>\n",
       "      <td>0.533910</td>\n",
       "      <td>0.538413</td>\n",
       "      <td>0.547585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832961</td>\n",
       "      <td>0.835780</td>\n",
       "      <td>0.829705</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.834141</td>\n",
       "      <td>0.835196</td>\n",
       "      <td>0.839709</td>\n",
       "      <td>0.837510</td>\n",
       "      <td>0.840218</td>\n",
       "      <td>0.846748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             84570        84681        84882        84903        84899  \\\n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000   \n",
       "mean      0.015665     0.343970     0.363538     0.387613     0.398507   \n",
       "std       0.031022     0.055841     0.049562     0.063795     0.049881   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.078412   \n",
       "25%       0.007948     0.308918     0.336383     0.346474     0.374475   \n",
       "50%       0.012213     0.348828     0.366497     0.393110     0.403393   \n",
       "75%       0.018851     0.380148     0.395242     0.431007     0.427896   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             84751        20727        20673        84886        84931  ...  \\\n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000  ...   \n",
       "mean      0.413384     0.467907     0.498276     0.504065     0.507939  ...   \n",
       "std       0.046365     0.056533     0.057990     0.054484     0.064758  ...   \n",
       "min       0.012040     0.000000     0.000000     0.001417     0.006800  ...   \n",
       "25%       0.392112     0.437314     0.470894     0.477831     0.473892  ...   \n",
       "50%       0.417438     0.474769     0.505651     0.511963     0.515634  ...   \n",
       "75%       0.440445     0.505662     0.533910     0.538413     0.547585  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "             23402        23373        20547        23400        25801  \\\n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000   \n",
       "mean      0.784472     0.784820     0.785090     0.785575     0.786399   \n",
       "std       0.073739     0.075120     0.071663     0.073360     0.072915   \n",
       "min       0.010429     0.012270     0.009344     0.013114     0.011818   \n",
       "25%       0.753869     0.750656     0.754850     0.754363     0.757840   \n",
       "50%       0.797340     0.796991     0.799000     0.799318     0.800262   \n",
       "75%       0.832961     0.835780     0.829705     0.835227     0.834141   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             20732        20510        25804        20485        20643  \n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000  \n",
       "mean      0.788054     0.790278     0.790857     0.792311     0.794083  \n",
       "std       0.071670     0.074936     0.072042     0.072515     0.076816  \n",
       "min       0.028747     0.008656     0.004457     0.009064     0.013454  \n",
       "25%       0.759076     0.757513     0.761268     0.761715     0.758362  \n",
       "50%       0.800568     0.804457     0.804153     0.807712     0.807217  \n",
       "75%       0.835196     0.839709     0.837510     0.840218     0.846748  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 1139 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_similarity.describe().sort_values(by=\"mean\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20643</th>\n",
       "      <th>20485</th>\n",
       "      <th>25804</th>\n",
       "      <th>20510</th>\n",
       "      <th>20732</th>\n",
       "      <th>25801</th>\n",
       "      <th>23400</th>\n",
       "      <th>20547</th>\n",
       "      <th>23373</th>\n",
       "      <th>23402</th>\n",
       "      <th>...</th>\n",
       "      <th>84931</th>\n",
       "      <th>84886</th>\n",
       "      <th>20673</th>\n",
       "      <th>20727</th>\n",
       "      <th>84751</th>\n",
       "      <th>84899</th>\n",
       "      <th>84903</th>\n",
       "      <th>84882</th>\n",
       "      <th>84681</th>\n",
       "      <th>84570</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "      <td>1139.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.794083</td>\n",
       "      <td>0.792311</td>\n",
       "      <td>0.790857</td>\n",
       "      <td>0.790278</td>\n",
       "      <td>0.788054</td>\n",
       "      <td>0.786399</td>\n",
       "      <td>0.785575</td>\n",
       "      <td>0.785090</td>\n",
       "      <td>0.784820</td>\n",
       "      <td>0.784472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507939</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.498276</td>\n",
       "      <td>0.467907</td>\n",
       "      <td>0.413384</td>\n",
       "      <td>0.398507</td>\n",
       "      <td>0.387613</td>\n",
       "      <td>0.363538</td>\n",
       "      <td>0.343970</td>\n",
       "      <td>0.015665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.076816</td>\n",
       "      <td>0.072515</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>0.074936</td>\n",
       "      <td>0.071670</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>0.073360</td>\n",
       "      <td>0.071663</td>\n",
       "      <td>0.075120</td>\n",
       "      <td>0.073739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>0.054484</td>\n",
       "      <td>0.057990</td>\n",
       "      <td>0.056533</td>\n",
       "      <td>0.046365</td>\n",
       "      <td>0.049881</td>\n",
       "      <td>0.063795</td>\n",
       "      <td>0.049562</td>\n",
       "      <td>0.055841</td>\n",
       "      <td>0.031022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.013454</td>\n",
       "      <td>0.009064</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>0.028747</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.013114</td>\n",
       "      <td>0.009344</td>\n",
       "      <td>0.012270</td>\n",
       "      <td>0.010429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012040</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.758362</td>\n",
       "      <td>0.761715</td>\n",
       "      <td>0.761268</td>\n",
       "      <td>0.757513</td>\n",
       "      <td>0.759076</td>\n",
       "      <td>0.757840</td>\n",
       "      <td>0.754363</td>\n",
       "      <td>0.754850</td>\n",
       "      <td>0.750656</td>\n",
       "      <td>0.753869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473892</td>\n",
       "      <td>0.477831</td>\n",
       "      <td>0.470894</td>\n",
       "      <td>0.437314</td>\n",
       "      <td>0.392112</td>\n",
       "      <td>0.374475</td>\n",
       "      <td>0.346474</td>\n",
       "      <td>0.336383</td>\n",
       "      <td>0.308918</td>\n",
       "      <td>0.007948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.807217</td>\n",
       "      <td>0.807712</td>\n",
       "      <td>0.804153</td>\n",
       "      <td>0.804457</td>\n",
       "      <td>0.800568</td>\n",
       "      <td>0.800262</td>\n",
       "      <td>0.799318</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.796991</td>\n",
       "      <td>0.797340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515634</td>\n",
       "      <td>0.511963</td>\n",
       "      <td>0.505651</td>\n",
       "      <td>0.474769</td>\n",
       "      <td>0.417438</td>\n",
       "      <td>0.403393</td>\n",
       "      <td>0.393110</td>\n",
       "      <td>0.366497</td>\n",
       "      <td>0.348828</td>\n",
       "      <td>0.012213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.846748</td>\n",
       "      <td>0.840218</td>\n",
       "      <td>0.837510</td>\n",
       "      <td>0.839709</td>\n",
       "      <td>0.835196</td>\n",
       "      <td>0.834141</td>\n",
       "      <td>0.835227</td>\n",
       "      <td>0.829705</td>\n",
       "      <td>0.835780</td>\n",
       "      <td>0.832961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547585</td>\n",
       "      <td>0.538413</td>\n",
       "      <td>0.533910</td>\n",
       "      <td>0.505662</td>\n",
       "      <td>0.440445</td>\n",
       "      <td>0.427896</td>\n",
       "      <td>0.431007</td>\n",
       "      <td>0.395242</td>\n",
       "      <td>0.380148</td>\n",
       "      <td>0.018851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             20643        20485        25804        20510        20732  \\\n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000   \n",
       "mean      0.794083     0.792311     0.790857     0.790278     0.788054   \n",
       "std       0.076816     0.072515     0.072042     0.074936     0.071670   \n",
       "min       0.013454     0.009064     0.004457     0.008656     0.028747   \n",
       "25%       0.758362     0.761715     0.761268     0.757513     0.759076   \n",
       "50%       0.807217     0.807712     0.804153     0.804457     0.800568   \n",
       "75%       0.846748     0.840218     0.837510     0.839709     0.835196   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             25801        23400        20547        23373        23402  ...  \\\n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000  ...   \n",
       "mean      0.786399     0.785575     0.785090     0.784820     0.784472  ...   \n",
       "std       0.072915     0.073360     0.071663     0.075120     0.073739  ...   \n",
       "min       0.011818     0.013114     0.009344     0.012270     0.010429  ...   \n",
       "25%       0.757840     0.754363     0.754850     0.750656     0.753869  ...   \n",
       "50%       0.800262     0.799318     0.799000     0.796991     0.797340  ...   \n",
       "75%       0.834141     0.835227     0.829705     0.835780     0.832961  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "             84931        84886        20673        20727        84751  \\\n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000   \n",
       "mean      0.507939     0.504065     0.498276     0.467907     0.413384   \n",
       "std       0.064758     0.054484     0.057990     0.056533     0.046365   \n",
       "min       0.006800     0.001417     0.000000     0.000000     0.012040   \n",
       "25%       0.473892     0.477831     0.470894     0.437314     0.392112   \n",
       "50%       0.515634     0.511963     0.505651     0.474769     0.417438   \n",
       "75%       0.547585     0.538413     0.533910     0.505662     0.440445   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "             84899        84903        84882        84681        84570  \n",
       "count  1139.000000  1139.000000  1139.000000  1139.000000  1139.000000  \n",
       "mean      0.398507     0.387613     0.363538     0.343970     0.015665  \n",
       "std       0.049881     0.063795     0.049562     0.055841     0.031022  \n",
       "min       0.078412     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.374475     0.346474     0.336383     0.308918     0.007948  \n",
       "50%       0.403393     0.393110     0.366497     0.348828     0.012213  \n",
       "75%       0.427896     0.431007     0.395242     0.380148     0.018851  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 1139 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_similarity.describe().sort_values(by=\"mean\", axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Solution\n",
    "\n",
    "From the dataframes above we can see that there quite a few documents that are close to many different documents, in the second dataframe we can see that documents 20643, 20485, 25804, 20510 (and probably more) are very very close to other documents since already they have a cosine similarity of 75% or more wit more than 75% of the documents. On the contrary, from the first dataframe of summary statistics, we can see that document 84570 is very far away from most other documents since it has an average cosine similarity of 1.5665% and the 75% highest cosine similarity with other document is 1.8851%, which tells us that it is far away from most other documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### TEXT SOLUTION\n",
    "If we calculate the average cosine similarity between each document and how it relates to all other documents, we see that many have high average values (i.e., > 0.7). However, there is also an outlier (doc 84570) that is highly dissimilar from all other documents (i.e., 0.015 average cosine sim.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "4. Can you find a document whose $k=5$ closest recommendations don't make sense?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is sorted_similarity_dict  {84570: 1.0000000000000002, 84899: 0.07841208353509484, 189620: 0.07500059598423084, 189450: 0.06787067954808844, 189730: 0.06632732389985732, 189623: 0.06154153587124326, 189653: 0.060639060474461985, 189679: 0.06060024283831565, 189706: 0.06055062624337072, 189320: 0.05976585440775372, 189425: 0.05958503488602902, 189639: 0.05855375697963844, 189565: 0.05580629045235695, 189427: 0.053797662305653554, 189716: 0.05315773799457127, 189657: 0.05281987504152739, 189568: 0.052645583316525364, 189711: 0.051651180286056246, 189717: 0.05140585653260072, 23359: 0.050442096816276094, 189550: 0.04830377495150673, 189668: 0.0475000414944878, 189424: 0.047477225362741654, 84815: 0.04625176226135221, 189707: 0.044313314432786036, 189404: 0.04427886714515124, 20474: 0.04415135665240457, 189476: 0.04404431192184146, 189645: 0.043827498968464516, 20590: 0.04345089620236705, 189395: 0.04329856411352706, 189396: 0.04309175344400497, 189341: 0.042884053405798206, 84675: 0.04273021829523148, 25756: 0.04250204285710929, 189763: 0.04236890994764999, 189737: 0.04089469794711459, 189387: 0.040814903193089516, 189752: 0.03997590091231413, 23367: 0.039692982340705125, 189674: 0.03915971703070932, 189452: 0.037877999550337375, 189778: 0.03766500113620106, 84611: 0.03734149388519785, 189702: 0.03729300141553053, 189589: 0.03715188083835686, 84674: 0.03702314392915663, 189764: 0.03697018739286269, 189569: 0.036800883867942016, 189596: 0.03634878208346676, 20688: 0.03634161781098861, 189349: 0.036187179461993096, 84782: 0.0359291173876464, 24947: 0.03568386290653131, 189580: 0.03567890751272063, 20442: 0.03566407947207465, 20592: 0.035282278869784765, 189638: 0.03524266528862106, 20496: 0.03468187963388299, 23418: 0.034626747862846405, 84849: 0.03442808676640699, 189678: 0.03431071119214906, 24913: 0.03413397558982117, 189661: 0.034058754821969364, 189619: 0.03404372584603306, 189633: 0.03358771055920652, 189769: 0.03335082389629379, 189324: 0.03327707834006173, 84697: 0.03303181258901863, 84798: 0.033009562602626094, 189735: 0.032846549087272504, 189542: 0.032819646158332286, 189760: 0.032813221609402095, 189340: 0.03254998030584259, 189612: 0.03232493856389836, 189313: 0.03231983766991046, 24957: 0.032134767104214305, 189482: 0.03200102404915463, 20767: 0.031763966297402295, 84947: 0.03126605300545426, 20611: 0.031259331415519905, 23316: 0.031174856563745565, 23304: 0.031047367356265582, 189708: 0.031037152867320422, 20695: 0.031000374275225846, 84669: 0.03096827428725386, 84704: 0.03091567079916493, 189650: 0.030775112153266275, 20652: 0.030772861251033543, 189649: 0.03068872005921154, 23352: 0.03065308582493466, 189748: 0.030192542028474243, 189360: 0.030121913496074598, 189511: 0.030000960046082457, 23289: 0.02988785744807701, 20519: 0.02963260802660591, 189540: 0.029622709089097376, 20667: 0.029594274955946808, 23257: 0.029590958640144473, 20716: 0.029507730761493683, 84573: 0.029329190372313043, 84912: 0.029316578378450024, 189610: 0.02908114610866905, 84737: 0.028879871787236222, 20732: 0.02874664938545648, 189437: 0.028619098061639762, 20475: 0.028351509612839748, 189365: 0.02831135720172352, 84698: 0.028212896589491342, 24936: 0.028125956404815154, 189394: 0.027977262749675397, 23296: 0.027893190084512635, 84538: 0.027880184331797234, 189454: 0.027459489153095158, 189659: 0.027434898539982988, 20521: 0.027013019816809036, 23387: 0.026951741472801518, 189567: 0.026946142392158774, 20486: 0.026802251456114038, 23335: 0.02676112205623831, 189531: 0.026742122232964953, 26193: 0.026705831808386333, 189746: 0.02670479845713428, 189643: 0.026564987804558078, 189732: 0.02649992878270447, 20528: 0.02647338723555781, 189734: 0.026424604776894928, 23244: 0.026399610815564557, 189524: 0.02633102414498558, 189762: 0.026326230313136235, 189677: 0.026208109170594015, 189658: 0.02604151704562493, 189521: 0.026019641977700295, 24904: 0.02599956558338781, 189441: 0.025871753843546653, 189537: 0.025811819468109707, 20490: 0.02575917660628445, 20451: 0.02541465715085849, 26515: 0.025388771611636683, 189548: 0.025316260068332905, 189347: 0.025315061747941046, 26211: 0.02530945660915417, 84714: 0.025009196701956053, 189684: 0.024991934800845456, 189768: 0.024896484180787867, 23416: 0.02471849003076952, 26209: 0.024634683810425852, 189337: 0.024608604969594158, 189353: 0.024522712310730745, 20456: 0.02448268212821176, 20701: 0.024429583289350607, 189448: 0.024409692790961834, 23333: 0.02440454245984272, 84825: 0.024388357844492906, 23271: 0.024299582102577405, 25778: 0.024267573150630096, 189397: 0.024241524916161182, 189545: 0.024184957259493194, 84743: 0.02412999981198898, 24956: 0.02411437015463501, 84871: 0.02385364476846038, 189660: 0.023764470177172075, 24942: 0.023674710234723582, 84571: 0.02356024919962606, 20740: 0.023412583375239257, 20748: 0.023392094340180955, 23357: 0.023368553274245318, 189562: 0.02325618172563031, 189415: 0.023122794079274363, 84624: 0.023117735949336513, 84725: 0.023063441148937665, 189310: 0.022841321206251434, 20480: 0.022841106986240433, 84919: 0.02281606684134683, 84878: 0.022787927659690344, 189584: 0.022722204389117765, 84761: 0.022676734572169528, 84547: 0.02265748748419441, 20288: 0.0226258970601717, 189672: 0.022603962777973986, 189461: 0.02254043026710977, 189576: 0.022474807699445053, 20468: 0.02246687559236438, 189709: 0.022446876410559556, 189563: 0.022445803402481604, 84763: 0.022433778049956294, 189389: 0.022424290302418413, 20661: 0.022331899501810863, 189519: 0.022278863246163176, 189591: 0.02227331697120231, 189738: 0.022194950709613054, 20433: 0.022119070574148643, 84801: 0.02206178283483876, 189557: 0.022059264566262702, 24981: 0.02203064908775929, 189529: 0.02201821003341545, 189767: 0.021931307435047044, 189495: 0.021873220208106607, 26197: 0.021806629359917718, 20639: 0.021806589224410977, 23322: 0.02179242954385748, 24964: 0.021778275941372452, 20735: 0.02171505385181086, 20633: 0.021654388988392597, 23294: 0.021623734708125147, 20683: 0.021609007953250065, 189355: 0.021549425910712686, 189356: 0.02152032273722934, 189544: 0.02145274895005168, 84874: 0.021423986360092535, 20447: 0.021422158008170714, 23239: 0.021420241178460524, 84781: 0.021350324744721656, 23253: 0.021349365738815636, 23286: 0.02126120769747028, 189426: 0.021257337372599733, 84819: 0.021237613621440648, 84799: 0.021222255243779884, 84872: 0.021185763608577976, 189490: 0.021159636568931706, 84877: 0.021096774109161293, 20649: 0.021072176757289337, 20657: 0.021062273585530807, 189651: 0.02105496466828561, 20423: 0.021038362874528465, 189309: 0.021034104571875784, 84569: 0.020985885180843857, 23413: 0.020917206848287764, 20609: 0.02088642663329515, 20753: 0.02087049841637856, 20420: 0.020854649573905697, 20541: 0.020782723402695123, 84921: 0.020746373862605114, 189423: 0.020704261464072532, 189559: 0.020629639968316026, 189666: 0.020628303899394067, 189460: 0.02057243661736695, 24960: 0.020542948222338236, 189602: 0.020510575372217867, 189505: 0.020506603512143253, 84890: 0.0204741740104979, 84932: 0.020469487509159202, 84647: 0.020429045593017735, 20428: 0.02040526800992724, 84643: 0.020384440490980153, 84563: 0.020343642427656225, 20718: 0.020291854060438562, 189398: 0.020189652652112384, 189431: 0.020116746935105793, 20570: 0.019980535673638458, 23313: 0.019828519318767387, 84736: 0.019827959085328944, 189342: 0.01982768933718368, 84821: 0.01981675048640084, 84548: 0.01979528946241957, 84747: 0.01972456811238607, 20591: 0.019626931291822903, 189392: 0.01950250770253351, 189462: 0.019498640075782038, 84811: 0.019495090890056677, 189771: 0.01946889884235487, 189775: 0.019463889930290065, 189483: 0.019446590146573604, 25800: 0.019427287978395123, 84679: 0.019396731123616572, 20678: 0.019375135026015337, 84588: 0.019330307852278968, 84934: 0.019316485271130175, 84685: 0.01928286151684182, 23240: 0.019225230637113812, 189662: 0.019191711540329382, 189680: 0.019161646784342685, 84753: 0.01915677787252175, 20624: 0.019150020018235014, 24923: 0.019123917074798193, 189443: 0.01911079940152037, 84596: 0.019102447508257056, 189484: 0.0190984999824218, 23266: 0.018981835794602785, 24943: 0.018971332898928703, 23297: 0.018958691293422397, 23336: 0.01895315215121571, 20449: 0.018929781253667323, 84757: 0.01888952907741568, 84875: 0.018871607787141722, 189366: 0.01883080719841695, 20443: 0.018788424035347927, 84887: 0.018727241481473107, 84574: 0.018726423892486996, 84543: 0.018711320741488003, 84584: 0.01870865104933719, 23265: 0.01865462909616969, 189592: 0.018642195169427583, 189673: 0.018641470851518217, 84726: 0.018613262905145873, 189383: 0.018521979301390264, 84864: 0.018404695949243762, 189741: 0.01834707226088328, 20425: 0.0183168921861669, 189640: 0.018275258113178542, 84610: 0.018261384925052172, 84585: 0.01824791693384639, 84734: 0.018239233651444735, 189486: 0.018217369455566904, 20729: 0.01811018681481182, 20435: 0.01806778074096245, 189351: 0.017951471424206274, 189622: 0.017935138299967478, 20438: 0.01791151913581868, 20439: 0.01790485910283229, 23285: 0.017872324151703454, 24909: 0.017780781042608815, 20522: 0.01777028068837342, 20513: 0.01776991669603079, 189378: 0.0177554505404076, 84904: 0.01773460642819324, 26404: 0.017725680905668298, 23267: 0.017701568766950664, 189699: 0.01769859350781957, 189621: 0.017660507629810196, 189457: 0.017644183029313924, 84612: 0.01763776350946948, 189777: 0.01757530404341097, 20710: 0.017572139473585713, 20761: 0.017486836826203157, 24958: 0.01743844182003368, 84906: 0.017432244523153843, 20153: 0.017415613371676825, 84843: 0.017367227853750573, 189761: 0.017367061026034743, 84600: 0.01735655454786886, 20430: 0.01732840483601871, 23405: 0.01732557677392717, 23340: 0.01724381935485549, 189756: 0.0172365524868019, 189385: 0.017186219828996104, 189285: 0.017183746872446736, 189692: 0.017154286026847765, 20559: 0.017098087442229654, 84583: 0.017095356979666458, 23225: 0.017045435122523506, 189500: 0.017026849250360895, 84599: 0.017016008243135938, 189393: 0.017012681670995032, 84720: 0.01700663532760517, 189446: 0.017003202282463825, 189628: 0.01698823971458752, 189718: 0.01696389269278887, 20418: 0.016960854931706404, 189634: 0.0169568354100711, 84841: 0.016956292106783958, 84708: 0.01694342900278837, 20719: 0.016910706636380824, 189670: 0.016892330598875648, 189442: 0.0168644165972677, 189691: 0.01683092953826381, 189554: 0.0168079521579029, 84589: 0.016801885564473172, 189364: 0.01679776843682039, 189381: 0.01679201159827227, 20509: 0.01673362584806485, 20759: 0.016732075114634763, 84593: 0.016719520584144727, 20758: 0.016713642920685803, 20663: 0.016700765456992103, 84659: 0.0166795553862781, 189555: 0.016651281817191076, 20421: 0.016572294429604683, 84860: 0.01656091100102466, 189533: 0.016532962905729195, 84713: 0.016528114718632744, 189323: 0.01651273611303757, 84579: 0.016511009466361044, 189553: 0.016505599433773752, 84916: 0.01643604915206492, 189772: 0.016332749385331972, 189671: 0.016309924257535222, 189491: 0.0162824634677636, 24931: 0.01627613904431844, 20681: 0.016262896119710036, 189615: 0.016262890024277252, 189538: 0.016204905055376598, 189439: 0.016175627158694975, 84925: 0.01613253362188696, 189614: 0.016087883435909826, 25766: 0.016035830624186363, 189451: 0.015993427412597083, 23412: 0.015982625604567126, 20491: 0.01597674319114392, 24908: 0.015969799510538146, 23393: 0.015944220845810524, 189611: 0.01592117690324461, 24987: 0.01587601153725501, 20558: 0.01583437236154613, 189685: 0.01578848425350503, 84883: 0.01572580985487891, 189308: 0.015674644550677738, 23230: 0.01567101122144454, 84779: 0.01562233081925261, 23241: 0.015609050203538347, 189689: 0.015590905107655255, 20283: 0.015567405028901393, 25773: 0.015567290971699038, 84826: 0.015558964758247026, 84805: 0.015550416031452494, 20564: 0.015539699086514619, 84741: 0.015518394691689159, 189632: 0.015499782512822895, 25758: 0.01549491951164302, 24938: 0.015437201591773399, 84557: 0.01529929707720935, 84608: 0.015293255035316379, 189710: 0.015279003564057895, 20497: 0.01526724494501973, 189558: 0.015260658780819763, 84914: 0.015236272570810828, 189408: 0.015226293700807442, 23394: 0.015131174031184622, 20668: 0.015124580385948525, 84630: 0.015100750093004336, 84695: 0.01508732817247557, 189314: 0.015060097543902709, 25771: 0.015058772899120347, 84917: 0.015040976824311386, 189551: 0.015026494536444246, 189513: 0.014999559860094836, 189380: 0.014976223018605118, 84870: 0.014966217682452324, 84717: 0.014963593704482933, 24977: 0.014963163265592562, 189496: 0.01496151039485472, 189616: 0.014956046476951961, 84897: 0.014936948564642243, 20640: 0.014881803348164128, 20596: 0.014853781797992122, 189436: 0.014849902151181074, 189421: 0.014834623297511233, 24989: 0.01482113010059132, 189298: 0.014813473173629578, 24983: 0.01479469949434496, 20489: 0.01478081028625551, 84601: 0.014771888759597486, 20692: 0.014736021805447308, 20500: 0.014709226876751958, 23328: 0.014701469367862914, 24929: 0.0146200731587654, 189590: 0.014603303380668165, 20651: 0.014523009235699425, 84567: 0.014508876406758, 24933: 0.014504120528833658, 189334: 0.014502060427858433, 24953: 0.014489311998703675, 25765: 0.014474791126464586, 189603: 0.014471528689330998, 189782: 0.014427845827475038, 84613: 0.01442245262778241, 84536: 0.014400567905860268, 84796: 0.01438487605884021, 26204: 0.014383622474161074, 189712: 0.014376629705188326, 24965: 0.014365497079890686, 189560: 0.01430440920613624, 20574: 0.014267714851115287, 189652: 0.014263119385876384, 20665: 0.014223591441951108, 189498: 0.014209170756461394, 20399: 0.014201742736698855, 189503: 0.014195477256385923, 20765: 0.01418070727262635, 23269: 0.014166291381510966, 84731: 0.014126705579448326, 84924: 0.014118574757285837, 84724: 0.014102577969046275, 84885: 0.014049575441035451, 24993: 0.014024588976204348, 189535: 0.014006136467795747, 23332: 0.014004407432057903, 84808: 0.013979199647513233, 23252: 0.013972355099034647, 84828: 0.01393998123610711, 23351: 0.01391588406997575, 189447: 0.013897323586082843, 84791: 0.013888896424099496, 189754: 0.013846149397618211, 23283: 0.01381660218747192, 189499: 0.013810711424206463, 189328: 0.013783970572122127, 23383: 0.01378028007777542, 84575: 0.013766527957255852, 84848: 0.013758403149531355, 23317: 0.01375262773563775, 20457: 0.013751275004334687, 189543: 0.01371932721500269, 26217: 0.01371147118844183, 26408: 0.013710295110627142, 20698: 0.013710021423569647, 23276: 0.013691023882579072, 23362: 0.013690806408365338, 189667: 0.013624990465515205, 84559: 0.013613118344238905, 23392: 0.013575628082701994, 84581: 0.01354826038391008, 189758: 0.013520304999888504, 189534: 0.013514079280703485, 189641: 0.013497960353550073, 23300: 0.013489211838161924, 189655: 0.013486469075937615, 25741: 0.01348042369141693, 20686: 0.013478250817891618, 84623: 0.013466490531822492, 20643: 0.013453578782290195, 189747: 0.01344543080456221, 189512: 0.01343863840163003, 23258: 0.013435872587984284, 189354: 0.01339832872998319, 20479: 0.013344835289539095, 20656: 0.01334165063314105, 20494: 0.013323619943122192, 189720: 0.013305032431826878, 20645: 0.013287962302108252, 20501: 0.013238740337408952, 23282: 0.013237087320066973, 189757: 0.013224977451494294, 23262: 0.013178429190666264, 84790: 0.013157387161955247, 25775: 0.013145548095493103, 189723: 0.013142126364230868, 24902: 0.013116998290162142, 23400: 0.013113738198070993, 84900: 0.013097292213219485, 26411: 0.013085282719077515, 24892: 0.013058159807890195, 84852: 0.013041859552816065, 24939: 0.013029593694543437, 25779: 0.012998785097561512, 26406: 0.012989054951313573, 84641: 0.012934793195818708, 84540: 0.01288322297802268, 20575: 0.012883125947635217, 23327: 0.01287845390580883, 20389: 0.01285752999861714, 84929: 0.01282117951816024, 84777: 0.012815775273553867, 20337: 0.012762559296217257, 23320: 0.012750408098995573, 20424: 0.012731903014558902, 24919: 0.012650460095058397, 84768: 0.012643221439547042, 189697: 0.012632333733048298, 189401: 0.012627092430578857, 84818: 0.01260046510571733, 23371: 0.012597509061886293, 20448: 0.012587945053401104, 23341: 0.012545704140055475, 84683: 0.012532843837862908, 189664: 0.012488434568336136, 189464: 0.012429957822478141, 84856: 0.012424506323419049, 24927: 0.012418246732103842, 20709: 0.012372446695045706, 23395: 0.01236424415881093, 23389: 0.012355172211954568, 24903: 0.012305201314023049, 25768: 0.01230272685204243, 23308: 0.012294182362346444, 84665: 0.012283155285709668, 23373: 0.012269805099035153, 25786: 0.012252229057124385, 189753: 0.012241466546752346, 84631: 0.01221319078004483, 84639: 0.012183394298971162, 20646: 0.012165140905959144, 84677: 0.012112041449328688, 20682: 0.012099669815027823, 84590: 0.012090216609614691, 84562: 0.012088247516929081, 20738: 0.01208824732968765, 84640: 0.012085690508724985, 84786: 0.01206747690426661, 189390: 0.012057039597177962, 84751: 0.012039800567171273, 84803: 0.01203949576469566, 23361: 0.012009618641262541, 84869: 0.011962825499226476, 84595: 0.011946515291182878, 24915: 0.011915783501790993, 189419: 0.011909174809670893, 20436: 0.011899560187600555, 23226: 0.011899530431983423, 23364: 0.011886381742192666, 84758: 0.011869051751606864, 84632: 0.011828367703589825, 20505: 0.011824145978844762, 84629: 0.011822990624716118, 25801: 0.011818485982891418, 20542: 0.011816084474386599, 20511: 0.011748676157209086, 20660: 0.011727332929123417, 20455: 0.01172196732391874, 23290: 0.011677877781599856, 84606: 0.011667339831339366, 189318: 0.011658166489100017, 23368: 0.011651759523051855, 84749: 0.011650967532064942, 189330: 0.011646525780766146, 23409: 0.011621991893465666, 189361: 0.011569480167364449, 84663: 0.011537281890836021, 20493: 0.011536197021403877, 26206: 0.011513505862330253, 20641: 0.011511302261595645, 25772: 0.011493253291979576, 20724: 0.011488475187599875, 20597: 0.01146984872798632, 20587: 0.011457486542491152, 84824: 0.01141416390693617, 84733: 0.011396121861707426, 23193: 0.011373074478488912, 20666: 0.011345945242082198, 84832: 0.011214706883816894, 24924: 0.011205583306895639, 84778: 0.011197005105277014, 84880: 0.01119338560141668, 24991: 0.011184325235331378, 84807: 0.011179085162639872, 23243: 0.011159999537906279, 23245: 0.011153471091558884, 189414: 0.011147376899078631, 20679: 0.01114203274186125, 189703: 0.01113069951803132, 24880: 0.011123269485742097, 189731: 0.011109348122706903, 20417: 0.011105085221485306, 84908: 0.01108038733760195, 84930: 0.011070070085789627, 20419: 0.011056584591193162, 84926: 0.01104414174221711, 20445: 0.011043160457029782, 84554: 0.011027189864623619, 20481: 0.011018864314275057, 20429: 0.011007120951935514, 23218: 0.010994982690783094, 26210: 0.01098273380958597, 23370: 0.010964725470335487, 84650: 0.010944459558029683, 84604: 0.010922706523962608, 84845: 0.010898989974185258, 189676: 0.010893924638562324, 25805: 0.010881499120658673, 84851: 0.010880686734211668, 84552: 0.010839987665430074, 84923: 0.010822065256575846, 84865: 0.010804019642416165, 84806: 0.010780724235396999, 25797: 0.010763204251008247, 84549: 0.010747676706570196, 26414: 0.010744594948106083, 189416: 0.010714515204905363, 20550: 0.010710118111932286, 189714: 0.010708775882127161, 189715: 0.010645288090908216, 84535: 0.010644936085775733, 20482: 0.010642401275384808, 23399: 0.01064127527408288, 23287: 0.010611007692903266, 24963: 0.010563710308424295, 26192: 0.0105490621602627, 189541: 0.010517064688789787, 23408: 0.010500694932734127, 23293: 0.010462602670090125, 189417: 0.0104575521901169, 84616: 0.010444238114899454, 23402: 0.010428655639214444, 24979: 0.010413935451163292, 84837: 0.01041128954478689, 189637: 0.01040428129794881, 20495: 0.010396548889811627, 189456: 0.010326278033758875, 20553: 0.010308060994881109, 25794: 0.010307646769238132, 189494: 0.010284385679369203, 84844: 0.010283588268418567, 23250: 0.010280856863730233, 84787: 0.010280042538107055, 84830: 0.010263562379719907, 84858: 0.01024443558557486, 84829: 0.01022137892325265, 23270: 0.010211177504152291, 20437: 0.010201559863637468, 23255: 0.010183234000311035, 20588: 0.010178389490429787, 20453: 0.010157514156007555, 26517: 0.010118675743392267, 84602: 0.010100193750622802, 84682: 0.010088131452764264, 26410: 0.01008597485788675, 24967: 0.01008430507863085, 84564: 0.010075903071509221, 20572: 0.010011216470513243, 20687: 0.010007512106914028, 189743: 0.009980552188714999, 23345: 0.009974127886782208, 23329: 0.009956231209070025, 25787: 0.00994380745403043, 20524: 0.009939190894375003, 84820: 0.009932847050067469, 84572: 0.009923628268189823, 84935: 0.009915483831305808, 84645: 0.009914971047441825, 84722: 0.009901996956930262, 20565: 0.009889637065001535, 189742: 0.009882318566839235, 189585: 0.009881938640415318, 84810: 0.009864871303206157, 189509: 0.009862475221974086, 84635: 0.009854960432279099, 189547: 0.00982237129851176, 84592: 0.009780508987209236, 24907: 0.009772198446225405, 23348: 0.009733899256927758, 84756: 0.00966967915494153, 84637: 0.009667415911587839, 26199: 0.009666190953218528, 20465: 0.009645681008268143, 84661: 0.009631418788910861, 84705: 0.009629359150848036, 20483: 0.009620378513970221, 189377: 0.00959136470587985, 84766: 0.009575983959549242, 84702: 0.009566009157931965, 23295: 0.009558205834407042, 26200: 0.009549306586703834, 26520: 0.009528876808438355, 84716: 0.009515931238885788, 84730: 0.009495127843077298, 84853: 0.009485324472474453, 24917: 0.009472558752836712, 189465: 0.009463342857973789, 23379: 0.009447190271310083, 189750: 0.009424937211356187, 189525: 0.009395601973239026, 189528: 0.009391106738498979, 20670: 0.009378776249368195, 84797: 0.00937790704213048, 84789: 0.009359796475928417, 20547: 0.009344212008239193, 26400: 0.009335147860285168, 23299: 0.009328061158494024, 84862: 0.009321031019352369, 84541: 0.009320329039462354, 189727: 0.009309618881846978, 20685: 0.009308629191713758, 20454: 0.009295030338791038, 189617: 0.009274934831294693, 20632: 0.009274014653224935, 24959: 0.00926248949753861, 84834: 0.00925201221498336, 189444: 0.009250974557644896, 20601: 0.009232198279803918, 23268: 0.00921610901709911, 24962: 0.009192486904541313, 20473: 0.00918710718135446, 23306: 0.009150488794110246, 20647: 0.00914418839435628, 20733: 0.00913273690624121, 24971: 0.009131934273519224, 84891: 0.0091214321042922, 84709: 0.009078090201700834, 20485: 0.009064404312531967, 84746: 0.009056289114110922, 24918: 0.009014748552228965, 23349: 0.0089981153381394, 189470: 0.008992308326101256, 20573: 0.00897955113069203, 25770: 0.008979521352859354, 84653: 0.008936740855453108, 20567: 0.008929630325692831, 84888: 0.008919910651143842, 20600: 0.008905268436866662, 24984: 0.008848480095467756, 20608: 0.008848434644382562, 84568: 0.008848207499647327, 20529: 0.008848111159832235, 20537: 0.008845231472189338, 25781: 0.008820319578426728, 20630: 0.008806528913832786, 23272: 0.008801226610445429, 20507: 0.008794873188798649, 189704: 0.008766296274185488, 84836: 0.008765052608538736, 26207: 0.008759552027720217, 84566: 0.008759196386466776, 23275: 0.008743299939759485, 189594: 0.008711792564767928, 20506: 0.008704926886447409, 20563: 0.00870359285946731, 25774: 0.008701297821561543, 189508: 0.008699526374614196, 24978: 0.008691370927583585, 84827: 0.00866902828842105, 20510: 0.008656336509482648, 23112: 0.008628938153930461, 189514: 0.008621330384271755, 189449: 0.008610165263613672, 189713: 0.008602334279489067, 20708: 0.008599736230997667, 189506: 0.008598548091169762, 20722: 0.008575138461607448, 84866: 0.00854860081446606, 26201: 0.00854411793634203, 24949: 0.008508201159012818, 20620: 0.008500585350653591, 20621: 0.008487881228790505, 84859: 0.008482655324699393, 189675: 0.008467534133537333, 84666: 0.008466407889840032, 84553: 0.008460034287237477, 23376: 0.008442471398693209, 24911: 0.008437111355323578, 20764: 0.008384897309112153, 84831: 0.008375527511895145, 20636: 0.008365177137662792, 24968: 0.008361751955265725, 84676: 0.008350789627086939, 189582: 0.008341924984004757, 20560: 0.008340355527596476, 24914: 0.00830241318665329, 23353: 0.008261967144690257, 20554: 0.008244372045378612, 84911: 0.008235219251782982, 84817: 0.008229472984272861, 20662: 0.008226619360117975, 84603: 0.008226177066376284, 20566: 0.008224049296252174, 20569: 0.008214636554728972, 84692: 0.008201805775550032, 26412: 0.008196229255637079, 20189: 0.008191042014466259, 20677: 0.008189839217737449, 23403: 0.008184474881328547, 84915: 0.008181320434100364, 20674: 0.008142641843451702, 84863: 0.008136586517156569, 189721: 0.008123570582816627, 23358: 0.008107270655980733, 20398: 0.008095829163599773, 84892: 0.008077733701399794, 84587: 0.008056666986700932, 20492: 0.008027597082100298, 84788: 0.008027585639626841, 23414: 0.00802742856666662, 84648: 0.008014290701025361, 189745: 0.007977223736395575, 84855: 0.007948603248730857, 24941: 0.007947190394550325, 20736: 0.007936397302882578, 23247: 0.007934581696928863, 84809: 0.007921466179271214, 189749: 0.007920154156563015, 20551: 0.007899865702356089, 20530: 0.00787631032915417, 84638: 0.007865611336531866, 20638: 0.007846131705370836, 20707: 0.00783684613664348, 20653: 0.0078055308623625605, 189369: 0.007791769523415854, 84835: 0.007789685468580438, 189469: 0.007772796454744002, 84651: 0.007769644830211245, 189729: 0.007765388829031374, 84920: 0.007748136474439701, 84556: 0.00774282042820533, 189595: 0.0077426380735424695, 23279: 0.007738047653551095, 20444: 0.007730899219551977, 84840: 0.007707029564502266, 20517: 0.0076976594779962285, 84667: 0.007669275778059513, 25790: 0.007639016943699159, 24948: 0.00761819166614561, 23384: 0.0076053092342788925, 84823: 0.007593249805832526, 84636: 0.007580708595171035, 189609: 0.007549987082760707, 84576: 0.007539733621045018, 84910: 0.007526913896697955, 20512: 0.007469483659267883, 84785: 0.0074472598442733495, 20713: 0.00742834834058935, 20458: 0.007421096684938253, 20441: 0.0074165123869935285, 23396: 0.00741074713143531, 20538: 0.0073877979159800225, 84703: 0.007364359616786669, 189770: 0.007353015387925084, 84598: 0.007352837546775409, 23325: 0.00735055527312507, 25789: 0.00734851932702531, 189319: 0.007343616700940829, 20531: 0.007338692477379017, 84706: 0.007337698593660708, 23347: 0.0073343747859203135, 84780: 0.007300042486414001, 20684: 0.007265958309910151, 189648: 0.00724754024528838, 84656: 0.007225677996257008, 84718: 0.0072047317974638, 20672: 0.00718124923914017, 23278: 0.007159830362663226, 20488: 0.007144545648854702, 84896: 0.007122202323821797, 84684: 0.007115246567773282, 189593: 0.007107471177888955, 84846: 0.007105781493296738, 20446: 0.00708768769601311, 84752: 0.007069299752199322, 84729: 0.007046807389487236, 84744: 0.00700141311270056, 84607: 0.006984387646951432, 84622: 0.00697326622086153, 189384: 0.0069563111765082765, 20629: 0.006954085777189629, 189501: 0.006928959605994283, 25791: 0.0069064450773299296, 189344: 0.006862835489150219, 84905: 0.0068515386329495685, 189497: 0.006849812095481439, 84784: 0.006833783903356283, 84739: 0.006832248935282494, 84931: 0.006800476222361236, 84759: 0.006794585282118533, 23311: 0.0067870927094540075, 189773: 0.006771525246717853, 189338: 0.006770440391175012, 26409: 0.006757235228752992, 84701: 0.006744395350862988, 84560: 0.006737059353422564, 20498: 0.006730845560739369, 20387: 0.006668175849218236, 23369: 0.006659397629183642, 26198: 0.0066361600572974605, 84861: 0.006600726728462876, 20526: 0.006574550731555407, 84686: 0.006515045250518309, 84660: 0.006498711893158624, 23305: 0.0064791900444613445, 25780: 0.006468493830677871, 20478: 0.006466456028365136, 84794: 0.0064526795499890224, 84792: 0.006427444149397756, 23319: 0.006399072538945156, 84615: 0.006379120569626188, 84577: 0.006376674645680051, 84605: 0.006361669823541393, 84913: 0.006348361277130372, 84742: 0.006343878293205912, 84764: 0.006309124624052518, 23324: 0.006277499740072282, 23204: 0.006266583917505455, 84771: 0.006250191913682939, 23254: 0.006230658330284301, 24944: 0.006226224561059319, 189647: 0.006195967765267176, 20571: 0.00616709483789881, 84633: 0.006166609809377843, 26205: 0.006162024247970461, 23274: 0.00611742018073455, 189606: 0.0061139902085333145, 26194: 0.0060764531464815134, 84800: 0.006065049876907366, 25784: 0.006047972211051456, 84594: 0.006046219422658799, 84783: 0.006044140436303139, 189445: 0.006016523115342583, 189332: 0.006014331365442916, 84854: 0.005993645507269262, 20642: 0.005984851420744668, 84833: 0.005975862382211626, 24920: 0.005971799186388064, 24952: 0.005926039467242856, 25785: 0.005916538332589873, 84795: 0.0059077518935550955, 84655: 0.005897295687317401, 189315: 0.005893746059379738, 84881: 0.005886520527703817, 20714: 0.0058651026392961885, 23237: 0.005864564136550775, 84621: 0.005861499570444416, 23281: 0.005855868925945555, 20637: 0.005837599075598247, 84804: 0.005836145392250416, 189539: 0.00583019386345016, 84634: 0.005803757636385769, 20577: 0.005792699929490839, 20614: 0.005759955449847768, 84816: 0.005723316146097119, 84657: 0.005702749065292484, 84551: 0.005682426425551421, 84565: 0.005619877763177349, 20675: 0.005580671139427689, 26203: 0.005549971141748548, 20527: 0.005532694596997129, 20476: 0.005490760739361942, 23326: 0.005442515906072112, 20285: 0.0054272273144198215, 84712: 0.005415435481714912, 20598: 0.005409060159309256, 20612: 0.005407383638052927, 189624: 0.005383287022073707, 84664: 0.0052929848654346264, 24980: 0.0052898068496179855, 84876: 0.0052762457037232174, 189613: 0.005262302367304024, 84813: 0.005255408592754689, 20525: 0.0051686350496445484, 189406: 0.005167527930437066, 189683: 0.005166930051441365, 20584: 0.00515696593490858, 84745: 0.005138653974712176, 84670: 0.0051087841798842775, 20518: 0.005089482630932373, 84907: 0.005082374018391502, 189343: 0.004998138385877525, 20452: 0.004924987725232362, 23318: 0.004896509494347793, 20605: 0.004869458260265804, 20594: 0.0048479990324764525, 20520: 0.004818579271811879, 20712: 0.004804633131473228, 84767: 0.004801133530545058, 24921: 0.004796700436456954, 84672: 0.00475120645260445, 24946: 0.004703121598063464, 20749: 0.004685338307454588, 84901: 0.0046642338738367055, 84711: 0.004659518752545909, 84769: 0.004656483658819702, 84802: 0.00465058703986936, 84721: 0.00464974969317158, 23360: 0.004640364852337079, 84694: 0.004621449259018118, 23307: 0.004595177474375405, 20622: 0.0045403393277233155, 84693: 0.00450994185000871, 25804: 0.004457022913444416, 84715: 0.004435186539276066, 84558: 0.004353952953910738, 84945: 0.00434689540863427, 20546: 0.004337642465844133, 26407: 0.0043197465211316085, 20731: 0.0043067739467306495, 84620: 0.00429481242768989, 84775: 0.004286546798636855, 84671: 0.004181959506788157, 84732: 0.0041804355615786895, 189586: 0.0041294604227183505, 20763: 0.004088073761813038, 23397: 0.004084662728370221, 84696: 0.004081505301618996, 189422: 0.004069436317787276, 84642: 0.004045448345294275, 84619: 0.004044241027232248, 25760: 0.004036737757771735, 24928: 0.003994985445445601, 84750: 0.003977422617148375, 23291: 0.0038784292603973544, 84728: 0.00387838686634176, 189635: 0.0038547797992959953, 84533: 0.003827365433619839, 20240: 0.003763233045334181, 84625: 0.003683105642084306, 84893: 0.0036694818967200815, 20355: 0.003651352596847051, 84618: 0.003591008955383763, 84902: 0.0034272126573083503, 84628: 0.0034255393604875157, 84658: 0.003368918286833498, 84561: 0.003367707181356154, 25788: 0.003362861575447427, 84928: 0.003301278024867524, 84723: 0.0032235047699415986, 20514: 0.0032022084284437946, 84539: 0.0031546437863166515, 84814: 0.003151173951959249, 84927: 0.0031388040150700154, 84868: 0.0031086567746372133, 84700: 0.0030910363734585706, 84578: 0.003077544225362884, 84586: 0.0030058346572671493, 189626: 0.0029998324604025854, 20556: 0.0029965215591417705, 84839: 0.0029925074628198026, 84889: 0.00298982882283962, 23312: 0.0029781891631492593, 189371: 0.0029432974861512418, 189663: 0.002877563063586054, 84850: 0.002871807779935636, 25769: 0.002851160634102552, 84754: 0.0028346759687983667, 84847: 0.0027931831150301723, 84644: 0.0027923501126790717, 24912: 0.002764603072994611, 84627: 0.002733285631407495, 84895: 0.0027021351399880837, 189350: 0.0026669955497650184, 84727: 0.0026326121809891008, 84689: 0.002621406630232262, 84740: 0.0024695451688740416, 20502: 0.0024545629567719364, 84894: 0.0024157715852378935, 20516: 0.0023784551604203695, 23375: 0.002356244797543458, 84707: 0.0022939895285431534, 20615: 0.0022218585806865654, 84884: 0.0021457745330249658, 84898: 0.00211642077980698, 84933: 0.002098709834297921, 84617: 0.0020640625627371062, 84545: 0.002007743598584629, 20477: 0.0019296734473371938, 84755: 0.0018300278352618563, 84614: 0.0017952837777368113, 20472: 0.0017588200486762424, 84710: 0.001692127195016829, 84954: 0.0015128633725933363, 84654: 0.00148923034032936, 84886: 0.0014173379843991833, 23284: 0.001392707028379296, 84760: 0.001363152023755672, 20734: 0.0011354057577386122, 20673: 0.0, 20727: 0.0, 84580: 0.0, 84681: 0.0, 84687: 0.0, 84776: 0.0, 84882: 0.0, 84903: 0.0, 189477: 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"This is sorted_similarity_dict \", most_similar_docs(84570, full_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dictionary above we can see that the document 84570 with title \"Muhammad Ali's boixng record: 56 wins, 5 losses, 37 knowckouts\" is the document where the 5 closest recommendations don't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "4 (continued). Why do you think that's the case (i.e., that a document has non-sensical closest-document pairs, or that it's hard to find such an outlier document). Please limit your response to 2-3 sentences.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "I think the document has non-sensical closest-document pairs because this document contains basically names of people, cities and numbers refering to dates and boxing scores, one of the names that repeats the most is New York. Thus, since the first document is just a collection of these words, any other document that is actually a news story will not have any sense in relation to document 84570 but could have some >0 cosine similarity if one of the name of the cities appears in the document. For instance, the documents with the highest cosine similarity w.r.t document 84570 all are related to some story happening in New York. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.3 TF-IDF [9 points total]</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.3a Create TF-IDF vectors [3 points]**\n",
    "\n",
    "\n",
    "Instead of using BoW vectors with raw counts, let's try using term frequency-inverse document frequency (TF-IDF) vectors. Implement the `create_TFIDF()` function below. Its inputs and outputs are identical to `create_BoW()`:\n",
    "- accept two inputs: the output from `load_corpus()` (e.g., `samp_tokenized` or `full_tokenized`) and the output from `create_vocab()` (e.g., `samp_vocab` or `full_vocab`)\n",
    "- it creates TF-IDF representations for each document (not the titles) and returns it. **NOTE:** You don't need to perform any smoothing or additional, vector normalization of each TF-IDF vector across dimensions on a per-doc basis, but if you wish to, you can. \n",
    "- specifically, the output needs to be a dictionary mapping each document id to a NumPy array $\\vec x \\in \\mathbb{R}^V$, where $x_i$ is the TF-IDF for the word whose index into the vocabulary is $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def helper_TF(wordDict, tokens):\n",
    "    tfDict = {}\n",
    "    tokensCount = len(tokens)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(tokensCount)\n",
    "    return tfDict\n",
    "\n",
    "def create_TFIDF(tokenized_texts: Dict[int, List[str]], vocab: Dict[str, int]) -> Dict[int, np.ndarray]:\n",
    "    \n",
    "    \"\"\"\n",
    "    bow_dict: Dict[int, np.array] = {}\n",
    "    num_docs = len(tokenized_texts.keys())\n",
    "    \n",
    "    # Unique words in documents\n",
    "    uniqueWords = set([])\n",
    "    for doc_id, tokens in tokenized_texts.items():\n",
    "        uniqueWords = uniqueWords.union(set(tokens))\n",
    "        \n",
    "    word_occ_per_doc = {}\n",
    "    # Compute the occurrence of each word in each document\n",
    "    for doc_id, tokens in tokenized_test.items():\n",
    "        word_occ_per_doc[doc_id] = dict.fromkeys(uniqueWords, 0)\n",
    "        for word in tokens:\n",
    "            word_occ_per_doc[doc_id][word] += 1\n",
    "    \n",
    "    \n",
    "    print(uniqueWords)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    bow_dict: Dict[int, np.array] = {}\n",
    "    num_docs = len(tokenized_texts.keys())\n",
    "    \n",
    "    count_docs_with_token = {}\n",
    "    for idx, term in enumerate(vocab):\n",
    "        curr_count = 0\n",
    "        for doc_id, tokens in tokenized_texts.items():\n",
    "            if term in tokens:\n",
    "                curr_count += 1\n",
    "        count_docs_with_token[term] = curr_count\n",
    "\n",
    "    for doc_id, tokens in tokenized_texts.items():\n",
    "        doc_list = [0] * len(vocab)\n",
    "        for idx, term in enumerate(vocab):\n",
    "            term_freq = tokens.count(term)#/len(tokens)\n",
    "            # Ask how we can improve this line of code\n",
    "            num_docs_with_term =count_docs_with_token[term] # sum([1 if term in toks else 0 for toks in tokenized_texts.values()])\n",
    "            inv_doc_freq = np.log(num_docs/num_docs_with_term)\n",
    "            doc_list[idx] = term_freq * inv_doc_freq\n",
    "        bow_dict[doc_id] = doc_list\n",
    "    return bow_dict\n",
    "\n",
    "# Ask about data type in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_TF(word_count_dict, tokens, vocab):\n",
    "    tfDict = dict.fromkeys(vocab, 0)\n",
    "    tokensCount = len(tokens)\n",
    "    #print(wordDict)\n",
    "    for word, count in word_count_dict.items():\n",
    "        tfDict[word] = count / float(tokensCount)\n",
    "    return tfDict\n",
    "\n",
    "def helper_IDF(word_occ_per_doc, vocab):\n",
    "    import math\n",
    "    N = len(word_occ_per_doc.keys())\n",
    "    \n",
    "    idfDict = dict.fromkeys(vocab, 0)\n",
    "    for doc in word_occ_per_doc.keys():\n",
    "        for word, val in word_occ_per_doc[doc].items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "\n",
    "def create_TFIDF_2(tokenized_texts: Dict[int, List[str]], vocab: Dict[str, int]) -> Dict[int, np.ndarray]:\n",
    "    \n",
    "    bow_dict: Dict[int, np.array] = {}\n",
    "    num_docs = len(tokenized_texts.keys())\n",
    "    \n",
    "    # Unique words in documents\n",
    "    uniqueWords = set([])\n",
    "    for doc_id, tokens in tokenized_texts.items():\n",
    "        uniqueWords = uniqueWords.union(set(tokens))\n",
    "        \n",
    "    word_occ_per_doc = {}\n",
    "    # Compute the occurrence of each word in each document\n",
    "    for doc_id, tokens in tokenized_texts.items():\n",
    "        word_occ_per_doc[doc_id] = dict.fromkeys(uniqueWords, 0)\n",
    "        for word in tokens:\n",
    "            word_occ_per_doc[doc_id][word] += 1\n",
    "    \n",
    "    #for doc_id, tokens in tokenized_texts.items():\n",
    "    tfDict_per_doc = helper_TF(word_occ_per_doc[doc_id], tokens, uniqueWords)\n",
    "    idfDict_per_doc = helper_IDF(word_occ_per_doc, uniqueWords)\n",
    "    \n",
    "    print(len(uniqueWords))\n",
    "    print(len(vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In the sanity check cell below, we encourage you to run your `create_TFIDF()` on the sample data. In our solutions, we did not implement smoothing or any post- normalization (you can if you want), and our output for the cell below yielded `[0, 0, 0, 0, 0, 0, 0, 0, 0, 23.07085806, 6.08197662, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35599"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.070858062030304, 6.0819766216224656, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# sanity check cell\n",
    "samp_tfidf = create_TFIDF(samp_tokenized, samp_vocab)\n",
    "print(samp_tfidf[20621][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35599\n",
      "35599\n"
     ]
    }
   ],
   "source": [
    "## TESTING \n",
    "# sanity check cell\n",
    "samp_tfidf_2 = create_TFIDF_2(full_tokenized, full_vocab)#(samp_tokenized, samp_vocab)\n",
    "#print(samp_tfidf_2[20621][:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(20621, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.070858062030304, 6.0819766216224656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.577796618689757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4327906486489863, 0.0, 14.281959752685427, 0.0, 0.0, 0.0, 0.0, 4.054651081081644, 0.0, 10.986122886681098, 1.6218604324326575, 3.243720864865315, 10.986122886681098, 2.4327906486489863, 10.986122886681098, 0.0, 9.887510598012987, 0.0, 9.887510598012987, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6218604324326575, 2.4327906486489863, 8.788898309344878, 8.788898309344878, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 6.591673732008658, 0.0, 0.0, 6.591673732008658, 0.4054651081081644, 1.6218604324326575, 0.0, 0.4054651081081644, 1.6218604324326575, 0.0, 1.6218604324326575, 0.0, 0.0, 0.0, 0.0, 0.0, 5.493061443340549, 0.0, 1.2163953243244932, 0.0, 5.493061443340549, 5.493061443340549, 1.2163953243244932, 5.493061443340549, 0.8109302162163288, 0.4054651081081644, 0.0, 0.0, 0.0, 4.394449154672439, 0.0, 0.8109302162163288, 0.0, 0.0, 0.4054651081081644, 4.394449154672439, 0.8109302162163288, 4.394449154672439, 4.394449154672439, 1.2163953243244932, 1.2163953243244932, 4.394449154672439, 0.4054651081081644, 0.8109302162163288, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 4.394449154672439, 4.394449154672439, 1.2163953243244932, 0.8109302162163288, 0.4054651081081644, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.295836866004329, 3.295836866004329, 3.295836866004329, 0.4054651081081644, 3.295836866004329, 0.8109302162163288, 0.8109302162163288, 3.295836866004329, 3.295836866004329, 0.0, 3.295836866004329, 0.4054651081081644, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 0.8109302162163288, 0.4054651081081644, 0.8109302162163288, 3.295836866004329, 0.0, 0.8109302162163288, 0.8109302162163288, 0.8109302162163288, 3.295836866004329, 0.4054651081081644, 0.4054651081081644, 3.295836866004329, 0.4054651081081644, 3.295836866004329, 0.8109302162163288, 3.295836866004329, 0.0, 3.295836866004329, 0.4054651081081644, 0.0, 0.0, 0.8109302162163288, 0.4054651081081644, 0.8109302162163288, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), (84549, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.243720864865315, 15.380572041353537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.084735175349207, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6218604324326575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 1.6218604324326575, 1.2163953243244932, 1.2163953243244932, 5.493061443340549, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 1.2163953243244932, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.394449154672439, 4.394449154672439, 1.2163953243244932, 4.394449154672439, 0.8109302162163288, 4.394449154672439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 0.8109302162163288, 0.8109302162163288, 3.295836866004329, 3.295836866004329, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), (189782, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.4327906486489863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 14.281959752685427, 0.0, 0.4054651081081644, 0.0, 0.0, 2.4327906486489863, 0.0, 0.0, 1.6218604324326575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6218604324326575, 0.8109302162163288, 0.0, 0.0, 8.788898309344878, 8.788898309344878, 8.788898309344878, 0.0, 0.0, 0.0, 0.0, 0.0, 2.027325540540822, 0.0, 0.0, 2.4327906486489863, 7.690286020676768, 0.0, 0.0, 0.0, 0.0, 2.027325540540822, 0.8109302162163288, 0.0, 2.027325540540822, 0.8109302162163288, 0.0, 0.8109302162163288, 0.0, 0.8109302162163288, 6.591673732008658, 6.591673732008658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2163953243244932, 0.0, 0.8109302162163288, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.2163953243244932, 0.0, 0.8109302162163288, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 1.2163953243244932, 0.8109302162163288, 0.0, 1.2163953243244932, 1.2163953243244932, 1.2163953243244932, 0.0, 0.0, 0.4054651081081644, 0.8109302162163288, 1.2163953243244932, 0.8109302162163288, 0.0, 0.0, 0.4054651081081644, 0.0, 0.8109302162163288, 0.0, 4.394449154672439, 4.394449154672439, 4.394449154672439, 4.394449154672439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.0, 0.8109302162163288, 0.8109302162163288, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8109302162163288, 0.0, 0.0, 0.4054651081081644, 0.8109302162163288, 0.4054651081081644, 0.0, 0.8109302162163288, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 3.295836866004329, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.0, 0.0, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.0, 0.0, 0.4054651081081644, 0.4054651081081644, 0.4054651081081644, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 2.1972245773362196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098, 1.0986122886681098])])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_tfidf.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that your code is correct, simply run (i.e., **DO NOT EDIT**) the cell below to create TF-IDF representations for the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "# keys: 1139\n",
      "first 5 sorted keys: [20153, 20189, 20240, 20283, 20285]\n",
      "last 5 sorted keys: [189773, 189775, 189777, 189778, 189782]\n",
      "vocab size: 35599\n",
      "sample 1: [0.04918753061073368, 0.119613140077087, 0.17957788225197593, 0.15845107257527288, 0.190309157938663]\n",
      "sample 2: [0.04655248432801581, 0.049252469443506405, 0.13732426289856983, 0.11091575080269102, 0.12687277195910865]\n",
      "sample 3: [7.518334016333337, 0.7064041135534911, 1.9575655357307706, 0.0, 1.5835983959312387]\n"
     ]
    }
   ],
   "source": [
    "full_tfidf = create_TFIDF(full_tokenized, full_vocab)\n",
    "print(type(full_tfidf))\n",
    "print(\"# keys:\", len(full_tfidf.keys()))\n",
    "print(\"first 5 sorted keys:\", sorted(full_tfidf.keys())[:5])\n",
    "print(\"last 5 sorted keys:\", sorted(full_tfidf.keys())[-5:])\n",
    "print(\"vocab size:\", len(full_tfidf[20337]))\n",
    "print(\"sample 1:\", full_tfidf[20337][:5])\n",
    "print(\"sample 2:\",full_tfidf[20442][:5])\n",
    "print(\"sample 3:\",full_tfidf[20355][100:105])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.3b TF-IDF vs BoW [3 points]**\n",
    "\n",
    "Let's continue to use the full dataset, `CS287_news_full.csv`\n",
    "\n",
    "Please write code in the cell below that allows you complete the following exercise:\n",
    "1. Find three documents $d$ that satisfy the following property: Using document $d$ as a query, let $\\mathcal{S}$ denote the set of $k = 5$ closest documents when using BoW representations.  Let $\\mathcal{T}$ denote the set of $k = 5$ closest documents when using TF-IDF representations. We want $|\\mathcal{S} \\cap \\mathcal{T}| \\leq 1$ (i.e. $\\mathcal{S}$ and $\\mathcal{T}$ have at most 1 common overlap). \n",
    "2. For each $d$, display the titles of $\\mathcal{S}$ and $\\mathcal{T}$. Based on the titles, does $\\mathcal{S}$ or $\\mathcal{T}$ comprise better recommendations for $d$?  Please justify your answer.\n",
    "3. Based on what you know about BoW and TF-IDF, why do you think $\\mathcal{S}$ and $\\mathcal{T}$ ended up with different recommmendations?\n",
    "\n",
    "For clarity, we will explicitly ask you each of these questions in separate prompts below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: you may use this cell to set up any code to help answer this question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "1. Find three documents $d$ that satisfy the following property: using document $d$ as a query, let $\\mathcal{S}$ denote the set of $k = 5$ closest documents when using BoW representations. Let $\\mathcal{T}$ denote the set of $k = 5$ closest documents when using TF-IDF representations. We want $|\\mathcal{S} \\cap \\mathcal{T}| \\leq 1$ (i.e. $\\mathcal{S}$ and $\\mathcal{T}$ have at most 1 common overlap). Your cell below should output your answer (i.e., three document ids)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: answer the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "2. For each $d$, display the titles of $\\mathcal{S}$ and $\\mathcal{T}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: answer the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "2 (continued). Based on the titles, does $\\mathcal{S}$ or $\\mathcal{T}$ comprise better recommendations for $d$? Please justify your answer in ~2 sentences.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "3. Based on what you know about BoW and TF-IDF, why do you think $\\mathcal{S}$ and $\\mathcal{T}$ ended up with different recommendations? (2-3 sentences).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.3c PCA-based visualization of the TF-IDF represented dataset [3 points]**\n",
    "\n",
    "Let's continue to explore our full dataset (i.e., `CS287_news_full.csv`) some more. Recall that there are three natural divisions in our dataset, based on news source (i.e. New York Times, Reuters, Fox News). Can TF-IDF encode the political differences between these news sources? To answer this question, in the cell below: use the [Sci-Kit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) library to run a **principal components analysis** (PCA), and extract the two directions of maximum variation among the TF-IDF vectors. Plot each document in a 2D grid based on its PCA components and color the document in the plot based on its news source (i.e., your documents should encompass three distinct colors, one for each news source).\n",
    "    \n",
    "**Before running PCA, please scale the TF-IDF vector for each document so that it has unit norm.  This will put each document on the same scale, so that PCA can better highlight the different word distributions between documents.**\n",
    "\n",
    "Briefly explain what you see and justify why it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 1.09861229, 1.09861229,\n",
       "        1.09861229]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(samp_tfidf.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'PCA of TFIDF vectors')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: answer the above. Remember to include a plot\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def helper_source_code(source):\n",
    "    if source == 'New York Times':\n",
    "        return 0\n",
    "    elif source == 'Fox News':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "tfidf_array = np.array(list(full_tfidf.values()))\n",
    "normalized_array = normalize(tfidf_array)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(tfidf_array)\n",
    "X_r = pca.transform(tfidf_array)\n",
    "\n",
    "# Getting document sources\n",
    "id_source = full_data_df[['id', 'publication']]\n",
    "id_source_coded = np.array([helper_source_code(source) for source in id_source['publication'].tolist()])\n",
    "source_list = id_source['publication'].unique()\n",
    "\n",
    "plt.figure()\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "lw = 2\n",
    "\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], source_list):\n",
    "    idx_to_consider = np.where(id_source_coded == i) \n",
    "    plt.scatter(X_r[idx_to_consider, 0], X_r[idx_to_consider, 1], color=color, alpha=.8, lw=lw,\n",
    "                label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of TFIDF vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2.3c (continued)** Briefly explain your plot above and justify why it makes sense (2-3 sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.4 Word2Vec from Scratch [45 points total]</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Word Representations\n",
    "So far, we have only considered how vectors can encode similarity at the document level. However, for decades, researchers have aspired to have useful, meaningful _word-level_ representations, too. After all, much of human language is built on piecing together words that have individual, semantic meaning. Thus, it makes sense for models to also try to capture this phenomenon. In 2013, `word2vec` was created, and not only did it yield the most compelling word-level representations to date, but it was the first, main deep learning architecture in NLP that had undeniably strong results.\n",
    "\n",
    "### PyTorch\n",
    "Back in 2013, deep learning frameworks (e.g., Theono, Caffe, Torch) existed, but they were difficult to use and had orders of magnitude fewer users/contributors than today. With the power of modern-day PyTorch, we will train word-level representations from scratch on our full news corpus. As we will see, even though we are using a relatively small dataset, we will still be able to do some useful reasoning with the word vectors we obtain.  Part of the purpose of this problem is to introduce you to PyTorch's excellent abstractions (e.g. `Dataset`, `DataLoader`, `Module`, `Optimizer`, etc.). Thus, even though the `word2vec` model that we consider here is a simpler version than the ones provided in third-party libraries, our comprehensive, from-scratch approach is more scalable to future problem sets that involve more complex models (e.g. large language models, neural translation models) -- especially when GPUs are involved.\n",
    "\n",
    "PyTorch is heavily object-oriented, which means that we will need to create our own versions of its classes in order to solve our specific problems.  You can find a refresher on object-oriented programming in Python [here](https://realpython.com/python3-object-oriented-programming/).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.4a Dataset Class [9 points]</b>\n",
    "\n",
    "Let's start by defining a custom PyTorch dataset class for our news corpus. We will call it the `MooogleDataset`.  Notice in the cell below that it extends the base PyTorch `Dataset`. As a subclass of `Dataset`, `MooogleDataset` is required by contract to implement three methods (you can read more about this [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)):\n",
    "- A `__init__` method that inputs and stores all the instance variables needed for creating a particular `MooogleDataset` object. As an example, we have implemented this function for you below to take in three arguments:\n",
    "    - the dictionary `tokenized_texts`\n",
    "    - the dictionary `vocab`,\n",
    "    - and an integer `vocab_size`. This will limit the number of unique words that our model is exposed to by only considering the `vocab_size`-most frequent words. Having such a cutoff is standard in deep learning, so that models do not become too large. We will use `vocab_size = 5000`.  Feel free to modify/extend the code in `__init__` if you think that will make implementing the next two functions easier. **Pro tip:** whenever you start to write _any_ code, it's good practice to start small just to ensure everything interacts and behaves correctly -- only once you're sure of such is it worth scaling up and validating the quality of your code. \n",
    "- A `__len__` method that tells us how large the dataset is. In our case, this should be the number of documents in the corpus. You need to implement this method.\n",
    "- A `__getitem__` method that gets a particular item in the dataset. You need to implement this method. The input is `idx`, which is an integer between 0 (inclusive) and the length of the dataset determined by `__len__` (exclusive).  Given `idx`, we want `__getitem__` to (a) find the `idx`-th document (when the documents are sorted in order by id) and (b) return the *integer representation* of that document wrapped in a PyTorch [`Tensor`](https://pytorch.org/docs/stable/tensors.html).  Words with vocabulary indices greater than `vocab_size - 1` are ignored. For example consider the document `[\"the\", \"cat\", \"ate\", \"the\", \"yummy\", \"fish\"]` with vocabulary `{\"the\": 0, \"ate\": 1, \"cat\": 2, \"fish\": 3, \"yummy\": 4}` and `vocab_size = 4`.  Then, the *integer representation* of the document is `[0, 2, 1, 0, 3]`, since \"the\"=0, \"cat\"=2, \"ate\"=1, \"the\"=0, \"yummy\"=3, and \"fish\"=2, but \"yummy\" is ignored (i.e., dropped) since its vocabulary index exceeds `vocab_size - 1`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "class MooogleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenized_texts: Dict[int, List[str]], vocab: Dict[str, int], vocab_size: int):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokenized_texts.keys())\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        doc_id = list(self.tokenized_texts.keys())[idx]\n",
    "        curr_doc_text = self.tokenized_texts[doc_id]\n",
    "        int_rep = []\n",
    "        for token in curr_doc_text:\n",
    "            if self.vocab[token] <= self.vocab_size - 1:\n",
    "                int_rep.append(self.vocab[token])\n",
    "            else:\n",
    "                int_rep.append(0)\n",
    "        return torch.tensor(np.array(int_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As a sanity check, run the test cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([ 91, 793, 794,  25,  16,  96,  24,  30, 146,   6, 795,  92, 291, 171,\n",
      "        276,  32,  21, 172,  46, 292,  16, 796,   2,  21,  93,  25, 109,   0,\n",
      "        172,   4,   5, 797,   6,   0, 798, 799,   0, 800, 801, 802,   3,  56,\n",
      "        119,  66,  46, 173, 803, 174,  47, 173,   6, 804,  41, 293, 805, 294,\n",
      "         24,  62,  77, 295,   0,  91, 806, 120,  35, 296, 807, 175,  90,   6,\n",
      "         32, 808, 297, 809, 121,  80,  58,  15,  25,  30, 298,   0, 299,  92,\n",
      "        171,  25,  39, 300, 810, 811,   5, 812,  93,  58, 813,  24,  30, 146,\n",
      "          6,   0, 814, 815,  57,  61,  32,  39, 816,   2, 301,   0, 817,   1,\n",
      "        818, 284,   2, 819,   0,  92, 820,   3, 149,   1,  46, 821, 822,  39,\n",
      "        823,  74, 824, 302,  25,  12,  96, 825,   2, 826, 176,   1,   0,  93,\n",
      "         46, 292, 827,   7,  24,  23,   4, 828, 829, 830,  32, 177,  15, 303,\n",
      "        304, 305, 122, 306,   2, 307,  11,  21,  93, 175, 831, 832, 833, 834,\n",
      "        178, 119, 177,  15, 303, 304, 305, 122, 306,   2, 307,  11,  21,  93,\n",
      "         25, 179,   7, 308,  24,  39,  27,  32,  12, 835, 309, 123,  80,   5,\n",
      "        165,  32,  30,  77, 310,   0,  91, 836, 120, 233, 837,  46,  90,   0,\n",
      "        132, 838,  12,   0, 839, 286,  18,   0, 840, 841,   4,  67, 288, 132,\n",
      "        842, 843,  82, 844, 308, 124, 122, 294,  24,  30,  77, 845, 295,   0,\n",
      "        159,   4,  41, 120,  35, 296,   2,  77, 281, 846, 290,  25, 179, 847,\n",
      "        848, 174,  47, 173,   6, 849,  41, 293,   0, 850, 851, 852,   7,  24,\n",
      "          3,  32,  20,  84,  43, 853,   4,   0, 138, 854, 180, 855,   7,  24,\n",
      "          3, 124,  20, 311, 243,  25, 179, 124, 856, 857, 858,   3,  97, 124,\n",
      "        122,   0, 277,   2, 174, 218,   2, 859, 860, 861,  13,   0, 862,   8,\n",
      "         12,   0, 309,   7, 863, 280, 311, 120,  28,  20,  45, 237, 864,  59,\n",
      "        865,  25,  39, 866,  32,  87,   0, 105,  12,   5, 867,   1, 868, 109,\n",
      "         22,   0,  71, 869, 870, 123,   5, 871, 872,  12, 873, 874,   0,  44,\n",
      "         63,   0, 875,  76, 300,   5, 876, 877,   4, 194,   4, 878,  13,  25,\n",
      "        879, 176,   1, 880,  32,  61,   0, 881,  13, 175,  90,   6,  32,  23,\n",
      "        882, 312, 313,  23, 883, 314, 188,   0, 313, 884,  14,  25, 885, 886,\n",
      "         66,   5, 887,   1,  95,  83, 315,  87,   0, 888, 889, 314, 890,   1,\n",
      "         55,  19, 891,   2,  47,  98,   4,   0, 892, 316,  25, 109,  46, 172,\n",
      "        161,  14, 299, 893, 291, 171, 317, 318,  23, 121,   2, 894,   5, 895,\n",
      "         83, 169,  13, 178,  16,   0, 896,  23, 897, 898,   3, 180, 206,   5,\n",
      "        899, 900,  11, 901, 902,   2,   5, 317, 318, 169, 178, 119, 164,  11,\n",
      "         58,  15,  28, 319, 320, 903, 904,   7, 905,  21, 906,   2,  25, 119,\n",
      "          7,   5, 907,  30,  47, 908,   1,  25, 909, 121,  46,  90,   6,  32,\n",
      "        312, 910, 911, 912, 913,   2,  41, 321])\n"
     ]
    }
   ],
   "source": [
    "# sanity check cell\n",
    "samp_dataset = MooogleDataset(samp_tokenized, samp_vocab, vocab_size=5000)\n",
    "\n",
    "# Alias for samp_dataset.__len__()\n",
    "print(len(samp_dataset)) # should output 3\n",
    "\n",
    "# Alias for samp_dataset.__getitem__(1)\n",
    "# should output a tensor that starts with tensor([ 91, 793, 794,  25,  16,  96,  24,  30, 146 ...\n",
    "print(samp_dataset[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that your code is correct, simply run (i.e., **DO NOT EDIT**) the cell below to parse the full corpus and output particular samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1139\n",
      "torch.Size([984])\n",
      "torch.Size([1083])\n"
     ]
    }
   ],
   "source": [
    "# evaluation cell\n",
    "full_dataset = MooogleDataset(full_tokenized, full_vocab, vocab_size=5000)\n",
    "print(len(full_dataset)) \n",
    "print(full_dataset[12].size())\n",
    "print(full_dataset[34].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The reason why we went through the trouble of extending PyTorch's `Dataset` class is that we can now use PyTorch's other classes (which are designed to work with `Dataset`) for free! One such class is `DataLoader`. A key concept in deep learning is the idea of \"batch\" training (i.e., batch gradient descent). For efficiency's sake, we don't want to just have our model train on one document at time (i.e., stochastic gradient descent); we want it to be able to process an entire \"batch\" of documents in parallel at once. The `DataLoader` implements this idea of generating random batches for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.4b DataLoader Class [3 points]</b>\n",
    "\n",
    "Carefully read the documentation for `DataLoader` [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Then, please complete the following tasks:\n",
    "1. Use the documentation to determine appropriate arguments for instantiating `DataLoader` for our `MooogleDataset` (containing the full document collection) in the code cell below.  We want to make sure that (a) there are 32 documents per batch, (b) the documents used to form the batch are randomly selected each time, and (c) there is never less than 32 documents per batch -- even for the final batch of the dataloader. \n",
    "2. One argument that our `DataLoader` definitely needs is `collate_fn`, which we have provided for you in the cell below. As you can see, our `collate_fn` is built on a PyTorch utility function called `pad_sequence`. Read the documentation for `pad_sequence` [here](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html), and briefly explain in the markdown cell below: (a) what our specific `collate_fn` below is doing; and, (b) why it is necessary when working with NLP data in particular.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PADDING_IDX = -1\n",
    "collate_fn = lambda x: pad_sequence(x, batch_first=True, padding_value=PADDING_IDX)\n",
    "\n",
    "### ASK ABOUT drop_last\n",
    "\n",
    "# for this code cell, you only need to pass the appropriate arguments into the DataLoader() below\n",
    "full_dataloader = DataLoader(full_dataset, batch_size=32, shuffle=True, drop_last=True, collate_fn=collate_fn) # TODO: add the correct arguments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## My answer for part 2\n",
    "\n",
    "\n",
    "The specific `collate_fn` is taking the current batch and padding every Tensor corresponding to a document with values -1 so that all the tensors have the same length (the length of the longest Tensor in our batch). We need to do this because in our neural architecture, we want all the inputs to the model to have the same dimensions and so we need all the input tensors to be of the same size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are confident that both `Dataset` and `DataLoader` were implemented correctly, run the evaluation cell below, which  should run without error and return a PyTorch **Tensor**.  This **Tensor** is the first batch of data in our data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1569])\n"
     ]
    }
   ],
   "source": [
    "# evaluation cell\n",
    "batch = next(iter(full_dataloader))\n",
    "print(batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.4c Understanding the Batch [2 points]</b>\n",
    "    \n",
    "What are the dimensions of the `batch` Tensor from the evaluation cell above? What do they represent? Please limit your response to 3-4 sentences.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1569])\n"
     ]
    }
   ],
   "source": [
    "# TODO: what are the dimensions of \"batch\"?\n",
    "batch_dim = batch.size()\n",
    "print(batch_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Results 2.4c\n",
    "\n",
    "The dimensions from the batch Tensor from the cells above are `32 x 1558`. This means that we have a batch of 32 Tensors (or vectors since we are talking of Tensors of dimension 1 by n) where the largest Tensor is of Dimension 1 by 1558. This means that all the other Tensors that have a smaller size (1 by m, where m is less than 1558) were padded with -1 values in order to reach the dimension 1 by 1558.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Congrats on making it this far! Now comes the most fun part yet: **implementing the Word2Vec model**. Recall that in the [original Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf), there were two ways that Word2Vec vectors could be trained (CBOW and skip-gram). We will be implementing the first method, called **Continuous Bag-of-Words** (CBOW). You can find a diagram of CBOW below.\n",
    "\n",
    "**CBOW Training Algorithm:** We will train a vector (also called an \"embedding\") $x_i \\in \\mathbb{R}^D$ for each word $i$ in our vocabulary of size $V$, where $D = 100$ is the size of the vector. To train the embeddings, we will use the documents from our news corpus. Here's the overall process of what we will do:\n",
    "1. Assume we have a document $d$ of length $T$ represented by a sequence of words $w_1, w_2, \\ldots, w_T$.  For each word $w_t$ in this document, we will try to predict $w_t$ based on its surrounding words (also known as its \"context\") $\\mathcal{C}_t = \\{w_{t - N}, \\ldots, w_{t - 1}, w_{t +1} \\ldots, w_{t + N}\\}$, where $N = 5$ denotes the context length (meaning the full context will include the 5 words that preceed our given word, and the 5 words that follow it).\n",
    "2. Compute the \"context embedding\" $x = \\frac{1}{2N} \\sum_{w \\in \\mathcal{C}_t} x_w$ by averaging the word embeddings of all words in the context.  Note that this averaging is done for each dimension of the embedding, so $x \\in \\mathbb{R}^D$.  Also note that this averaging will eliminate any notion of word order among words in the context, hence \"continuous bag-of-words\".   \n",
    "3. Use $x$ as the feature vector in a multi-class logistic regression (softmax) problem to predict the missing word $w_t$ out of $V$ possible words in our vocabulary. \n",
    "4. Optimize the word embeddings to minimize the logistic regression loss with batch gradient descent.\n",
    "\n",
    "![CBOW](cbow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "To implement CBOW in PyTorch, we will extend the ``Module`` class (see below).  If you are new to PyTorch, it is expected that this part will be challenging, so don't worry if it takes you a lot of time to get a working implementation.  Definitely check out the excellent PyTorch tutorial on ``Module`` [here](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html). \n",
    "\n",
    "<div class='q_pink'><b>2.4d The CBOW Module [20 points]</b>\n",
    "    \n",
    "All models in PyTorch are implemented through the ``Module`` class.  Below, write a custom ``Module`` called ``CBOW`` that accomplishes the CBOW algorithm we outlined above. Note, every ``Module`` needs to implement the following two methods:\n",
    "1. An ``__init__()`` method that initializes the ``Module`` object.  This ``__init__()`` method typically involves three things: (a) Inputing and saving instance variables (similar to what we did in ``MooogleDataset`` above), (b) Defining the layers/architecture of the network based on these instance variables, and (c) Initializing the layers of the network. \n",
    "    - For ``CBOW``, we have supplied the signature of the ``__init__()`` method, which includes the instance variables you'll need (e.g. ``context_size``, ``vocab_size``, etc.); your job is to accomplish (a), (b), and (c).  For step (b), you can find a list of PyTorch layers [here](https://pytorch.org/docs/stable/nn.html) [hint: check out ``Embedding`` and ``Linear`` in particular].  For step (c), you can find PyTorch functions for initializing tensors [here](https://pytorch.org/docs/stable/nn.init.html).  **We highly recommend initializing all word vectors as zero vectors and the logistic regression weights uniformly between -1 and +1.**    \n",
    "   \n",
    "   \n",
    "2. A ``forward()`` method that takes a `batch` of data from the `DataLoader` and returns a tuple of two tensors: (a) a tensor of predictions `preds` and (b) tensor of targets `targets`.  \n",
    "    - For ``CBOW``, the input `batch` is a 2D `LongTensor` (i.e. tensor of integers).  It has dimensions $B \\times T$, where $B$ is the batch size and $T$ is the sequence length (i.e. number of words).  Keep in mind that many rows in `batch` will end with a string of -1's to indicate padding.  \n",
    "    - The first output `preds` is expected to be a 2D `FloatTensor` (i.e. tensor of decimals) with dimensions $M \\times V$, where $M$ is the number of (non-pad) words in the batch and $V$ is the size of the vocabulary.  The $(i, j)$-th entry of `preds` will contain CBOW's predicted *log probability* of the $i$-th word in the batch being equal to the $j$-th word in the vocabulary.  \n",
    "    - The second output `targets` is expected to be a 1D `LongTensor` with length $M$.  The $i$-th entry of `targets` will contain the true identity (i.e. \"target\") of the $i$-th word in the batch.\n",
    "    - Writing a `forward()` method that goes directly from `batch` to (`preds`, `targets`) may be quite a challenging task, so we have broken down the problem into subproblems that may be easier to think about.  Each subproblem will involve implementing a different *helper function* (see cell below) that can be used to implement by full `forward()` method.  We encourage you to test your helper functions with sample inputs as you implement them [hint: recall that you can get a sample batch from any `DataLoader` object `dataloader` by running `batch = next(iter(dataloader))`].\n",
    "        - *Subproblem 1*: Following the **CBOW Training Algorithm** above, the first step is to take the batch of documents ``batch`` along with some integer ``context_size`` and extract a ``contexts`` tensor and a ``targets`` tensor.  This logic will be implemented in the ``split_data()`` helper function below [hint: you may want to make use of [this PyTorch function](https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html) in your implementation].  Since each batch has multiple documents, each document has multiple contexts, and each context is comprised of multiple words, ``contexts`` will be a 3D tensor with dimensions $B \\times C \\times N$, where $C$ is the number of contexts [hint: $C$ is a function of $T$ and $N$].  The `targets` tensor will be a 2D tensor with dimensions $B \\times C$, since each context has one target word to predict.  In future steps, we will be using the ``contexts`` to predict the ``targets`` (think of ``contexts`` as the \"X-variable\" and ``targets`` as the \"y-variable\" in logistic regression).  \n",
    "        - *Subproblem 2*: The next step has more to do with tensor organization than the actual CBOW algorithm.  If you look inside your ``contexts`` and ``targets`` tensors from the previous subproblem, you'll notice that some of these contain $-1$, which corresponds to padding.  These exist because we needed to pad some of the documents in order to process them as a batch with common sequence length $T$.  However, we don't actually want to train our model on any contexts or targets that contain padding; padding has nothing to do with the CBOW algorithm.  Thus, to filter out the pads, you will flatten the 3D `contexts` tensor and 2D `targets` tensor by implementing the `flatten_tensors()` helper function below.  The outputs should be a flattened 2D `contexts` tensor with dimensions $M \\times N$ and a flattened 1D `targets` tensor with dimension $M$, where $M$ is the total number of real (i.e. non-pad) words in the batch [hint: to check your understanding, think about why it must be the case that $M < B \\cdot C$].\n",
    "        - *Subproblem 3*: Assuming that you have implemented both `split_data()` and `flatten_tensors()` correctly, you are now ready to write the `CBOW.forward` method, which will call both of these helpers.  The output of `flatten_tensors()` will yield `contexts` and `targets`.  Following the description of the **CBOW Training Algorithm** above, figure out how to pass `contexts` through the layers you defined in ``CBOW.__init__`` to yield ``preds``, the log probabilities tensor of size $M \\times V$.  Then, return ``preds`` and ``targets`` to finish the function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from torch.nn import Module, Embedding, Linear, init\n",
    "from torch import LongTensor, BoolTensor, FloatTensor\n",
    "\n",
    "# Helper function \n",
    "def split_data(batch: LongTensor, context_size: int) -> Tuple[LongTensor, LongTensor]:\n",
    "    contexts = batch.unfold(1, context_size*2 + 1, context_size*2 + 1)\n",
    "    targets = contexts[:,:,context_size]\n",
    "    contexts = torch.cat((contexts[:,:,:context_size], contexts[:,:,context_size + 1:]),2)\n",
    "    return contexts, targets\n",
    "    \n",
    "# Helper function \n",
    "def flatten_tensors(context: LongTensor, target: LongTensor, padding_idx: int) -> Tuple[LongTensor, LongTensor]:\n",
    "    trimmed_contexts = []\n",
    "    trimmed_targets = []\n",
    "    for pair in zip(context, target):\n",
    "        curr_context = pair[0]\n",
    "        curr_target = pair[1]\n",
    "        try:\n",
    "            first_pad_idx_ctxt = ((curr_context == padding_idx).nonzero(as_tuple=True)[0]).min(dim=0).values.item()\n",
    "            first_pad_idx_targ= ((curr_target == padding_idx).nonzero(as_tuple=True)[0]).min(dim=0).values.item()\n",
    "        except IndexError:\n",
    "            continue\n",
    "        cutting_idx = min(first_pad_idx_ctxt, first_pad_idx_targ)\n",
    "        trimmed_contexts.append(curr_context[:cutting_idx, :])\n",
    "        trimmed_targets.append(curr_target[:cutting_idx])\n",
    "    target = torch.cat(trimmed_targets, dim=0)\n",
    "    context = torch.cat(trimmed_contexts, dim=0)\n",
    "    return context, target\n",
    "\n",
    "class CBOW(Module):\n",
    "    \n",
    "    def __init__(self, context_size: int, vocab_size: int, vector_size: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_size = vector_size\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.embeddings = Embedding(self.vocab_size, self.vector_size)\n",
    "        torch.nn.init.zeros_(self.embeddings.weight)\n",
    "        self.linear1 = Linear(self.vector_size, self.vocab_size)\n",
    "        #self.softmax = torch.nn.Softmax()\n",
    "        torch.nn.init.uniform_(self.linear1.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch: LongTensor) -> Tuple[FloatTensor, LongTensor]:\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        contexts, targets = split_data(batch, self.context_size)\n",
    "        contexts, targets = flatten_tensors(contexts, targets, self.padding_idx)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        embeds = self.embeddings(contexts)\n",
    "        avg_embeds = torch.sum(embeds, dim=1)\n",
    "        out = self.linear1(avg_embeds)\n",
    "        log_probs_2 = torch.nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs_2, targets\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts, targets = split_data(batch, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 222, 1467,  511,  ...,   -1,   -1,   -1],\n",
      "        [  18,    0,  752,  ...,   -1,   -1,   -1],\n",
      "        [   0,   54,   57,  ...,   -1,   -1,   -1],\n",
      "        ...,\n",
      "        [2498,    0,    0,  ...,   -1,   -1,   -1],\n",
      "        [3142, 1452,    0,  ...,   -1,   -1,   -1],\n",
      "        [  54,   57, 2003,  ...,   -1,   -1,   -1]])\n",
      "torch.Size([32, 2184])\n"
     ]
    }
   ],
   "source": [
    "print(batch)\n",
    "print(batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are confident with your `CBOW` implementation, run the following cell to test (a) the initialization of the model and (b) the `forward` function on a random batch from the dataloader.  It should run without error if you have implemented everything correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2269, 10]) torch.Size([2269])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,   23,  ...,   14,  229,  329],\n",
       "        [ 411,    3,   37,  ..., 1194,    1,    0],\n",
       "        [3313,  254,   11,  ...,    3,    0, 2398],\n",
       "        ...,\n",
       "        [ 105,   10,  633,  ...,  568, 3062,   10],\n",
       "        [3807,   25,  102,  ...,   15,    0,   17],\n",
       "        [2752,    0, 1776,  ...,    4,    0,    0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(full_dataloader))\n",
    "contexts, targets = split_data(batch, 5)\n",
    "contexts, targets = flatten_tensors(contexts, targets, -1)\n",
    "print(contexts.shape, targets.shape)\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1894, 5000]) torch.Size([1894])\n",
      "tensor([[-8.4457, -8.5755, -8.5787,  ..., -8.5267, -8.4971, -8.4899],\n",
      "        [-8.4457, -8.5755, -8.5787,  ..., -8.5267, -8.4971, -8.4899],\n",
      "        [-8.4457, -8.5755, -8.5787,  ..., -8.5267, -8.4971, -8.4899],\n",
      "        ...,\n",
      "        [-8.4457, -8.5755, -8.5787,  ..., -8.5267, -8.4971, -8.4899],\n",
      "        [-8.4457, -8.5755, -8.5787,  ..., -8.5267, -8.4971, -8.4899],\n",
      "        [-8.4457, -8.5755, -8.5787,  ..., -8.5267, -8.4971, -8.4899]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([   7,  712,    6,  ..., 1260,  203,  638])\n"
     ]
    }
   ],
   "source": [
    "# evaluation cell\n",
    "batch = next(iter(full_dataloader))\n",
    "model = CBOW(context_size=5, vocab_size=5000, vector_size=100, padding_idx=PADDING_IDX)\n",
    "preds, targets = model(batch) # Alias for model.forward(batch)\n",
    "print(preds.shape, targets.shape)\n",
    "print(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.4e Training the Model [9 points]</b>\n",
    "    \n",
    "Great! Now that we have a Dataset, a DataLoader, and a model, we can start training the model! Please read steps 3 and 4 of [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). In the tutorial, you'll notice that training will require three steps:\n",
    "- Choosing an [optimizer](https://pytorch.org/docs/stable/optim.html) (we recommend [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) with learning rate 0.001, but feel free to experiment)\n",
    "- Choosing a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "- Writing a training loop (i.e. step 4 in the tutorial).  In the training loop, you'll have to set how many times to iterate through the dataloader; this is also called the *number of epochs*.  For starters, 1-2 epochs should be enough to see some decent results in the next section, but feel free to train for more epochs to see if you can get even better word vectors. **In your training loop, please print out your loss every 10 batches.** This is insightful to the user (and grader).\n",
    "\n",
    "For reference, our solution runs 3 epochs in 2 minutes 45 seconds (on a 2019 Macbook Pro w/ a 2.6 GHz CPU and no GPU).\n",
    "\n",
    "In the cell below, write and run code to train your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iteration    11, loss: 9.346\n",
      "Epoch 1, iteration    21, loss: 8.465\n",
      "Epoch 1, iteration    31, loss: 8.433\n",
      "Epoch 2, iteration    11, loss: 9.212\n",
      "Epoch 2, iteration    21, loss: 8.339\n",
      "Epoch 2, iteration    31, loss: 8.311\n",
      "Epoch 3, iteration    11, loss: 9.068\n",
      "Epoch 3, iteration    21, loss: 8.202\n",
      "Epoch 3, iteration    31, loss: 8.159\n",
      "Epoch 4, iteration    11, loss: 8.897\n",
      "Epoch 4, iteration    21, loss: 8.038\n",
      "Epoch 4, iteration    31, loss: 7.975\n",
      "Epoch 5, iteration    11, loss: 8.699\n",
      "Epoch 5, iteration    21, loss: 7.843\n",
      "Epoch 5, iteration    31, loss: 7.775\n",
      "Epoch 6, iteration    11, loss: 8.461\n",
      "Epoch 6, iteration    21, loss: 7.618\n",
      "Epoch 6, iteration    31, loss: 7.559\n",
      "Epoch 7, iteration    11, loss: 8.201\n",
      "Epoch 7, iteration    21, loss: 7.413\n",
      "Epoch 7, iteration    31, loss: 7.309\n",
      "Epoch 8, iteration    11, loss: 7.934\n",
      "Epoch 8, iteration    21, loss: 7.192\n",
      "Epoch 8, iteration    31, loss: 7.105\n",
      "Epoch 9, iteration    11, loss: 7.702\n",
      "Epoch 9, iteration    21, loss: 6.954\n",
      "Epoch 9, iteration    31, loss: 6.914\n",
      "Epoch 10, iteration    11, loss: 7.534\n",
      "Epoch 10, iteration    21, loss: 6.805\n",
      "Epoch 10, iteration    31, loss: 6.780\n",
      "Finished Training\n",
      "tensor([[ -1.8511,  -6.5531,  -6.4225,  ...,  -9.5054, -10.1843,  -9.1312],\n",
      "        [ -2.7758,  -6.7106,  -6.6910,  ...,  -9.1377,  -9.7503,  -8.8600],\n",
      "        [ -1.9748,  -6.6711,  -6.4632,  ...,  -9.4500, -10.1433,  -9.0848],\n",
      "        ...,\n",
      "        [ -1.8485,  -6.4493,  -6.6132,  ...,  -9.4027, -10.1893,  -9.1830],\n",
      "        [ -2.8738,  -6.6941,  -6.7403,  ...,  -9.1174,  -9.7386,  -8.8008],\n",
      "        [ -2.3403,  -6.6790,  -6.5905,  ...,  -9.2808, -10.0160,  -8.9822]],\n",
      "       grad_fn=<LogSoftmaxBackward>) tensor([  32, 4357,  196,  ...,  667,    0,  270])\n",
      "torch.Size([1830, 5000]) torch.Size([1830])\n",
      "CPU times: user 1min 56s, sys: 16.2 s, total: 2min 13s\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import NLLLoss\n",
    "\n",
    "# TODO: train the model!\n",
    "loss_function = NLLLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(full_dataloader):\n",
    "        \n",
    "        #print(\"These are model weights\", model.parameters)\n",
    "        #print(\"These are preds \", preds.shape)\n",
    "        #print(\"These are targets \", targets.shape)\n",
    "        preds, targets = model(batch)\n",
    "        loss = loss_function(preds, targets)\n",
    "        #print(\"This is loss \", loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            #print(\"This is running_loss \", running_loss)\n",
    "            print('Epoch %d, iteration %5d, loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss/10))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "print(\"Finished Training\")\n",
    "\n",
    "print(preds, targets)\n",
    "print(preds.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1077, 1669,  881,  ..., 1546,  913,    7])\n",
      "tensor([  32, 4357,  196,  ...,  667,    0,  270])\n"
     ]
    }
   ],
   "source": [
    "print(preds.argmax(dim=0))\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.7658, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_function(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Great job on writing Word2Vec from scratch!  Hopefully you learned a lot about the inner workings of PyTorch.  This will go a long way in preparing us for training more complicated models in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.4f Evaluate the word2vec embeddings [4 points]</b>\n",
    "\n",
    "Now, let's see if the Word2Vec vectors we trained were any good.  Complete the following exercise:\n",
    "1. Extract the Word2Vec embeddings for each word from your model and print the shape of this data structure.\n",
    "2. For each word $w$ in the list [\"man\", \"thursday\", \"senate\", \"jersey\", \"injured\", \"republican\"], find and display the five other words whose vectors have the highest **cosine similarity** with the vector for $w$.  Also, display their cosine similarities.  Are the results in-line with what you would expect?  Why or why not?\n",
    "3. Explore the embeddings yourself and find one word with not-so-sensible neighbors.  Why might this be the case?\n",
    "\n",
    "For clarity, we will explicitly ask you each of these questions in separate prompts below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "\n",
    "**1.** Extract the Word2Vec embeddings for each word from your model and print the shape of this data structure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the embeddings for each word is  torch.Size([5000, 100])\n"
     ]
    }
   ],
   "source": [
    "# TODO: answer the above\n",
    "\n",
    "## Ask if just extracting the weights is enough for this part.\n",
    "embedding_weights = model.embeddings.weight\n",
    "print(\"The shape of the embeddings for each word is \", embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of embeddings\n",
    "\n",
    "The shape of the embeddings is 5000 by 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    " \n",
    "**2.** For each word  𝑤  in the list [\"man\", \"thursday\", \"senate\", \"jersey\", \"injured\", \"republican\"], find and display the five other words whose vectors have the highest cosine similarity with the vector for  𝑤 . Also, display their cosine similarities.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-251-1cf0e05e9f19>:3: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  result = np.dot(vec_a, vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word man the five words with closest vectors are: \n",
      "who with similarity 0.9995402693748474\n",
      "some with similarity 0.9994845390319824\n",
      "are with similarity 0.9994792342185974\n",
      "have with similarity 0.9994630217552185\n",
      "about with similarity 0.9994577765464783\n",
      "\n",
      "\n",
      "For word thursday the five words with closest vectors are: \n",
      "two with similarity 0.9992311596870422\n",
      "so with similarity 0.999219536781311\n",
      "also with similarity 0.9991958737373352\n",
      "it’s with similarity 0.9991766810417175\n",
      "many with similarity 0.9991711378097534\n",
      "\n",
      "\n",
      "For word senate the five words with closest vectors are: \n",
      "government with similarity 0.9991100430488586\n",
      "time with similarity 0.9991043210029602\n",
      "while with similarity 0.9990742206573486\n",
      "several with similarity 0.9990631341934204\n",
      "up with similarity 0.9990314245223999\n",
      "\n",
      "\n",
      "For word jersey the five words with closest vectors are: \n",
      "both with similarity 0.9976621270179749\n",
      "york with similarity 0.9975260496139526\n",
      "city with similarity 0.9974883794784546\n",
      "cases with similarity 0.997445285320282\n",
      "only with similarity 0.9974420070648193\n",
      "\n",
      "\n",
      "For word injured the five words with closest vectors are: \n",
      "including with similarity 0.9963160157203674\n",
      "left with similarity 0.9961108565330505\n",
      "between with similarity 0.9960464239120483\n",
      "up with similarity 0.9960164427757263\n",
      "million with similarity 0.996010959148407\n",
      "\n",
      "\n",
      "For word republican the five words with closest vectors are: \n",
      "they with similarity 0.9994316697120667\n",
      "after with similarity 0.9994218349456787\n",
      "one with similarity 0.9993859529495239\n",
      "more with similarity 0.999382734298706\n",
      "their with similarity 0.9993751645088196\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## HElper functions\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    result = np.dot(vec_a, vec_b)/(np.linalg.norm(vec_a)*np.linalg.norm(vec_b))\n",
    "    return result\n",
    "\n",
    "def most_similar_embs(word1_idx, embeddings, limit=5):\n",
    "    word1_emb = embeddings[word1_idx]\n",
    "    word1_emb_dict: Dict[int, float] = {}\n",
    "    for word_idx, embs in enumerate(embeddings):\n",
    "        word1_emb_dict[word_idx] = cosine_similarity(word1_emb.detach().numpy(), embs.detach().numpy())\n",
    "    sorted_vocab_dict = {k: v for k,v in sorted(word1_emb_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return list(sorted_vocab_dict.items())[:limit]\n",
    "    \n",
    "\n",
    "# TODO: answer the above\n",
    "idx_per_word = [full_vocab[word] for word in ['man', 'thursday', 'senate', 'jersey', 'injured', 'republican']]\n",
    "rev_full_vocab = {idx: word for word, idx in full_vocab.items()}\n",
    "\n",
    "for idx in idx_per_word:\n",
    "    sorted_vocab_dict = most_similar_embs(idx, embedding_weights, limit=6)\n",
    "    #print(sorted_vocab_dict)\n",
    "    print(f\"For word {rev_full_vocab[idx]} the five words with closest vectors are: \")\n",
    "    for sorted_itms in sorted_vocab_dict[1:]:\n",
    "        print(f\"{rev_full_vocab[sorted_itms[0]]} with similarity {sorted_itms[1]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**2 (continued).** Are the results in-line with what you would expect? Why or why not? (2-3 sentences).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "\n",
    "**3.** Explore the embeddings yourself and find one word with not-so-sensible neighbors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: answer the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'>\n",
    "    \n",
    "**3 (continued).** Why might this be the case? (1-2 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.5 From Word2Vec to Doc2Vec [4 points total]</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "OK, time to get back to Mooogle. The cows appear to be growing restless. Hopefully you've seen how training Word2Vec gives us nice word vectors that capture some knowledge about usage and meaning. However, what we ultimately care about in this information retrieval problem is recommending good _documents_. Thus, we will try to use Word2Vec to do document recommendation for Mooogle.\n",
    "\n",
    "Word2Vec provides a vector *for each word*, yet Mooogle requires a vector *for each document*. How do we go from word vectors to document vectors? One very simple way is to define the document vector as the average of word vectors for all the words in the document.  \n",
    "\n",
    "Unfortunately, our in-house Word2Vec vectors from the previous section were not trained on enough data.  Thus, they will not lead to good document vectors. However, we can import some pretrained Word2Vec vectors that were trained on many documents from Google News using the awesome [gensim library](https://radimrehurek.com/gensim/index.html). Run the following code to download these vectors. **WARNING:** This will use 1.6 gb of hard drive space and will thus take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "gnews = gensim.downloader.load('word2vec-google-news-300')\n",
    "gnews_vecs = gnews.vectors\n",
    "gnews_vocab = gnews.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.5a doc2vec [0 points]</b>\n",
    "    \n",
    "**Note:** This section has already been implemented for you. Your job is to read the code and make sure you understand what's happening.\n",
    "    \n",
    "As the cell above suggests, Google News has its own vectors and vocabulary. Below, we use these objects, along with your `tokenized_texts` to implement the following function `run_doc2vec()`, which assigns a vector to every document based on the Word2Vec word vectors. If a word in the corpus happens to be missing from the Google News vocabulary, we skip it when computing document vectors [note: the denominator in the average is adjusted accordingly].\n",
    "\n",
    "The function accepts as inputs:\n",
    "- the output from `load_corpus()` (e.g., `samp_tokenized`)\n",
    "- `gnews_vocab`\n",
    "- `gnews_vecs`\n",
    "\n",
    "It return a dictionary, just like `create_TFIDF()`, whereby the keys are the doc ids and the values are the corresponding vectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def run_doc2vec(tokenized_texts: Dict[int, List[str]], vocab: Dict[str, int], vecs: np.ndarray) -> Dict[int, np.ndarray]:\n",
    "    doc_vecs = {}\n",
    "    for doc_id, doc in tokenized_texts.items():\n",
    "        doc_vecs[doc_id] = np.mean([vecs[vocab[word]] for word in doc if word in vocab], axis=0)\n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Execute the following cells to make sure that `run_doc2vec` is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# sanity check cell\n",
    "samp_doc2vec = run_doc2vec(samp_tokenized, gnews_vocab, gnews_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Once you are convinced that the code is correct, simply run (i.e., **DO NOT EDIT**) the cell below to convert the dataset to doc2vec format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "full_doc2vec = run_doc2vec(full_tokenized, gnews_vocab, gnews_vecs)\n",
    "print(len(full_doc2vec[20679]))\n",
    "print(full_doc2vec[189708])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'><b>2.5b Run Moogle [4 points]</b>\n",
    "    \n",
    "We now produce recommendations based onthe Doc2Vec embeddings, and compare them to those produced by TF-IDF. **You do not need to write any code in the cell below. Simply answer the subsequent question.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "doc2vec_df = pd.DataFrame(full_doc2vec).T\n",
    "doc2vec_norms = np.sqrt((doc2vec_df ** 2).sum(axis=1))\n",
    "doc2vec_cosines = doc2vec_df @ doc2vec_df.T / doc2vec_norms.values / doc2vec_norms.values.reshape(-1, 1)\n",
    "doc2vec_cosines\n",
    "\n",
    "# Get top 5 recommendations for doc2vec\n",
    "doc2vec_recs = doc2vec_cosines.apply(lambda x: pd.Series(x.nlargest(6).index[1:])).T\n",
    "# Display titles of recommendations\n",
    "for doc_id in [189496, 20398, 20516, 189657]:\n",
    "    print('**** Query ****')\n",
    "    print(titles.loc[doc_id])\n",
    "    print('**** Doc2Vec Recommendations ****')\n",
    "    for rec_id in doc2vec_recs.loc[doc_id]:\n",
    "        print(titles.loc[rec_id])\n",
    "    print('**** TF-IDF Recommendations ****')\n",
    "    for rec_id in tfidf_recs.loc[doc_id]:\n",
    "        print(titles.loc[rec_id])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_pink'> How do our Doc2Vec recommendations compare to TF-IDF's? Why? Are Doc2Vec recommendations good overall? What are some alternative ways to adapt word2vec for documents? (At least 2-3 sentences.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='header_green'>\n",
    "    \n",
    "# 3. RESEARCH [20 points]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As we build a foundation in NLP, it's also important to also see what the latest, cutting-edge work (research) looks like. It's incredibly worthwhile to learn about the types of problems people work on, their methodology and approach to the problem, the datasets they work on, the issues they raise, and the solutions they posit. The field moves incredibly fast, but the __approach__ to ML/NLP research is relatively stable -- different types of papers are accepted as the years progress, but that's a different story.\n",
    "\n",
    "We want to help you get practice reading research papers, which mostly entails thinking critically about the work, being able to discern the main takeaways/conclusions, and to reflect on the work in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.1: Read an NLP research paper [0 points]</b>\n",
    "    \n",
    "Select and read a paper that was published in ACL, NAACL, EMNLP, or COLING in 2020 or 2021. You can find a list of such published papers by searching Google (Mooogle can't help you here), a la \"ACL 2020 accepted papers\". For this assignment, you are allowed to pick either a short paper (4-5 pages) or long paper (8-9 pages), **but you must not select a workshop paper**. List below the name of the paper, authors, venue, and year published.\n",
    "\n",
    "While I highly encourage you to look at the aforementioned venues to find a paper that interests you, alternatively, you could select one of the follow three papers:\n",
    "\n",
    "- Famous alternative, follow-up to word2vec:\n",
    "> GloVe: Global vectors for word representation. Jeffrey Pennington, Richard Socher, and Christopher Manning. EMNLP 2014.\n",
    "\n",
    "- Influential paper on probing, and the title is a reference to a famous quote \"You can't cram the meaning of a whole sentence into a %&!$# vector\\\":\n",
    "> What you can cram into a single vector: Probing sentence embeddings for linguistic properties. A Conneau, G Kruszewski, G Lample, L Barrault, M Baroni. ACL 2018.\n",
    "\n",
    "- Not as famous, but a solid approach to an important and interesting problem:\n",
    "> Improving Vector Space Word Representations Using Multilingual Correlation. Manaal Faruqui and Chris Dyer. EACL 2014.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR PAPER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.2: Problem? [2 point]</b> What is the problem that it is trying to address? In other words, what is it trying to solve? (2-3 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.3: Solution? [2 point]</b> At a very high-level, what was their solution? (2-3 sentences). Here, you don't have room to go into the small details (e.g., about the model), so you'll need to summarize the most important elements that comprised the solution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.4: Data? [2 points]</b> What dataset(s) did they use? Are they freely available? What's the size of the data? (2-3 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.5: Model [2 points]</b> Very related to the 'solutions' question, describe here any models that they used, and what made it effective (2-3 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.6: Results? [2 points]</b> What are their main results? (~2 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.7: Strengths? [2 points]</b> List 2-3 strengths of the paper\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.8: Weaknesses? [2 points]</b> Although you may be new to this problem and all of its content, try to list 2-3 weaknesses of the paper (anything that you think could strengthen the paper is sufficient).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.9: Evaluation [2 points]</b>\n",
    "    \n",
    "How would you evaluate this paper in terms of:\n",
    "- scientific contribution\n",
    "- effectiveness to solve the problem\n",
    "- how convincing it was.\n",
    "    \n",
    "Give each of these elements a score from 1-10 (10 is best). No word explanation necessary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>3.10: Research Ideas [4 points]</b> Think of 1-2 research ideas that you have based on this paper. It doesn't have to be grand; most research is very incremental. Specifically, your research idea should have a concrete __question__ that you're aiming to answer. List it below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_green'><b>BONUS POINTS [5 points]</b> I mention the full details in the syllabus on the course website. However, in short, these bonus points cannot bring one's grade to exceed 100. That is, if someone received a 97 on this homework, doing this bonus could allow their grade to reach 100 points. If the person had an 83 on the homework, then the most they could achieve is an 88.\n",
    "    \n",
    "The task: read another research paper (allowed to be a Short Paper) and answer the same questions again. Please copy and paste all of the questions below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='header_blue'>\n",
    "    \n",
    "# 4. SELF-REFLECTION [0 points]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='q_blue'><b>4.1: Self-reflection and Feedback [0 points]</b>\n",
    "\n",
    "Are you thriving in the course? Are there elements that are particularly confusing to you? I want everyone to be and feel fully supported. Toward this, I strongly urge you all to think critically about your own learning and efforts. Please provide us with feedback about how you're doing in the course and if there's anything further or different we can do to better assist your learning. I want everyone to give their earnest account, so the form is completely anonymous.\n",
    "\n",
    "</div>\n",
    "\n",
    "[Anonymous Self-Reflection and Feedback Form](https://forms.gle/3LT6UfhtCtqp2G7X9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernel_loop": {
   "byte_size": "82791"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
