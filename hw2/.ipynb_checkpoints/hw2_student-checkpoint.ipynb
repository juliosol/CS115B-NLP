{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "\n",
       "hr {\n",
       "    height: 1px;\n",
       "    background-color: black;\n",
       "    border: none;\n",
       "}\n",
       "\n",
       "div.quote {\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12px;\n",
       "\talign-items: center;\n",
       "\tmax-width: 80%;\n",
       "\ttext-align: center;\n",
       "}\n",
       "\n",
       "div.header_purple {\n",
       "\tbackground-color: #D0C7FF; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_purple {\n",
       "\tbackground-color: #9183D9;\n",
       "\tborder-color: #FF8484;\n",
       "\tborder-left: 5px solid #FF8484; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_teagreen {\n",
       "\tbackground-color: #DEEFB7; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_teagreen {\n",
       "\tbackground-color: #DEEFB7;\n",
       "\tborder-color: #5FB49C;\n",
       "\tborder-left: 5px solid #5FB49C; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_aquagreen {\n",
       "\tbackground-color: #AACFC4; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_aquagreen {\n",
       "\tbackground-color: #AACFC4;\n",
       "\tborder-color: #5F758E;\n",
       "\tborder-left: 5px solid #5F758E; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_lightpurp {\n",
       "\tbackground-color: #CBC5EA; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_lightpurp {\n",
       "\tbackground-color: #CBC5EA;\n",
       "\tborder-color: #73628A;\n",
       "\tborder-left: 5px solid #73628A; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_silver {\n",
       "\tbackground-color: #707078; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_silver {\n",
       "\tbackground-color: #707078;\n",
       "\tborder-color: #090909;\n",
       "\tborder-left: 5px solid #090909; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_pink {\n",
       "\tbackground-color: #FFC8C8;\n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_pink {\n",
       "\tbackground-color: #FFC8C8;\n",
       "\tborder-color: #FF8484;\n",
       "\tborder-left: 5px solid #FF8484; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_yellow {\n",
       "\tbackground-color: #FDFFB6; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_yellow {\n",
       "\tbackground-color: #FDFFD0;\n",
       "\tborder-color: #FFD6A5;\n",
       "\tborder-left: 5px solid #FFD6A5; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_orange {\n",
       "\tbackground-color: #FFD6A5; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_orange {\n",
       "\tbackground-color: #FFD6A5;\n",
       "\tborder-color: #D6562C;\n",
       "\tborder-left: 5px solid #D6562C; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.header_green {\n",
       "\tbackground-color: #CAFFBF; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_green {\n",
       "\tbackground-color: #CAFFBF;\n",
       "\tborder-color: #98C98E;\n",
       "\tborder-left: 5px solid #98C98E; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "div.header_blue {\n",
       "\tbackground-color: #C9FAFF; \n",
       "\twidth: 100%;\n",
       "\tbox-shadow: 5px 5px 10px 2px #888;\n",
       "\tfont-size: 15px;\n",
       "\tpadding-top: 8px;\n",
       "\tpadding-left: 20px;\n",
       "\tpadding-right: 20px; \n",
       "\tpadding-bottom: 20px;\n",
       "}\n",
       "\n",
       "div.q_blue {\n",
       "\tbackground-color: #C9FAFF;\n",
       "\tborder-color: #A0C4FF;\n",
       "\tborder-left: 5px solid #A0C4FF; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "h1 {\n",
       "    text-align: left; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "h2 { \n",
       "    text-align: left; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "#        RUN THIS CELL\n",
    "################################\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"style.css\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class='header_teagreen'>\n",
    "\n",
    "# <img style=\"float: left; padding-right: 10px; width: 60px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> AC295/CS287/E-115B: Deep Learning for NLP\n",
    "\n",
    "<br/>\n",
    "<hr color=black>\n",
    "\n",
    "## Homework 2: Recurrent Neural Networks and Machine Translation\n",
    "### THE TEA GREEN BOOK\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2021**<br/>\n",
    "**Instructor**: Chris Tanner<br/>\n",
    "**Release Date**: September 21 (Tues)<br/>\n",
    "<font color=\"red\">**Due Date**: October 5 (Tues) @ 11:59pm (EST)</font>\n",
    "\n",
    "<hr color=black>\n",
    "<center>\n",
    "<div class='quote'>\n",
    "\n",
    "_\"We don't speak the same language [...] you missin' every single shot that you ain't taken\"_\n",
    "\n",
    "    Malcolm McCormick (August 3, 2018)\n",
    "</div>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_teagreen'>\n",
    "    \n",
    "# OVERVIEW\n",
    "\n",
    "</div>\n",
    "<br/>\n",
    "This assignment spans the content covered in the following lectures:\n",
    "\n",
    "- **Lecture 5:** Recurrent Neural Networks (RNNs)\n",
    "- **Lecture 6:** LSTMs\n",
    "- **Lecture 7:** seq2seq + Attention\n",
    "- **Lecture 8:** Machine Translation\n",
    "\n",
    "Language is inherently sequential in nature, which makes it conducive to models that can capture the contextual, long-range dependencies. Toward this, you will first gain experience working with a simple RNN. The RNN aims to capture the data's meaning in its hidden layer (like all neural models), which will be used toward a classification task (sentiment analysis of IMDb movies).\n",
    "\n",
    "After seeing the effectiveness of this basic model, you will then extend this model for two missions. As a warm-up, we would like you to try your hand on performing sentiment analysis on an IMDB review dataset. Afterwards, your *true* mission, should you accept, is to make a complete Machine Translation system. There are some evil hackers on the Dark Web who are stealing and selling others' personal information, financial documents, CS287 assignments, etc. To make their operation covert, they are speaking in a \"Mystery Language\". Forensics suspect their mystery language is actually one of the following languages:\n",
    "- Danish\n",
    "- English\n",
    "- German\n",
    "- Finnish\n",
    "- Spanish\n",
    "\n",
    "You are tasked with determining the true identity of their mystery language, so that their words can be read and understood. Help save the day and keep everyone's data safe (for now). ~~This tape will self-destruct in five seconds.~\n",
    "\n",
    "As a heads-up, this assignment is shorter than Homework 1, as we wanted to provide you all with more time to focus on research. This pattern will persist, as each homework assignment will be shorter than the previous. With that said, the models at hand are increasing in complexity, and thus please ensure you allocate sufficient time to building, debugging, and running the models. For example, the last part of Problem 2 (programming) requires you to run your model on 5 different corpora. Our solutions can train the full model in just a few minutes (i.e., 1-5); however, if your solution is overwhelmingly slow, the total running time can start to become a hindrance.\n",
    "\n",
    "If your code runs slowly on your own local machine, you are free to run your code on [Google Colab](https://colab.research.google.com/) -- just be mindful that the academic policy is still in tact, and that nobody else should have access to your code. In terms of grading, in general, we will not be too picky with the _efficiency_ of your code, unless it's egregious. Having a sound solution is the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_teagreen'>\n",
    "    \n",
    "# LEARNING OBJECTIVES\n",
    "\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "The purpose of this homework is to help you:\n",
    "\n",
    "- understand the sequential nature of language\n",
    "- develop a strong foundation in working with recurrent networks\n",
    "- gain experience working with text classification\n",
    "- understand the mechanics of a basic, neural Machine Translation system\n",
    "- think critically about how language can be used and leveraged (e.g., for classification and translation), while recognizing strengths and weaknesses.\n",
    "\n",
    "To assist you reach these learning objectives, this homework is structured into three parts:\n",
    "- <span style=\"background-color: #FDFFB6\"><b>Foundation (concepts):</b></span> demonstrate an understanding of the core concepts taught in lectures\n",
    "- <span style=\"background-color: #FFC8C8\"><b>Application (programming):</b></span> gain experience putting that knowledge into practice \n",
    "- <span style=\"background-color: #CAFFBF\"><b>Research (creating new knowledge):</b></span> use your current NLP knowledge and skills to go beyond the course material, to grasp cutting-edge results and to critically accept or challenge that information. This serves as practice for you to research your own NLP interests and to be well-equipped to continuously learn the latest, greatest NLP work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_teagreen'>\n",
    "    \n",
    "## SUPPORT\n",
    "\n",
    "</div>\n",
    "\n",
    "- **Supplemental Resources:** See the list of [supplemental resources](https://harvard-iacs.github.io/CS287/supplemental) for a wealth of rich information concerning Machine Learning, NLP, and Math. Some of the courses listed concern the exact topics covered in this homework and lectures.\n",
    "- **Sanity Check cells:** Throughout the homeworks, we sometimes provide 'sanity check' cells which allow you to see our expected outputs. You should ensure your code produces the same. <span style=\"background-color: #FDFFB6\"><b>**NOTE:** We are not claiming that passing the sanity check cells indicates that you have _fully_ implemented everything correctly; rather, they provide simple checks to help inform you if you are on the right track.</span>\n",
    "- **Ed**: If you are stuck on anything conceptual (not code) about the content from lectures, please post a question on Ed. This is your community, and please contribute and help each other out. If your questions concern the homework, you can post these on Ed, too, but make sure you are not posting any of your code or solutions in general. If you think you've spotted a bug in our homework questions, or something that needs clarifying, please let us know on Ed! <span style=\"background-color: #FDFFB6\">Extra credit will be awarded for bugs.</span> We want to correct these issues ASAP.\n",
    "- **OH:** After having given a wholehearted attempt, if you are having trouble with the homework, please come to Office Hours.\n",
    "- **Classmates:** We have a strict policy about the homeworks being individual. You are free to discuss _concepts_ with one another, to help each other learn the material. However, no student shall ever discuss their solutions or see another student's solutions to any problem. Once you see someone's coding solution, it's nearly impossible to harness that information in a way that you can write your own unique solution. You've been robbed of a learning opportunity and will likely just regurgitate someone else's work. As a reminder, if you want to take a shortcut on any problem by looking online for already-existing solutions, that's permissible, but you must cite your sources. Otherwise, it constitutes cheating. Posting any pieces of this homework online for others to see if a flagrant violation of our academic policy.\n",
    "- **Other:** I want everyone to be and feel fully supported. If there's anything else we, as a teaching staff, can do to further assist in your learning, please let us know. Related, at the end of this homework assignment, you are expected to complete an anonymous feedback form. I urge you to critically and earnestly think about your own learning, communicate to us your thoughts, and to optionally tell us possible adjustments we could make so that you meet our learning expectations and you achieve your own learning goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_lightpurp'>\n",
    "    \n",
    "# 1. FOUNDATION (CONCEPTS) [10 points]\n",
    "\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_lightpurp'><b>1.1 RNN [2 points]</b>\n",
    "\n",
    "Recall the standard, canonical RNN that was discussed in Lecture 5. Our hidden layer at timestep $t$ is defined as $h_{t} = f([h_{t-1}; x_{t}])$, where:\n",
    "\n",
    "- $f()$ is any non-linear activation function\n",
    "- $x$ corresponds to the input Embedding (densely encoded, not one-hot representation)\n",
    "- $;$ represents vector concatenation\n",
    "- this is a slightly abstract view, as we are not explicitly showing the two weight matrices by which $h_{t-1}$ and $x_{t}$ are being multipled before they're concatenated together. We're also not indicating the bias.\n",
    "\n",
    "Let's say we change $h_{t}$ as follows:\n",
    "\n",
    "$h_{t} = f([x_{t-1}; x_{t}])$\n",
    "\n",
    "In 2-3 sentences, discuss how and why you expect this to affect performance.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE\n",
    "Idea: We are neglecting the embedding/signals we learned from previous layers, so basically we are deleting the recurrence in the network and only considering the input of previous time step and this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_lightpurp'><b>1.2 seq2seq with Attention [2 points]</b>\n",
    "\n",
    "NOTE: This question concerns the standard Attention discussed in Lecture 7, not Self-Attention\n",
    "Which of the following statements about Attention are true (select all that apply)?\n",
    "\n",
    "- A: Attention is used for situations where you have both an encoder and decoder model\n",
    "- B: Attention determines how much emphasis to place on each hidden state from an encoder\n",
    "- C: Attention determines how much emphasis to place on each hidden state from a decoder\n",
    "- D: Attention scores can be computed in various ways (e.g., dot-product, bilinear transformation, feed-forward neural network)\n",
    "\n",
    "No explanation needed, just write below the letters you believe are true statements.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE\n",
    "Correct answers are B and D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_lightpurp'><b>1.3 Machine Translation: Directionality [2 points]</b>\n",
    "\n",
    "Let's say we have a parallel corpus of 100,000 sentences in two languages: \"source language\" and \"target language\". For example, perhaps the \"source language\" is Indonesian and the \"target language\" is English, and sentence $i$ in the English corpus is the translation of sentence $i$ in the Indonesian corpus.\n",
    "\n",
    "Let's model Machine Translation with a `seq2seq model with Attention` that operates on word tokens. Generally, for any particular pair of languages (e.g., Language $A$ and Language $B$), will the model yield the \"same\" results regardless of if language $A$ serves as the \"source\" language or \"target\" language?\n",
    "\n",
    "Few clarifying statements about what we're asking:\n",
    "\n",
    "- We are contrasting (1) treating language $A$ as the source language and $B$ as the target language; versus, (2) treating $A$ as the target language and $B$ as the source language\n",
    "- We are **not** contrasting language pairs ($A$,$B$) with language pairs ($C$, $D$), or even ($B$, $C$).\n",
    "- By \"same results\", we are not nitpicking about floating point precision and natural variation. Rather, we assert that any model trained on a given pair $(A, B)$, when run a few times, will naturally produce slightly varying results (due to the stochasticity of a NN), which one can summarize as a performance _range_. We are effectively asking if reversing the language pairs to be $(B,A)$ will yield performance that is consistently within the same performance _range_ as $(A, B)$.\n",
    "- We assume all other experimental setup is sound; e.g., nothing unusual with the train/dev/test splits. So, you can expect that in every situation, the first 70k sentences are used for train, the next 10k for dev, and the remaining 20k for test.\n",
    "\n",
    "Please discuss in 3-4 sentences your model expectations and justify.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE\n",
    "Switching the source and target yields different results \n",
    "In decoder you are keeping track of all the previous hidden states to predict a word but in encoder we don't do that.  \n",
    "Check at different in prediction time (test time).\n",
    "Think of differences of what you do in encoding vs decoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_lightpurp'><b>1.4 Machine Translation: Denoising [4 points]</b>\n",
    "\n",
    "Again, let's say we have a parallel corpus of 100,000 sentences in two languages (\"source language\" and \"target language\"), and that we're using a `seq2seq model with Attention` that operates on word tokens.\n",
    "\n",
    "**PART 1 (2 points):**\n",
    "If we alter our corpus such that for **every distinct, non-white space character** in the source language, we instantly replace it with a corresponding **different, distinct, non-whitespace character**. Then, we run the same model, from scratch, on this modified corpus. Would you expect the performance of the system to decrease, stay the same, or improve? Discuss in ~3 sentences what your expectations are and why.\n",
    "\n",
    "**PART 2 (2 points):**\n",
    "If we alter our corpus such that for **every distinct, non-white space character** in the source language, we instantly replace it with a corresponding **different, distinct, character (that may include whitespace)**. Then, we run the same model, from scratch, on this modified corpus. Would you expect the performance of the system to decrease, stay the same, or improve? Discuss in ~3 sentences what your expectations are and why.\n",
    "\n",
    "Few clarifying statements about what we're asking in PART 1 and PART 2:\n",
    "\n",
    "- each distinct character in the source language (e.g., 'a' will be consistently replaced with a different distinct character such as 'p')\n",
    "- no two characters in the original source language should become replaced by the same character (i.e., if 'a' is replaced with 'p', then no other letter will also be replaced by a 'p')\n",
    "- this is not a Distributed Systems class; we're **not** treating this replacement as a _series_ of replacements. That is, we do not need to worry about any race conditions. Letters are simply getting mapped to others.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR RESPONSE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_yellow'>\n",
    "    \n",
    "# 2. APPLICATION (PROGRAMMING) [70 points]\n",
    "\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "\n",
    "In this programming exercise, you will be guided through an implementation of the encoder-decoder.\n",
    "\n",
    "We start with an RNN-based `Encoder`, from which we can extract the final hidden state and cell state. LSTM units are used so that we can retain long-term dependencies. We treat the final vectors as a summary of the sentence, which we can use for binary classification on the text. We will apply this idea to the sentiment analysis of an IMDB movie review dataset, where we hope to determine whether a review is positive or negative. \n",
    "\n",
    "Building on the `Encoder`, we will further add on an RNN-based `Decoder`. This time, we will work on a translation task between source language and target language. We feed the source sentence to the Encoder, obtain the final states and feed them into the `Decoder`. The outputs of the Decoder is our translation.\n",
    "\n",
    "In total, you will gain hands-on experience building an important, powerful model (RNN-based Encoder/Decoder), which you'll demonstrate as being highly useful for completely different tasks and data. We're starting to see the beauty of using deep learning models for NLP!\n",
    "\n",
    "### DATA\n",
    "\n",
    "See the relevant section for a detailed description of the data we use. \n",
    "\n",
    "### PANDAS\n",
    "In all homework assignments, including this one, you are free to use `Pandas`. You are not required to use it at all, but it has some highly useful functionality, e.g., its `read_csv()` function, `DataFrame` and `Series` data structures, and its ability to quickly filter/sort/edit data (which is particularly helpful when experimenting and exploring your data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities you will need or are free to use\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, NLLLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_yellow'>\n",
    "\n",
    "# IMDb Sentiment Analysis\n",
    "\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "For this problem, we're going to build a binary sentiment classifier for movie reviews using the classic [IMDb dataset](https://ai.stanford.edu/~amaas/data/sentiment/). \n",
    "\n",
    "## Data\n",
    "\n",
    "Since you've already seen how to process and handle raw text data in HW1, we've gone ahead and handled the data processing and infrastructure for you this time. We've trimmed the reviews of punctuation and HTML tags, removed stop words, and [lemmatized](https://en.wikipedia.org/wiki/Lemmatisation) the remaining tokens using [NLTK's WordNet Lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html). For example, the review:\n",
    "> Billy Crystal normally brings the crowd to laughter, but in this movie he and all the rest of them cannot bring any smile on my face.... or perhaps just one. They call it comedy, I say it's a waste of my time.\n",
    "\n",
    "Becomes:\n",
    "> ['billy', 'crystal', 'normally', 'bring', 'crowd', 'laughter', 'movie', 'rest', 'bring', 'smile', 'face', 'perhaps', 'one', 'call', 'comedy', 'say', 'waste', 'time']\n",
    "\n",
    "For the sake of training speed, we've limited the original corpus of 50,000 reviews to just 5,000 by filtering for only reviews that are shorter than 100 words. \n",
    "\n",
    "We've provided two datasets for you to use in the form of Python Pickle files ([documentation on what Pickling is](https://docs.python.org/3/library/pickle.html)):\n",
    "- `imdb_train.pkl` contains 4000 reviews for training. \n",
    "- `imdb_test.pkl` contains 1000 reviews for testing.\n",
    "\n",
    "Each pickled file contains a dictionary formatted as follows: \n",
    "\n",
    "```\n",
    "{\n",
    "    'tokenized_reviews': [\n",
    "        [token, token, token, ...],\n",
    "        [token, token, token, ...],\n",
    "        ...\n",
    "    ],\n",
    "    'full_reviews': [review review review ...],\n",
    "    'sentiments': [sentiment sentiment sentiment ...]\n",
    "}\n",
    "```\n",
    "Where each sentiment is binary, either `\"positive\"` or `\"negative\"`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.1 Load the data [0 points]</b>\n",
    "\n",
    "In the cell below, we load each dataset for you (simply run the cell). Note that you can load each dataset using the `pickle` module in Python's standard library: \n",
    "```py\n",
    "import pickle\n",
    "with open(\"imdb_train.pkl\", \"rb\") as fp:\n",
    "    train_dict = pickle.load(fp)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DO NOT NEED ADD ANY CODE IN THIS CELL\n",
    "with open(\"data/imdb_train.pkl\", \"rb\") as f:\n",
    "    train_dict = pickle.load(f)\n",
    "    \n",
    "with open(\"data/imdb_test.pkl\", \"rb\") as f:\n",
    "    test_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to inspect and perform some *quick* exploratory data analysis on the IMDb reviews and sentiments. For instance, what percentage of the sentiments are positive? Understanding the data before training the data can often lead to very useful observations. Should you be worried if it turns out that only $1\\%$ of the datapoints are labelled negative?\n",
    "\n",
    "This part is for your understanding only. Do not worry, we will not grade it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training set, there is 50.0% of negative reviews and 50.0% of positive reviews.\n",
      "In test set, there is 50.0% of negative reviews and 50.0% of positive reviews.\n",
      "For training set, number of reviews is 4000 and length of max review is 81\n",
      "For test set, number of reviews is 1000 and length of max review is 80\n"
     ]
    }
   ],
   "source": [
    "# TODO: inspect the dicts and perform some quick exploratory analysis\n",
    "# Percentage of positive and negative reviews training\n",
    "train_perc_neg = len([x for x in train_dict['sentiments'] if x=='negative'])/len(train_dict['sentiments'])*100\n",
    "train_perc_pos = len([x for x in train_dict['sentiments'] if x=='positive'])/len(train_dict['sentiments'])*100\n",
    "\n",
    "print(f\"In training set, there is {train_perc_neg}% of negative reviews and {train_perc_pos}% of positive reviews.\")\n",
    "\n",
    "test_perc_neg = len([x for x in test_dict['sentiments'] if x=='negative'])/len(test_dict['sentiments'])*100\n",
    "test_perc_pos = len([x for x in test_dict['sentiments'] if x=='positive'])/len(test_dict['sentiments'])*100\n",
    "\n",
    "print(f\"In test set, there is {test_perc_neg}% of negative reviews and {test_perc_pos}% of positive reviews.\")\n",
    "\n",
    "# Number of reviews and max size\n",
    "num_reviews_train = len(train_dict['full_reviews'])\n",
    "num_reviews_test = len(test_dict['full_reviews'])\n",
    "max_review_train = max([len(x) for x in train_dict['tokenized_reviews']])\n",
    "max_review_test = max([len(x) for x in test_dict['tokenized_reviews']])\n",
    "\n",
    "print(f\"For training set, number of reviews is {num_reviews_train} and length of max review is {max_review_train}\")\n",
    "print(f\"For test set, number of reviews is {num_reviews_test} and length of max review is {max_review_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the data into a Dataset\n",
    "\n",
    "For time efficiency, we've also provided you a `torch.utils.data.Dataset` class below which generates an indexed vocabulary from a given corpus. Note that it replaces infrequent words with the `<UNK>` token and appends an `<EOS>` tag to the end of each review. Also note that it can accept another corpus' vocabulary instead of generating its own. This is helpful because, as you should remember from lecture, we need our model to be able to interpret and use all of the tokens that we encounter at _test time_. We always rely on our training set to provide us with the vocabulary we are expected to see, so it's nice to have the freedom to easily \"import\" another expansive vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DO NOT NEED ADD ANY CODE IN THIS CELL\n",
    "\n",
    "# Some useful helper functions, which we will need for later...\n",
    "PADDING_IDX = 0\n",
    "EOS_IDX = 1 \n",
    "UNK_IDX = 2\n",
    "\n",
    "def get_word_counts(reviews: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Given a tokenized corpus (in this case reviews), we count the frequency of\n",
    "    each word in the corpus\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    for review in reviews:\n",
    "        for word in review:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "    return word_counts\n",
    "\n",
    "def generate_vocab(word_counts: Dict[str, int], min_freq: int) -> Tuple[Dict[str, int], int]:\n",
    "    \"\"\"\n",
    "    Given a set of word counts, we generate a vocabulary. We return two things\n",
    "    from this method:\n",
    "\n",
    "        1. A dict mapping tokens to indices\n",
    "        2. THe length of the vocab\n",
    "    \n",
    "    Words that occur fewer than `min_freq` are replaced with <UNK>\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    vocab = {word: i+3 for i, (word, count) in enumerate(sorted_words) if count > min_freq}\n",
    "    vocab[\"<PAD>\"] = PADDING_IDX\n",
    "    vocab[\"<EOS>\"] = EOS_IDX \n",
    "    vocab[\"<UNK>\"] = UNK_IDX\n",
    "    return vocab, len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DO NOT NEED ADD ANY CODE IN THIS CELL\n",
    "class IMDBDataset(Dataset):   \n",
    "    def __init__(self, reviews: List[List[str]], sentiments: List[str], min_freq=3, vocab=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if vocab is None:\n",
    "            word_counts = get_word_counts(reviews)\n",
    "            self.vocab, self.vocab_size = generate_vocab(word_counts, min_freq)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            self.vocab_size = len(vocab)\n",
    "\n",
    "        self.reviews, self.targets = self._get_idx_dataset(reviews, sentiments, self.vocab)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.reviews[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "    def _get_idx_dataset(self, corpus: List[List[str]], labels: List[str], vocab):\n",
    "        reviews = []\n",
    "        targets = []\n",
    "        for review, label in zip(corpus, labels):\n",
    "            reviews.append([vocab[token] if token in vocab else vocab[\"<UNK>\"] for token in review] + [vocab[\"<EOS>\"]])\n",
    "            targets.append(0.0 if label == \"negative\" else 1.0)\n",
    "        \n",
    "        return reviews, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.2 Instantiate Two Datasets [2 points]</b>\n",
    "    \n",
    "Instantiate two `IMDBDataset`'s, one train and one test, **making sure to use the training vocabulary when instantiating the test dataset** (as this is all our model rightfully knows).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize the following datasets\n",
    "train_ds = IMDBDataset(train_dict['tokenized_reviews'], train_dict['sentiments'])\n",
    "test_ds = IMDBDataset(test_dict['tokenized_reviews'], test_dict['sentiments'])\n",
    "\n",
    "#raise NotImplementedError   # TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div class='q_yellow'><b>2.3 Collation and padding [2 points]</b>\n",
    "\n",
    "Each review is of varying length, but we'd like to combine them together into a single batch (i.e., a 2D matrix, where all data instances are of the same length). So, in order to batch them, we'll need to define a custom collate function which pads them all to the same length. This functionality is the same as in the first homework. However, unlike last time, we'll also need to return a Tensor of labels. Note that since our dataset's `__getitem__()` function returns multiple values (i.e., a Tuple), so our collate function will receive as input a **list of tuples.**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_classifier(batch: List[Tuple[torch.tensor, torch.tensor]]) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    batch_tokens = [item[0] for item in batch]\n",
    "    batch_targets = [torch.unsqueeze(item[1],0) for item in batch]\n",
    "    padded_batch = pad_sequence(batch_tokens, batch_first=True, padding_value=PADDING_IDX)\n",
    "    return (padded_batch, torch.stack(batch_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.4 Instantiating DataLoaders [1 point]</b>\n",
    "\n",
    "Instantiate two `DataLoader` objects, one train and one test, with our custom collate function and with other arguments as appropriate (see the [docs](https://pytorch.org/docs/stable/data.html) for more details).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: intialize the dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, drop_last=True, collate_fn=pad_collate_classifier)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False, drop_last=True, collate_fn=pad_collate_classifier)\n",
    "\n",
    "#raise NotImplementedError   # TODO: implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 219, 3146,    3,  ...,    0,    0,    0],\n",
      "        [ 281,   61,    4,  ...,  105,  215,    1],\n",
      "        [  20,    8,   31,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  43, 3346, 3423,  ...,    0,    0,    0],\n",
      "        [ 137,   64,    3,  ...,    0,    0,    0],\n",
      "        [ 131,   48,   22,  ...,    0,    0,    0]]), tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]]))\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.5 The LSTM Encoder [7 points]</b>\n",
    "\n",
    "Now it's time to implement the Encoder class. As usual, we'll extend the `torch.nn.Module` class. Remember that we have to implement the following methods:\n",
    "1. An `__init__()` method which instantializes the `Module` object. Here, we typically save input variables (such as model hyperparameters) and define/instantiate our layers. \n",
    "    - For the LSTM Encoder, we've provided the signature of `__init__()`. \n",
    "    - For simplicity, instantiate a fresh Embedding layer (rather than using existing embeddings)\n",
    "    - No need implement your own LSTM from scratch! For this homework, it suffices to use the PyTorch [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) class. Be sure to read the docs thoroughly to understand the parameters.\n",
    "\n",
    "2. A `forward()` method which takes as input (1) a batch of data from the `DataLoader`; and (2) the lengths of each sequence in the batch. The `forward()` method returns a tuple of two tensors: (a) the final hidden state (i.e., \"short-term memory\") and (b) the final cell state (i.e., \"long-term memory\") of the LSTM.\n",
    "\n",
    "In your code for `forward()`, you will need to make use of a function that is important for RNNs: the `torch.nn.utils.rnn` method [`pack_padded_sequence`](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html).  To gain some intuition on what this method is doing, see this [StackOverflow post](https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch/) (especially the diagrams).  There are two reasons why we need `pack_padded_sequence()`:\n",
    "- First, packing saves computation for batch sequences, especially if there is a lot of padding involved. Packing before feeding sequences to an LSTM can speed up computation if there is one sequence in your batch that is much longer than all of the others.\n",
    "- The second, and more important, reason is that packing is used for *extracting the correct hidden state for each element of the batch* after passing the sequence through the LSTM.  Let's say we have $B$ sequences in our batch, and they have lengths $t_1, t_2, \\ldots, t_B$.  The size of `input_seqs` (i.e., the input to `forward()`) will be $B \\times T$, where $T = \\max_{i=1}^B t_i$.  Recall that in LSTMs, a hidden state is generated *for each time step* from $1$ to $T$. However, we want to ensure that for the first element in the batch, we end up extracting the hidden state corresponding to time $t_1$ (and for the second element, $t_2$, and for the third element, $t_3$, etc.). In short, packing helps us achieve this.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ASK WHERE WE SHOULD USE PADDING IDX\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, padding_idx: int):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.embeddings = nn.Embedding(input_size, hidden_size)#, padding_idx=self.padding_idx) \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True) ## ASK WHAT TO DO WITH THESE DIMENSIONS\n",
    "        \n",
    "    def forward(self, input_seqs: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        #print(\"the input_seqs shape in encoder \", input_seqs.shape)\n",
    "        seqs_length = [((t == 0).nonzero(as_tuple=True)[0])[0].item() if len(((t == 0).nonzero(as_tuple=True)[0])) != 0 else t.shape.numel() for t in input_seqs]\n",
    "        embedding_sequences = self.embeddings(input_seqs)\n",
    "        packed_seq_batch = torch.nn.utils.rnn.pack_padded_sequence(embedding_sequences, lengths=seqs_length, batch_first=True, enforce_sorted=False)\n",
    "        output, (hn, cn) = self.lstm(packed_seq_batch.float()) \n",
    "        return (hn, cn)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation cell\n",
    "\n",
    "#LSTM = EncoderLSTM(10, 10, 4)\n",
    "# evaluation cell\n",
    "batch = next(iter(train_dl))\n",
    "model = EncoderLSTM(train_dl.dataset.vocab_size, 100, 4)\n",
    "hidden_cell, final_cell = model(batch[0]) # Alias for model.forward(batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 100])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cell.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.6 Binary Classifier [7 points]</b>\n",
    "\n",
    "Now we can use our Encoder class to create a binary classifier! Again, we'll subclass `torch.nn.Module`, and you'll need to implement the `__init__()` and `forward()` methods. \n",
    "\n",
    "Since we've already designed the Encoder, we can use an instance of our LSTM Encoder class here. In particular, we can take the final (hidden, cell) states as features and use them as input to a standard classifier network. We'll leave the particular details up to you here. \n",
    "\n",
    "As this is a binary classifier, you may want to add a final sigmoid output layer to your network. However, best practice is to leave the model without the softmax and instead incorporate it into the loss function (take a look at [`torch.nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) and the more general [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) due to numerical stability improvements from the [log-sum-exp trick](https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/). Thus, your model should output logits (i.e., values *before* softmax), rather than probabilities (i.e., values *after* softmax), and use an appropriate loss function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, padding_idx: int):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(self.hidden_size, 512) \n",
    "        #self.layer_2 = nn.Linear(512, 256)\n",
    "        self.layer_out = nn.Linear(512, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(32)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.encoder = EncoderLSTM(self.input_size, self.hidden_size, 4)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        hidden_cell, final_cell = self.encoder(input_seq)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        #concat_cell_torch = torch.cat([hidden_cell, final_cell], 0)\n",
    "        concat_cell = hidden_cell.contiguous().view(-1, self.hidden_size)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        x = self.layer_1(hidden_cell)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.layer_2(x)\n",
    "        #x = self.batchnorm2(x)\n",
    "        #x = self.dropout(x)\n",
    "        output = self.layer_out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell\n",
    "#batch = next(iter(train_dl))\n",
    "#final_model = BinaryClassifier(train_dl.dataset.vocab_size, 100, 4)\n",
    "#predictions = model(batch[0]) # Alias for model.forward(batch)\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.7 Train the classifier [9 points]</b>\n",
    "\n",
    "Now, we get to train our classifier! Remember that we'll need to:\n",
    "1. Choose an optimizer. (Adam with default settings works well for us but feel free to experiment!)\n",
    "2. Choose a loss function. (Remember that for classifiers, this should be tied to your model's final, output layer.)\n",
    "3. Write a training loop. (We recommend 10-15 epochs.) **Please compute and print/graph a loss and a classification accuracy for each epoch on both the test and train sets.**\n",
    "    \n",
    "For reference, our solution runs 8 epochs in 7 minutes 25 seconds (on an i7-8650U @ 1.9 GHz).\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, training loss: 0.695, training accuracy: 56.304\n",
      "Epoch 1, testing loss: 0.027, testing accuracy: 49.290\n",
      "Epoch 2, training loss: 0.600, training accuracy: 66.976\n",
      "Epoch 2, testing loss: 0.015, testing accuracy: 51.161\n",
      "Epoch 3, training loss: 0.507, training accuracy: 74.464\n",
      "Epoch 3, testing loss: 0.020, testing accuracy: 50.806\n",
      "Epoch 4, training loss: 0.342, training accuracy: 85.520\n",
      "Epoch 4, testing loss: 0.042, testing accuracy: 53.194\n",
      "Epoch 5, training loss: 0.211, training accuracy: 92.032\n",
      "Epoch 5, testing loss: 0.057, testing accuracy: 54.645\n",
      "Epoch 6, training loss: 0.128, training accuracy: 95.440\n",
      "Epoch 6, testing loss: 0.029, testing accuracy: 53.290\n",
      "Epoch 7, training loss: 0.066, training accuracy: 97.864\n",
      "Epoch 7, testing loss: 0.105, testing accuracy: 50.581\n",
      "Epoch 8, training loss: 0.033, training accuracy: 98.848\n",
      "Epoch 8, testing loss: 0.098, testing accuracy: 52.258\n",
      "Epoch 9, training loss: 0.019, training accuracy: 99.472\n",
      "Epoch 9, testing loss: 0.087, testing accuracy: 52.581\n",
      "Epoch 10, training loss: 0.007, training accuracy: 99.880\n",
      "Epoch 10, testing loss: 0.130, testing accuracy: 53.000\n",
      "Finished Training\n",
      "CPU times: user 38min 19s, sys: 4min 31s, total: 42min 50s\n",
      "Wall time: 15min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "final_model = BinaryClassifier(train_dl.dataset.vocab_size, 512, 4)\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "# TODO: train the model!\n",
    "loss_function = BCEWithLogitsLoss()\n",
    "optimizer =  Adam(final_model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "epochs = 10 \n",
    "\n",
    "losses = {\n",
    "    'train': [], # keep track of your losses in these lists\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "accuracies = {\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    avg_training_acc = 0\n",
    "    test_running_loss = 0.0\n",
    "    avg_testing_acc = 0\n",
    "    \n",
    "    #curr_training_losses = []\n",
    "    #curr_test_losses = []\n",
    "    #curr_train_acc = []\n",
    "    #curr_test_acc = []    \n",
    "    #for i, (train_batch, test_batch) in enumerate(zip(train_dl,test_dl)):\n",
    "    for i, train_batch in enumerate(train_dl):\n",
    "        word_seq, targets = train_batch\n",
    "        \n",
    "        logit_output = torch.squeeze(final_model(word_seq), 0)\n",
    "        loss = loss_function(logit_output, targets)\n",
    "        training_acc = binary_acc(logit_output, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        avg_training_acc += training_acc.item()\n",
    "        \n",
    "      \n",
    "    for i, test_batch in enumerate(test_dl):\n",
    "        test_word_seq, test_targets = test_batch\n",
    "        \n",
    "        final_model.eval()\n",
    "        test_logit_output = torch.squeeze(final_model(test_word_seq), 0)\n",
    "        test_loss = loss_function(test_logit_output, test_targets)\n",
    "        testing_acc = binary_acc(test_logit_output, test_targets)\n",
    "        \n",
    "        \n",
    "        test_running_loss += test_loss.item()\n",
    "        avg_testing_acc += testing_acc.item()\n",
    "        \n",
    "        \n",
    "    print('Epoch %d, training loss: %.3f, training accuracy: %.3f' %\n",
    "            (epoch + 1, running_loss/len(train_dl), avg_training_acc/len(train_dl)))\n",
    "    print('Epoch %d, testing loss: %.3f, testing accuracy: %.3f' %\n",
    "            (epoch + 1, test_loss/len(test_dl), avg_testing_acc/len(test_dl)))\n",
    "    \n",
    "    losses['train'].append(running_loss/len(train_dl))\n",
    "    losses['test'].append(test_running_loss/len(test_dl))\n",
    "    accuracies['train'].append(avg_training_acc/len(train_dl))\n",
    "    accuracies['test'].append(avg_testing_acc/len(test_dl))\n",
    "    \n",
    "print(\"Finished Training\")\n",
    "\n",
    "#print(preds, targets)\n",
    "#print(preds.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [0.6945189666748047, 0.5995441625118255, 0.5074057605266571, 0.3418519648313522, 0.21117696098983288, 0.1278781119287014, 0.06550801435858011, 0.033349051187746226, 0.019261738791130485, 0.007361825395841151], 'test': [0.7281078340545777, 0.779636662813925, 0.8741336599473031, 1.0130852662747907, 1.3597813575498519, 1.5452370922411642, 2.197565516156535, 2.3472642206376597, 2.561699732657402, 3.2127631364330167]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff3f91fb250>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXyU5bn/8c+VfSV7QiCEgKyKCoK471rFtVZrkWLVtgd/Pe2p3V1+p9Z6TnvsaY+1/mzrUWvVqrggKipaRKFqVVZRWWUVQiAJgUASsk3m+v1xT5IhBDKQSZ7J5Hq/XnllMs8zz1wZwnfuuZ/7uW9RVYwxxvR9MV4XYIwxJjws0I0xJkpYoBtjTJSwQDfGmChhgW6MMVHCAt0YY6KEBbo5YiISKyK1IlIcAbW8LyI39fSxReRGEXmjJ+oQkeEiUnt0VRrTzgK9HwiEb+uXX0Tqg37++pEeT1VbVDVNVbf2RL3hICI3iMjGTu5PEJFdInLJkRxPVZ9Q1Slhqq1URM4NOvYmVU0Lx7E7PE+ciKiIlIT72CYyWaD3A4HwTQuExlbgiqD7nu64v4jE9X6VYfcikCciZ3a4/1KgCXir90sypmdZoBtE5D9F5DkRmSkiNcB0ETlNRD4SkWoR2SEiD4hIfGD/A1p+IvJUYPsbIlIjIh+KyLBDPFeMiMwSkZ2BYy8UkbFB2w97LBG5RETWicheEfkDIJ09j6ruB2YB3+iw6RvAU6raIiI5IjJXRCpFZI+IvCoigw9R97dFZGEodYjISBFZICJVgU8DfxORjMC2mcAg4I3AJ6QficgIEdGgxxeJyGsisltE1ovINzv8W80MvE41IrJSRE7qrObDCfw73CUiX4hIhYg8LiIDAttSROSZQP3VIrJYRHID274lIlsCz71JRKYe6XObnmOBblpdDTwDZADPAT7gViAXOAO4BLjlMI+fBvwcyMZ9CviPw+z7GjASGAisBP4WyrFEJB8X0rcH6ioFTjnM8zwBXCciSYHHZwGXAU8GtscAjwDFwFCgGfjDYY5HiHUI8J9AIXAsMDzw+6Cq1wNlwJTAJ6T7OnmK54DNuOD/GvDfInJO0PYv416zTOAN4IGuau7Et4HpwLnAMUAW7b/7zUAKUATkAP8KNAQC/z7gIlVNx/1dfHoUz216iAW6afW+qr6qqn5VrVfVJaq6SFV9qroJeBg45zCPn6WqS1W1GXgaGN/ZToHjP66qNaraANwNTBSR1BCOdTmwQlVfCmz7H6DyMDW9C+wGrgz8PBVYqaorA7VUBo5Vr6r7gF938Tu2Omwdqvq5qr6tqk2qWgH8PsTjEvg0Mhm4XVUbVHU58FfghqDd/qGqf1fVFlywd/pad+HrwO9UdbOq1gB3AtNEJAb3xpYLjAicL1mqqq0nbRUYJyJJqrpDVVcfxXObHmKBblptC/5BRMaIyOuBrpF9wD24/+SHsjPo9n6g05N84kbI/Hfg4/o+YENgU/CxD3WsQcF1qqof1zrulLqZ5/5Ge7fLDbhWe2stqSLyqIhsDdTyDof/HVsdtg4RGSgiz4vI9sBxHw/xuK3H3qWqdUH3fQEEdwV1fH2C3wxDNShw3ODnSADycPXOB1p/h3tFJC7wpnc98F1gZ6BbaNRRPLfpIRboplXHaTf/F9cdMkJVBwB3cYj+6iP0DdyJyfNx3TsjAveHcuwdwJDWHwKtyaIuHvMk8CUROR2YBMwM2vYzYBgwOfA7nh/KLxBCHb8BGoHjA8e9iQN/v8NNcVoG5Hb4xFIMbA+xtlCV4bqZgp+jCagMfLK4W1XHAmfiuuO+DqCqb6jqhbjupA24vxMTISzQzaGkA3uBusBJy8P1nx/pcRuBKlw/7a+O4LGvAeNF5KrASJwf4lqUh6SqG4FFuPMDb6hqcBdNOq6Fu0dEcnBvWuGoIx2oA/aKyBDgJx0eX47rV++s3s3AUuDXIpIoIuNxfdoHjUY6AokikhT0FYt7Y/uRiJSISDru32GmqvpF5HwRGRd4o9qH64JpEZFCEblCRFJw4V8HtHSjLhNmFujmUH4M3AjU4Fphz4XpuH/FtQ7LgFXAB6E+UFXLcScJf4t7QyjGhXVXnsC1Rp/scP99uE8JVYE6Dnnh0BHW8QtcP/heYA5uCGWwXwO/DIwg+UEnT/E13EnjnbiTr3eq6oJQajuEtUB90NcNuJPBzwHvAZtw/863BvYfBMzGhfkqXPfLTCAW+CnuE0oVcDrwvW7UZcJMbIELY4yJDtZCN8aYKGGBbowxUcIC3RhjooQFujHGRAnPJmHKzc3VkpISr57eGGP6pGXLlu1S1U6H63oW6CUlJSxdutSrpzfGmD5JRL441DbrcjHGmChhgW6MMVHCAt0YY6JERK1M09zcTGlpKQ0NDV6X0uOSkpIoKioiPj7e61KMMVEiogK9tLSU9PR0SkpKEAnHxH6RSVWpqqqitLSUYcM6XdjHGGOOWER1uTQ0NJCTkxPVYQ4gIuTk5PSLTyLGmN4TUYEORH2Yt+ovv6cxpvdEXKAbY0xUW/gb2PlZjxzaAj1IdXU1f/rTn474cZdeeinV1dU9UJExJqosfgQW/hpWzu6Rw1ugBzlUoLe0HH5Rlrlz55KZmdlTZRljosGG+fDGbTBqCpz/7z3yFBE1ysVrt99+Oxs3bmT8+PHEx8eTlpZGYWEhK1asYPXq1Xz5y19m27ZtNDQ0cOuttzJjxgygfRqD2tpapkyZwplnnskHH3zA4MGDeeWVV0hOTvb4NzPGeKpiLbxwM+SPhWsehZjYHnmaiA30X766itVl+8J6zGMHDeAXVxx3yO333nsvK1euZMWKFSxcuJDLLruMlStXtg0tfOyxx8jOzqa+vp6TTz6Za665hpycnAOOsX79embOnMkjjzzCddddx4svvsj06dPD+nsYY/qQul3wzHUQlwTXPwuJaT32VBEb6JFg8uTJB4wTf+CBB3jppZcA2LZtG+vXrz8o0IcNG8b48eMBmDhxIlu2bOm1eo0xEcbXCM9Nh9pyuOl1yBzSo08XsYF+uJZ0b0lNTW27vXDhQubPn8+HH35ISkoK5557bqfjyBMTE9tux8bGUl9f3yu1GmMijCq8eits/RCufQyKJvX4U9pJ0SDp6enU1NR0um3v3r1kZWWRkpLC2rVr+eijj3q5OmNMn/L+ffDJTDj3Thh3Ta88ZcS20L2Qk5PDGWecwbhx40hOTqagoKBt2yWXXMJDDz3ECSecwOjRozn11FM9rNQYE9FWz4G374Fx18I5P+u1pxVV7bUnCzZp0iTtuMDFmjVrGDt2rCf1eKG//b7G9AtlH8NjU2DgOLjxNYhPCuvhRWSZqnbaf2NdLsYYEy77ymDm9ZCaC1OfCXuYd6XLQBeRJBFZLCKfiMgqEfllJ/skishzIrJBRBaJSElPFGuMMRGrqQ5mToXGGpj2HKTl93oJobTQG4HzVfVEYDxwiYh07ED+FrBHVUcAvwd+E94yjTEmgvn98NItbo6Wax+DAm9G6XUZ6OrUBn6MD3x17Hi/CngicHsWcIHYdILGmP7inf+ANa/Cl34Foy72rIyQ+tBFJFZEVgAVwFuquqjDLoOBbQCq6gP2Ajkd9kFEZojIUhFZWllZ2b3KjTEmEqx4xg1RnHgTnPodT0sJKdBVtUVVxwNFwGQRGddhl85a4wcNn1HVh1V1kqpOysvLO/JqjTEmknzxAcz5Pgw7Gy79HXjcMXFEo1xUtRpYCFzSYVMpMARAROKADGB3GOrrVUc7fS7A/fffz/79+8NckTEmYu3eBM9+HbKGwnVPQqz36wOHMsolT0QyA7eTgQuBtR12mwPcGLh9LfCOejXAvRss0I0xIWnYC89MBfXDtOchOcvrioDQrhQtBJ4QkVjcG8DzqvqaiNwDLFXVOcBfgL+JyAZcy3xqj1Xcg4Knz73ooovIz8/n+eefp7Gxkauvvppf/vKX1NXVcd1111FaWkpLSws///nPKS8vp6ysjPPOO4/c3FwWLFjg9a9ijOkpLT544SbYvRFueBlyjvG6ojZdBrqqfgpM6OT+u4JuNwBfDWtlb9we/mWaBh4PU+495Obg6XPnzZvHrFmzWLx4MarKlVdeybvvvktlZSWDBg3i9ddfB9wcLxkZGdx3330sWLCA3Nzc8NZsjIksb94OG9+BK/8fDDvL62oOYFeKHsK8efOYN28eEyZM4KSTTmLt2rWsX7+e448/nvnz53Pbbbfx3nvvkZGR4XWpxpjesuhhWPIInP5vcNI3vK7mIJE7OddhWtK9QVW54447uOWWWw7atmzZMubOncsdd9zBl770Je66665OjmCMiSrr58Obt8HoS+HCgy6YjwjWQg8SPH3uxRdfzGOPPUZtrbumavv27VRUVFBWVkZKSgrTp0/nJz/5CcuXLz/oscaYKFOxBmbdDPnHwVce6bEl5LorclvoHgiePnfKlClMmzaN0047DYC0tDSeeuopNmzYwE9/+lNiYmKIj4/nz3/+MwAzZsxgypQpFBYW2klRY6JJ3S545msQnwzTenYJue6y6XM91N9+X2P6HF8jPHEl7FgBN82FooleV3TY6XOthW6MMZ1RdVeBbvsIrv1rRIR5V6wP3RhjOvPe/8CnzwaWkPuK19WEJOICvQ9eYHpU+svvaUyftPoVN4NiLy8h110RFehJSUlUVVVFfdipKlVVVSQl9e5qJsaYEGxfDrNvgaLJcNUfPZ9w60hEVB96UVERpaWl9IepdZOSkigqKvK6DGNMsH1l8Oy0wBJyT/f6EnLdFVGBHh8fz7Bhw7wuwxjTHzXVueGJjTXwrXmeLCHXXREV6MYY4wm/H2bPgPKVcP2zni0h110W6MYY8849sPY1uPi/PF1Crrsi6qSoMcb0uo+fhvd/DxNv9nwJue6yQDfG9F9b/gmv3grDzoFLf9unRrR0xgLdGNM/7d4Ez00PLCH3REQsIdddFujGmP6nvtqNaEEjagm57rKTosaY/qVtCblNEbeEXHdZoBtj+g9VeONnsGlBRC4h113W5WKM6T8WPwxL/xKxS8h1l7XQjTHRT9VNuPXm7RG9hFx3WaAbY6JXbaWbAnf5k7Drcxh4fEQvIdddXQa6iAwBngQGAn7gYVX9Q4d9zgVeATYH7pqtqveEt1RjjAmBv8X1kS9/EtbOBX8zDDnFzZx43NWQkOp1hT0mlBa6D/ixqi4XkXRgmYi8paqrO+z3nqpeHv4SjTEmBNXbYMXT8PFTsHcbJGfDKbfAhBsgf4zX1fWKLgNdVXcAOwK3a0RkDTAY6BjoxhjTu3xN8PmbsPwJ2PA2oDD8PLjoHhhzGcQlel1hrzqiPnQRKQEmAIs62XyaiHwClAE/UdVVnTx+BjADoLi4+EhrNcYYZ9d616XyyUyoq4T0QXD2T2HC1yGrxOvqPBNyoItIGvAi8ANV3ddh83JgqKrWisilwMvAyI7HUNWHgYcBJk2aFN3LEhljwqtpvxupsvxJ2PoBxMTBqEvgpBthxAVRe6LzSIQU6CISjwvzp1V1dsftwQGvqnNF5E8ikququ8JXqjGmXypb4UL8sxegcR9kH+OGHZ54PaQXeF1dRAlllIsAfwHWqOp9h9hnIFCuqioik3EXLFWFtVJjTP9RXw0rZ7kg3/EJxCXBsVe51vjQ0/v8rIg9JZQW+hnADcBnIrIicN+dQDGAqj4EXAt8R0R8QD0wVaN9pWdjTHipwtYPXYivehl89VBwPFz6Ozj+2qiZQKsnhTLK5X3gsG+Hqvog8GC4ijLG9CO1Fe7k5vInoWoDJKTD+OvdpfmF4601fgTsSlFjTO/zt8DGBW644bq54PdB8Wlw1o9d10oUX/zTkyzQjTG9p3qrW/Lt46dgXymk5Lpl3yZ8A/JGeV1dn2eBbozpWb5GWPeG61LZ+I6775jz4eJfuYmy4hK8rS+KWKAbY8JLFSrXui6Vje/AF/+E5v0woAjOuc1d/JNpFxb2BAt0Y0z31e2CTQtdgG98B2p2uPtzR7m5VEZ9yV2Sbxf/9CgLdGPMkfM1wtaP3KyGG99xY8XBDS0cfq7rUhl+HmQO8bLKfscC3RjTNVWoXNfeAm/tRomJc1PTnv/vLsQLx1sr3EMW6MaYzrV1owRa4TVl7v6cka4b5ZjzoeQMSEz3tEzTzgLdGOP4GmHbovZWeGs3SlJmezfKMefZCc0IZoFuTH/V2o3S2g++5f2Du1GGnw+DrBulr7BAN6Y/qasKBHjHbpQRMGF6oBvlTOtG6aMs0I2JZr5G2La4QzeKQlLGgaNRsoZ6XKgJBwt0Y6LVZ7Ngzvehuc51oxRNhvP+rwtx60aJShboxkSjLz6El78DgybAGT9w3ShJA7yuyvQwC3Rjos2eLfDc1yFjCFz/LKRke12R6SUxXhdgjAmjhn3wzFQ3Pe205y3M+xlroRsTLVp8MOubULUeps+G3BFeV2R6mQW6MdFi3v+FDW/B5ffD8HO8rsZ4wLpcjIkGS/4Cix6CU78Lk272uhrjEQt0Y/q6jQtg7k9h5MXwpf/wuhrjIQt0Y/qyXevhhRshbzRc86iNLe/nLNCN6av274ZnroOYeDc80caZ93tdBrqIDBGRBSKyRkRWicitnewjIvKAiGwQkU9F5KSeKdcYA4CvCZ67AfZuh6nP2KX7BghtlIsP+LGqLheRdGCZiLylqquD9pkCjAx8nQL8OfDdGBNuqvD6j+CL9+Erj0Cx/VczTpctdFXdoarLA7drgDXA4A67XQU8qc5HQKaIFIa9WmMMfPggfPw3OPuncMJ1XldjIsgR9aGLSAkwAVjUYdNgYFvQz6UcHPqIyAwRWSoiSysrK4+sUmMMrHsD5v0cjr0Kzr3T62pMhAk50EUkDXgR+IGq7uu4uZOH6EF3qD6sqpNUdVJeXt6RVWpMf7fzM5j1LTdT4pcfghgb02AOFNJfhIjE48L8aVWd3ckupUDw8t5FQFn3yzPGAFBT7uZoScqAqTMhIcXrikwECmWUiwB/Adao6n2H2G0O8I3AaJdTgb2quiOMdRrTfzXXw7PToH43THsWBtjpKdO5UEa5nAHcAHwmIisC990JFAOo6kPAXOBSYAOwH7Brj40JB1V45XuwfSl87SkoPNHrikwE6zLQVfV9Ou8jD95Hge+GqyhjTMA//htWzoILfgFjr/C6GhPh7KyKMZFq5Yuw8Ndw4jQ484deV2P6AAt0YyJR6TJ4+V+h+HS44n6Qw35INgawQDcm8uwthZlTIa3A9ZvHJXpdkekjbIELYyJJY60bnuhrgBtfhdQcrysyfYgFujGRwt8Cs/8FKlbBtBcgf4zXFZk+xgLdmEgx/25YNxem/BZGXuh1NaYPsj50YyLBx0/BBw/Ayd+GU2Z4XY3poyzQjfHalvfh1R/A8PPgkt94XY3pwyzQjfFS1UZ4bjpkD4OvPg6x1gtqjp4FujFeqa92wxMRmPYcJGd6XZHp46w5YIwXWnzwwk2wezN84xXIHu51RSYKWKAb44U3b4NNC+CqP0LJGV5XY6KEdbkY09sWPQxLHoXTvw8TpntdjYkiFujG9Kb1813rfPSlcOHdXldjoowFujG9pWItzLoZ8o+DrzwCMbFeV2SijAW6Mb2hbhc8cx3EJ7tVhxLTvK7IRCE7KWpMT/M1urHmteVw01zIKPK6IhOlLNCN6Umq7irQrR/CtY9B0USvKzJRzLpcjOlJ7/8ePnkGzr0Txl3jdTUmylmgG9NT1rwKb/8Sxl0L5/zM62pMP2CBbkxP2PEJzJ4BgyfBVQ/aEnKmV1gfujHhVL8H1s6Fd/4TkrNh6jNuZIsxvaDLQBeRx4DLgQpVHdfJ9nOBV4DNgbtmq+o94SzSmIjWsM8tTLHqJdjwNvibIWuYWw80vcDr6kw/EkoL/XHgQeDJw+zznqpeHpaKjOkLGmvh8zdh5WzYMB9aGmFAEZxyCxz3FRh8knWzmF7XZaCr6rsiUtLzpRgT4ZrqYP08F+Lr57mFnNMLYdI34biroehkiLHTUsY74epDP01EPgHKgJ+o6qrOdhKRGcAMgOLi4jA9tTE9qLke1r/lulM+fxOa90NqPky4AcZ9BYacaiFuIkY4An05MFRVa0XkUuBlYGRnO6rqw8DDAJMmTdIwPLcx4edrdH3hq15yfeNNtZCSAydOdS3xoWfYPCwmInU70FV1X9DtuSLyJxHJVdVd3T22Mb3G1wSbFsKq2bD2dWjcB8lZrhV+3Feg5CxbHs5EvG7/hYrIQKBcVVVEJuPGtld1uzJjelpLM2x+14X4mtegoRoSM2DsFS7Eh58DsfFeV2lMyEIZtjgTOBfIFZFS4BdAPICqPgRcC3xHRHxAPTBVVa07xUSmFh988b7rTlk9B+p3Q0I6jLnMdacccx7EJXpdpTFHJZRRLtd3sf1B3LBGYyKTv8VNjrXqJVj9CtRVQnwqjJ7iQnzEhRCf5HWVxnSbdQqa6OT3Q+liN8Rw9StQuxPikmHUxa5ffMRFkJDidZXGhJUFuokuZR/Dpy/A6pdh33aITYSRF7kQH3UJJKR6XaExPcYC3USHuiqY9+9uqtrYBNeNcuHdrlslMd3r6ozpFRbopm9ThU+fh7/fAQ174awfw+nfh+RMryszptdZoJu+a/cmeO1HsGmBu+z+ij9AwXFeV2WMZyzQTd/T0gwfPggL74WYeLj0d24+Fbt60/RzFuimbyldBq9+H8pXwpjL4dLfwoBBXldlTESwQDd9Q2ONWzRi0f9C+kA31/jYK7yuypiIYoFuIt/auTD3J7CvDE7+NlxwFyQN8LoqYyKOBbqJXPt2wBs/gzVzIP9Y+OoTMORkr6syJmJZoJvI4/fDsr/C/LvdVLYX3OWGItpEWcYclgW6iSwVa+DVW2HbIhh2Nlx+P+Qc43VVxvQJFugmMjQ3wHu/g/fvd1d2fvkht6CErctpTMgs0I33Nr8Hr/0AqjbACVPh4l9Baq7XVRnT51igG+/s3w1v/Rw+fgqySuCGl+CY872uypg+ywLd9D5VWPkivHm7C/Uzfwhn/8ymszWmmyzQTe/aswVe/zFsmA+DTnKt8oHHe12VMVHBAt30jhYffPQnWPhfIDFwyW9g8r/Y/CvGhJEFuul525e7oYg7P4VRU+Cy30FGkddVGRN1LNBNz2mshQW/hkV/htR8uO5JGHulDUU0podYoJue8fnfXV/53m1uatsLfmGLThjTwyzQTXjVlMObt8GqlyBvDHzz71B8qtdVGdMvxHS1g4g8JiIVIrLyENtFRB4QkQ0i8qmInBT+Mk3E8/th2ePwx5Nh7etw3r/DLe9ZmBvTi0JpoT8OPAg8eYjtU4CRga9TgD8Hvpto11gDWxfBlndhw9tu0YmhZ8IV90PuSK+rM6bf6TLQVfVdESk5zC5XAU+qqgIfiUimiBSq6o4w1WgiRdN+2PaRu1R/y3tu9Iq2uGXgiibBVX+E8V+3k57GeCQcfeiDgW1BP5cG7jso0EVkBjADoLi4OAxPbXpUcwOULm4P8NKl4G8GiYXBJ8EZt8Kws2DIKZCQ6nW1xvR74Qj0zppj2tmOqvow8DDApEmTOt3HeMjX6EJ7y3suxEuXQEujuxCocDyc9q9QcjYUn+JmRDTGRJRwBHopMCTo5yKgLAzHNT3N1wRlH7s+8M3vwbbF4KsHxF2OP/lfoOQsGHoaJGV4Xa0xpgvhCPQ5wPdE5FncydC91n8eoVp8sGMFbH7XtcK3fgTN+922gnEw8SbXhTL0dEjO8rRUY8yR6zLQRWQmcC6QKyKlwC+AeABVfQiYC1wKbAD2Azf3VLHmCPlb3OX2rX3gX3wITTVuW94YdwJz2FluZEpqjre1GmO6LZRRLtd3sV2B74atInP0/H43dLC1D/yLD6Bxr9uWMxJO+KrrQik5E9Lyva3VGBN2dqVoX9ZUB5Xr3MnLze/CF/+E+j1uW9YwOO4qdxKz5EwYUOhtrcaYHmeB3he0NLvl2cpXuUWUK9ZAxWo3t3jrgKLMYhh9metCKTnTZjM0ph+yQI8kfj9Ub2kP7Nbw3rXejf8GNwY8ZwQUnggnXg/5Y93trKGelm6M8Z4FuhdUoWZnUGivdl+V69pHnYBrdecfC6Mudt/zj3WX1Mclele7MSZiWaD3tP27D25xV6yGhur2fdIKXEt74k3ue/6xkDfaLt4xxhwRC/RwaaqDyrUHhnb5aqjd2b5PYoYL7OOuDrS4x7qv1Fzv6jbGRA0L9FD5mqCuAmrLoTbwvXpr0AnKL2g7QRmX5FrYx5zf3uLOHwsDBtnEVcaYHtO/A93vd8P8astdS7q2Q2AH324dDhhMYl2f9qAJ7iKd1vDOKrHFj40xvS46A72xtkMwdxLQtRWuxe33Hfz4uGRIL3B927kjAxfiFLiLcYK/p+ZDXELv/37GGNOJvhfotRVuQqmOAV0T9HNz3cGPk1hIzWsP6oHjAuEcHNSB2wlp1jVijOlz+l6gb3kfZgVNF5OU2R7Egyd23pJOK4CUbOsGMcZEtT4X6HsLT6f8ylcoKiohJbvQxmQbY0xAnwv0d0v9/NvzdcAqirM3M6ogndED0xg9cACjC9IZlptKQlyXa18bY0zU6XOBftoxOTw0fSKfl9ewrryGdTtrWLCugha/GzIYHysMz01j1MB0Rhe0B31RVjIxMdYvboyJXuJmv+19kyZN0qVLl4blWI2+FjZV1rFupwv5z3fWsHZnDdur69v2SUmIZWSBC/lRBemMGTiAUQPTyEtLROwEqDGmjxCRZao6qbNtfa6F3pnEuFjGFg5gbOGAA+6vaWhmfUWtC/rA1/w1FTy/tLRtn6yUeEYPTGd0QTqjBqYzZmA6IwvSGZAU39u/hjHGdEtUBPqhpCfFc1JxFicVH7ic2q7axrZWfGvXzaxlpdQ1tbTtMzgzmVEFaW0hP6ognWPy0kiKt5EyxpjIFNWBfii5aYnkjkjk9BHtc6j4/cr26no+Lw8K+p01vL9hF80trlsqNkYoyUlhdCDgRxe41nxJTgpxsXYi1hjjrX4Z6J2JiRGGZKcwJDuFC8YWtN3f3OJny666thOw63bWsLpsH2+s3Enr6YeE2BiG56UysiCdUflp7ntBGkNzUom1E7HGmF5igd6F+NgYRgZa4pef0FzGluMAAA03SURBVH7//iYfGypq+by8lvXlrkW//Is9vPpJWds+CXExHJOX5rpuCtIZme++D8lOsaA3xoSdBfpRSkmI44SiTE4oyjzg/trG1qCvCQR9LUs27+aVFe1BnxTfGvTpjCxIY1S+68KxoZXGmO4IKdBF5BLgD0As8Kiq3tth+03Ab4HtgbseVNVHw1hnn5GWGMf4IZmMH3Jg0LeOuGkN+c/La/hwYxUvfby9bZ/k+FhG5KcxsiDNjboJBP7gzGQbWmmM6VKXgS4iscAfgYuAUmCJiMxR1dUddn1OVb/XAzVGhUONuNlb38yGivaQX19ey/vrdzF7eXvQpybEMiLQP9/Wqi9IpzAjyYLeGNMmlBb6ZGCDqm4CEJFngauAjoFujkJGcjwTh2YzcWj2AfdX729ifUV7yH9eXsOCdZW8sKx9DH16YhwjCtI4sSiTC8bmc8qwHJv2wJh+LJRAHwxsC/q5FDilk/2uEZGzgc+BH6rqtk72MSHKTEng5JJsTi45MOh31zW1nYRtbdXPXLyVxz/YQlpiHOeMyuOCsfmcNzqfrFSbq92Y/iSUQO/sM33H+QJeBWaqaqOI/B/gCeD8gw4kMgOYAVBcXHyEpRqA7NQEThmewynDc9ruq29q4Z8bdvH22nLeXlPB65/tIEZg0tBsLhibzwVjCzgmL9W6Z4yJcl3O5SIipwF3q+rFgZ/vAFDV/zrE/rHAblXNONxxwzmXi2nn9yufbd/L22vKmb+mgtU79gFQkpPChWMLuGBsAZNKsoi3C6GM6ZMON5dLKIEeh+tGuQA3imUJME1VVwXtU6iqOwK3rwZuU9VTD3dcC/Tesb26nncC4f7hxiqaWvwMSIrj3NH5XHhsAeeMyiMj2eatMaav6NbkXKrqE5HvAX/HDVt8TFVXicg9wFJVnQN8X0SuBHzAbuCmsFVvumVwZjI3nFbCDaeVUNvo4/31lcxfU8GCtRXM+aSMuBjh5BLXNXPRsQUMzUn1umRjzFGKiulzzZFr8SsrtlUzf005b68p5/PyWgBG5Ke5cB9bwITiLLui1ZgI060ul55igR5Ztlbt5+215cxfU86iTbvx+ZXs1ATOHZ3HhWMLOHtUHmmJdmGxMV6zQDdHZF9DM+9+Xsnbayp4Z20Fe+ubiY8VTh2eEzixmk9RVorXZRrTL1mgm6Pma/Gz7Is9vL22gvmry9m0qw6AMQPT28L9xKJMm4PGmF5igW7CZlNlLW+vqWD+mnKWfrGHFr+Sm5bI+WPyOGtkHpOHZVMwIMnrMo2JWhbopkdU729i4bpK5q8p5x/rKqlp9AFQnJ3C5GHZTC7J5uRh2ZTkpNhFTcaEiQW66XHNLX5Wl+1jyZbdLN68myVbdrNnfzPgVoiaPCyrbSqDsYUDbPSMMUfJAt30OlVlY2UtizfvYfHmKpZs2cP26nrATSp20tAsJg9zAX9CUYat1WpMiCzQTUTYXl3Pks27WbxlN0s272Z9hRv7nhAbw4lDMlwLflg2E4dmMSDJrl41pjMW6CYi7alrYskW1z2zeMseVm3fi8+viMDYgQPaWvAnD8siP91OtBoDFuimj9jf5OPjrdVtffAfb62mvrkFcJOLtbbgJ5dkM9ROtJp+qltzuRjTW1IS4jhjRC5njMgF3InWldv3Bk607uGtNeVtC3zkpSe6UTQlWZw8LJsxA+1EqzHWQjd9ht+vbKisbWvBL9m8m7K9DYA70TqxJCswiiad4blpFGUlE2fTBJsoYy10ExViYoRRgcWzp586FIDSPfvbWvBLtuxm4bp1bfvHxwpDc1IZnpvK8Ly0wHd3O9tWczJRyALd9GlFWSkUZaVw9YQiwF3stKGilk2VdWzc5b5v2lXHgnUVNLe0fxrNTIlvD/q8VIbnpnFMXirFOSkkxtkQStM3WaCbqJKZksCkkmwmdViL1dfip3RPPZsCIb+xso5NlbX84/NKZgUtvB0jMCQ7hWG5LuRdiz6VY/LSyE9PtBOxJqJZoJt+IS42hpLcVEpyUzl/zIHbahqa2byrzrXmK2vZGLj90aYqGpr9bfulJca5oM9rD/vWn1MS7L+S8Z79FZp+Lz0pnhOKMjmhKPOA+/1+Zee+hkC3TWvLvpalW/Yw55MygscTFGYkHRT0RVnJFGYkk2rzyJteYn9pxhxCTIwwKDOZQZnJnDky94BtDc0tbKlqb9W7Pvs6Xl6xnZoG3wH7DkiKY1BmMoUZSW3HK8xIojAjmUGZSQzMSLJ+exMWFujGHIWk+FjGDBzAmIEDDrhfVdlV28SWqjrKquspq25gx97275+U7mV3XdNBx8tNS6AwIzj02wO/MCOZ/PREG4JpumSBbkwYiQh56YnkpScecp/6phZ27K1nx94Gyqrd99bQ31JVxwcbq6htPLCVHyNQMCDJtewzkxkU1MJ3Lf5kclITbKGRfs4C3ZhelpwQGxgumXbIffY1NLOjuoGyvfXs6NDKX7V9L/NXl9Po8x/wmITYGAZmJLW18gszXHfOgKR40pPiSG/77m6nJcbZ1bVRxgLdmAg0ICmeAQPjGT0wvdPtqsruuqYDWvmt4V9WXc/izbvZua+BFv/hrwRPS4w7IOQ7Bv+A4DeBxAO3D0iKJy3J3hQiSUiBLiKXAH8AYoFHVfXeDtsTgSeBiUAV8DVV3RLeUo0xrUSEnLREctISGTc4o9N9WvxKVV0jNQ2+wFfzAd/3dXJfVW0TW3bVtT2mqcXf6bGDpSbEHtT6b/0+IHBfSkIcCXEx7ivWfY+PDf5ZSIiNJT5O2rZ33C8uRuw6gC50GegiEgv8EbgIKAWWiMgcVV0dtNu3gD2qOkJEpgK/Ab7WEwUbY0ITGyPkpyeR33kjPyQNzS0d3gyC3xAOvq+msZk9+5vYuns/NQ3N7Gvw0eTr+k0hFCIQHxtDYmwM8YHAb38DiCUhVg56o4iPC+wf9OYQHyfEx7jbcbHu8XGx4vYJuh0f+B4XdDu+w+24GGl7s4mPiwkcV4j16M0nlBb6ZGCDqm4CEJFngauA4EC/Crg7cHsW8KCIiHo185cxJiyS4mNJio897EnerjT6Wtjf2EJzi59Gn5/mFj9NLX6afUpTSwtNPqWpxU9T6zafv+3n4PuaW/w0tnS8T2nyBR038JjaRt8Bxwnev7nF/dxFb1S3iNAW7nEd3gjiYoVpk4v59lnDw/68oQT6YGBb0M+lwCmH2kdVfSKyF8gBdgXvJCIzgBkAxcXFR1myMaYvSYyLjchx9i3+9nD3tQa9X2n2+fH5/TT5FJ+/9c2g/Xbrm4Kvxb0R+YLeJNq3+WlqUXyt9weO23o7N+3o3yAPJ5RA7+xzQ8f3tlD2QVUfBh4GN31uCM9tjDE9IjZGiI2Jjar1bEO5UqEUGBL0cxFQdqh9RCQOyAB2h6NAY4wxoQkl0JcAI0VkmIgkAFOBOR32mQPcGLh9LfCO9Z8bY0zv6rLLJdAn/j3g77hhi4+p6ioRuQdYqqpzgL8AfxORDbiW+dSeLNoYY8zBQhqHrqpzgbkd7rsr6HYD8NXwlmaMMeZI2Gw/xhgTJSzQjTEmSligG2NMlLBAN8aYKCFejS4UkUrgi6N8eC4drkLt5+z1OJC9Hu3stThQNLweQ1U1r7MNngV6d4jIUlWd5HUdkcJejwPZ69HOXosDRfvrYV0uxhgTJSzQjTEmSvTVQH/Y6wIijL0eB7LXo529FgeK6tejT/ahG2OMOVhfbaEbY4zpwALdGGOiRJ8LdBG5RETWicgGEbnd63q8JCJDRGSBiKwRkVUicqvXNXlNRGJF5GMRec3rWrwmIpkiMktE1gb+Rk7zuiaviMgPA/9HVorITBFJ8rqmntCnAj1oweopwLHA9SJyrLdVecoH/FhVxwKnAt/t568HwK3AGq+LiBB/AN5U1THAifTT10VEBgPfByap6jjcNOBROcV3nwp0ghasVtUmoHXB6n5JVXeo6vLA7Rrcf9jB3lblHREpAi4DHvW6Fq+JyADgbNxaBahqk6pWe1uVp+KA5MCKaikcvOpaVOhrgd7ZgtX9NsCCiUgJMAFY5G0lnrof+Bng97qQCDAcqAT+GuiCelREUr0uyguquh34HbAV2AHsVdV53lbVM/paoIe0GHV/IyJpwIvAD1R1n9f1eEFELgcqVHWZ17VEiDjgJODPqjoBqAP65TknEcnCfZIfBgwCUkVkurdV9Yy+FuihLFjdr4hIPC7Mn1bV2V7X46EzgCtFZAuuK+58EXnK25I8VQqUqmrrJ7ZZuIDvjy4ENqtqpao2A7OB0z2uqUf0tUAPZcHqfkNEBNdHukZV7/O6Hi+p6h2qWqSqJbi/i3dUNSpbYaFQ1Z3ANhEZHbjrAmC1hyV5aStwqoikBP7PXECUniAOaU3RSHGoBas9LstLZwA3AJ+JyIrAfXcG1oA15t+ApwONn03AzR7X4wlVXSQis4DluJFhHxOlUwDYpf/GGBMl+lqXizHGmEOwQDfGmChhgW6MMVHCAt0YY6KEBboxxkQJC3RjjIkSFujGGBMl/j/WJncJzPc4PgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optionally plot your losses\n",
    "print(losses)\n",
    "pd.DataFrame(losses).plot(title='Train and Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [56.304, 66.976, 74.464, 85.52, 92.032, 95.44, 97.864, 98.848, 99.472, 99.88], 'test': [49.29032258064516, 51.16129032258065, 50.806451612903224, 53.193548387096776, 54.645161290322584, 53.29032258064516, 50.58064516129032, 52.25806451612903, 52.58064516129032, 53.0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff41b619410>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# optionally plot your losses\n",
    "print(accuracies)\n",
    "pd.DataFrame(accuracies).plot(title='Train and Validation Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.8 Explore the model [2 points]</b>\n",
    "    \n",
    "Congrats on training your model! Hopefully you see meaningful results on a macroscopic scale (in terms of training/test loss). Now, let's explore our model from a more _microscopic_ perspective. **Please display three sentences** in the test set that (a) were classified correctly by your model and (b) were classified incorrectly **(for a grand total of 6 sentences)**. Do you notice anything interesting about these sentences that could help explain why the model classified them correctly or incorrectly? (Note: There is no particular, exact answer we are looking for here; rather, we are just looking to see if you say something reasonable, justifying, and plausible.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explore your trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"header_yellow\"> \n",
    "    \n",
    "# Machine Translation \n",
    "    \n",
    "</div>\n",
    "</br>\n",
    "\n",
    "Now that we've warmed up by implementing a sentiment analysis model, we're reading to try a new task: machine translation. Fortunately, we can re-use some of your work from the previous part! We now want to add a Decoder component to your architecture to perform sequence-to-sequence translation. \n",
    "\n",
    "Why are we doing machine translation? As we mentioned at the beginning of this notebook, your task is to help save the day by determining which language the Dark Web hackers are speaking. That is, we've obtained some text from them which seems non-sensical. However, the investigators currently suspect it's either Danish, English, German, Finnish, or Spanish.\n",
    "\n",
    "Your first task is to assume it's English and to see how well you can translate from a bunch of their \"noisy\" data to English (we were fortunate to have a parallel corpus that includes the exact English equivalent of their noisy data).\n",
    "\n",
    "The investigation includes some amazing linguists who were able to also provide parallel corpora for all other languages mentioned above. So, your second task is to consider each of these languages as the true \"source\" language, and aim to translate each to the \"noisy\" target (aka mystery language) that the hackers have been communicating with.\n",
    "\n",
    "Best of luck!\n",
    "\n",
    "Pictured below is our only footage of the suspects:\n",
    "\n",
    "<img src=\"images/hackers1.png\" width=\"400\"><img src=\"images/hackers2.png\" width=\"400\"> (Fun Fact: movie star Angelina Jolie was in this movie, Hackers (1995))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Like before, we have provided pre-tokenized data for you to use. The data\n",
    "files consist of a train set `mt_train_sentences.pkl`, a validation set `mt_val_sentences.pkl`, and a test set `mt_test_sentences.pkl`, \n",
    "with corresponding lists of examples from all six languages:\n",
    "\n",
    "- Danish\n",
    "- English\n",
    "- German\n",
    "- Finnish\n",
    "- Spanish\n",
    "- Mystery langauge\n",
    "\n",
    "\n",
    "The file is formatted as:\n",
    "\n",
    "```\n",
    "{\n",
    "    'danish': [\n",
    "        [token token token ...],\n",
    "        [token token token ...],\n",
    "        ...\n",
    "    ]\n",
    "    'english': [\n",
    "        [token token token ...],\n",
    "        [token token token ...],\n",
    "        ...\n",
    "    ],\n",
    "    'mystery': [\n",
    "        [token token token ...],\n",
    "        [token token token ...],\n",
    "        ...\n",
    "    ],\n",
    "    # etc\n",
    "}\n",
    "```\n",
    "\n",
    "The key indicates the source language, and the $n$-th sentence (`[token token token ...]`) in each list are parallel translations in one of the six languages.\n",
    "\n",
    "Again, in short, your tasks are to:\n",
    "\n",
    "**(a)** train your model on the mystery -> English corpus;\n",
    "\n",
    "**(b)** train your model on each of the five pairings:\n",
    "-  Danish -> Mystery\n",
    "-  English -> Mystery\n",
    "-  German -> Mystery\n",
    "-  Finnish -> Mystery\n",
    "-  Spanish -> Mystery\n",
    "\n",
    "**Note:** these pairings are the reverse of what you did in part **a**. This is because it's natural to view the Mystery language as being the denoised version of the true, \"source\" language. So, we wish to model the mapping from the original source to this denoised version.\n",
    "\n",
    "**(c)** use the results to figure out the identity of the original mystery language you've been given.\n",
    "\n",
    "**Note:** we've randomly assigned mystery languages to each student, so your\n",
    "friend's language will not match up with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.9 Process translation data [2 points]</b>\n",
    "    \n",
    "In the cell below, we provide pre-processing code to load in each dataset and build in its dataloader. \n",
    "Almost everything is given to you *except* the collate function, which you will\n",
    "have to implement in the same style as the previous part, but now adapted for\n",
    "seq2seq translation.\n",
    "\n",
    "Please spend some time reading through the code and playing around with the objects to make sure you understand them.  For example, we recommend that you explore a few batches in each dataloader to understand what will be fed to the model.  Recall that you can generate a batch from any dataloader `dl` with `next(iter(dl))`. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/mt_train_sentences.pkl\", \"rb\") as f:\n",
    "    train_dict = pickle.load(f)\n",
    "\n",
    "with open(\"data/mt_val_sentences.pkl\", \"rb\") as f:\n",
    "    val_dict = pickle.load(f)\n",
    "\n",
    "with open(\"data/mt_test_sentences.pkl\", \"rb\") as f:\n",
    "    test_dict = pickle.load(f)\n",
    "\n",
    "class TranslationDataset(Dataset):   \n",
    "    def __init__(self, source: List[List[str]], target: List[List[str]], min_freq=3, source_vocab=None, target_vocab=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if source_vocab is None and target_vocab is None:\n",
    "            self.source_vocab, self.source_vocab_size = generate_vocab(get_word_counts(source), min_freq)\n",
    "            self.target_vocab, self.target_vocab_size = generate_vocab(get_word_counts(target), min_freq)\n",
    "        else:\n",
    "            self.source_vocab = source_vocab\n",
    "            self.source_vocab_size = len(source_vocab)\n",
    "\n",
    "            self.target_vocab = target_vocab\n",
    "            self.target_vocab_size = len(target_vocab)\n",
    "\n",
    "        self.source, self.target = self._get_idx_dataset(source, target, self.source_vocab, self.target_vocab)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.source[idx]), torch.tensor(self.target[idx])\n",
    "\n",
    "    def _get_idx_dataset(self, source: List[List[str]], target: List[List[str]], source_vocab: Dict[str, int], target_vocab: Dict[str, int]):\n",
    "        source_toks = []\n",
    "        target_toks = []\n",
    "\n",
    "        for src, tar in zip(source, target):\n",
    "            source_toks.append([source_vocab[token] if token in source_vocab else source_vocab[\"<UNK>\"] for token in src] + [source_vocab[\"<EOS>\"]])\n",
    "            target_toks.append([target_vocab[\"<EOS>\"]] + [target_vocab[token] if token in target_vocab else target_vocab[\"<UNK>\"] for token in tar] + [target_vocab[\"<EOS>\"]])\n",
    "        \n",
    "        return source_toks, target_toks\n",
    "\n",
    "train_ds = TranslationDataset(train_dict[\"mystery\"], train_dict[\"english\"])\n",
    "test_ds = TranslationDataset(test_dict[\"mystery\"], test_dict[\"english\"], source_vocab=train_ds.source_vocab, target_vocab=train_ds.target_vocab)\n",
    "val_ds = TranslationDataset(val_dict[\"mystery\"], val_dict[\"english\"], source_vocab=train_ds.source_vocab, target_vocab=train_ds.target_vocab)\n",
    "\n",
    "# TODO: implement this padding function\n",
    "# ASK IF BASICALLY WE NEED TO PAD BOTH TENSORS in batch[0] to be of the same size or do we need to patch all \n",
    "# tensors in batch[0] (for all 32 entries) \n",
    "def pad_collate(batch: List[Tuple[torch.tensor, torch.tensor]]) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    batch_source_lang = [item[0] for item in batch]\n",
    "    batch_target_lang = [item[1] for item in batch]\n",
    "    padded_source_lang = pad_sequence(batch_source_lang, batch_first=True, padding_value=PADDING_IDX)\n",
    "    padded_target_lang = pad_sequence(batch_target_lang, batch_first=True, padding_value=PADDING_IDX)\n",
    "    return (padded_source_lang, padded_target_lang)\n",
    "    \n",
    "train_dl = DataLoader(train_ds, collate_fn=pad_collate, num_workers=0, shuffle=True, batch_size=32)\n",
    "test_dl = DataLoader(test_ds, collate_fn=pad_collate, num_workers=0, shuffle=True, batch_size=32)\n",
    "val_dl = DataLoader(val_ds, collate_fn=pad_collate, num_workers=0, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24])\n",
      "torch.Size([32, 14])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.10 The LSTM Decoder [8 points]</b>\n",
    "    \n",
    "We are going to build a sequence-to-sequence (seq2seq) translation system.  Our seq2seq will have two major components: (1) an encoder that maps a source sentence to a tuple of (hidden, cell) vectors and (2) a decoder that uses the (hidden, cell) vectors to generate a translation.  The good news is that the encoder has already been implemented for you -- that is, you did it yourself in the previous problem!  We can simply reuse the `EncoderLSTM` that you used for the text classifier as our encoder; part of the point of this exercise is to illustrate that deep learning enables the use of common architectures for NLP problems that are quite different from one another (e.g. text classification vs. machine translation).\n",
    "\n",
    "Thus, our next task is to build the `DecoderLSTM` object.  Like the `EncoderLSTM`, there are two main functions to implement:\n",
    "- `__init__()`: Defines the layers of our network.  We will need an `Embedding` layer to encode the target vocabulary as vectors and an `LSTM` layer for decoding our target words.\n",
    "- `forward()`: Takes three arguments -- (a) the `input_seqs` of the target (i.e. a batch size x sequence length tensor), (b) `hidden_init`, the last hidden state of the encoder, and (c) `cell_init`, the last cell state of the encoder.  This function will return a tuple of three tensors -- (a) the hidden states of the LSTM for all time steps, (b) the final hidden state of the LSTM, and (c) the final cell state of the LSTM.\n",
    "\n",
    "Before moving on to the next section, we recommend testing your `forward()` function with randomly generated inputs (try  `torch.randint` for integer tensors and `torch.randn` for float/decimal tensors) to make sure it works without error.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, padding_idx: int):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.embeddings = nn.Embedding(input_size, hidden_size)#, padding_idx=self.padding_idx) \n",
    "        self.decoder_lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True) ## ASK WHAT TO DO WITH THESE DIMENSIONS\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_seqs: torch.tensor, \n",
    "        hidden_init: torch.tensor, \n",
    "        cell_init: torch.tensor\n",
    "    ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "        embedding_sequences = self.embeddings(input_seqs)\n",
    "        output, (hn, cn) = self.decoder_lstm(embedding_sequences, (hidden_init, cell_init))\n",
    "        return (output, hn, cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1089, -0.2577, -0.0240,  ...,  0.0620,  0.0277, -0.0481],\n",
       "          [-0.0345, -0.1542, -0.1204,  ..., -0.0497, -0.0327, -0.1304],\n",
       "          [-0.2038, -0.1323,  0.0542,  ...,  0.0081, -0.0424, -0.2093],\n",
       "          ...,\n",
       "          [ 0.0429, -0.0839,  0.0370,  ..., -0.0443,  0.1593, -0.1101],\n",
       "          [-0.0899, -0.0823,  0.0840,  ..., -0.0758,  0.1651, -0.1370],\n",
       "          [-0.1659, -0.0858,  0.1083,  ..., -0.0959,  0.1644, -0.1493]],\n",
       " \n",
       "         [[ 0.1064, -0.2630, -0.0173,  ...,  0.0939,  0.0534,  0.0219],\n",
       "          [-0.1145,  0.1725, -0.1873,  ..., -0.1757,  0.1707, -0.1821],\n",
       "          [ 0.1707,  0.1943,  0.0417,  ..., -0.1207,  0.0167, -0.3924],\n",
       "          ...,\n",
       "          [ 0.0932, -0.1047,  0.1837,  ..., -0.0669,  0.0730, -0.1541],\n",
       "          [-0.0634,  0.0921,  0.1692,  ...,  0.0798,  0.0629, -0.1504],\n",
       "          [-0.1360, -0.1379, -0.0147,  ...,  0.1210,  0.0769, -0.1689]],\n",
       " \n",
       "         [[ 0.0698, -0.2592,  0.0119,  ...,  0.0368,  0.0502,  0.0312],\n",
       "          [-0.1534,  0.1623, -0.1371,  ..., -0.1973,  0.1623, -0.1756],\n",
       "          [ 0.1569,  0.1872,  0.0528,  ..., -0.1430,  0.0050, -0.3872],\n",
       "          ...,\n",
       "          [-0.2564, -0.0899,  0.1252,  ..., -0.1262,  0.1485, -0.1511],\n",
       "          [-0.2571, -0.0911,  0.1346,  ..., -0.1331,  0.1515, -0.1544],\n",
       "          [-0.2573, -0.0918,  0.1386,  ..., -0.1365,  0.1540, -0.1557]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1212, -0.2257, -0.0276,  ...,  0.0869,  0.0417, -0.0067],\n",
       "          [-0.0845,  0.1715, -0.1891,  ..., -0.1779,  0.1583, -0.1867],\n",
       "          [ 0.1766,  0.1910,  0.0454,  ..., -0.1224,  0.0021, -0.3943],\n",
       "          ...,\n",
       "          [-0.1148, -0.0821,  0.0732,  ..., -0.0931,  0.1529, -0.1341],\n",
       "          [-0.1795, -0.0842,  0.1058,  ..., -0.1087,  0.1561, -0.1467],\n",
       "          [-0.2137, -0.0873,  0.1231,  ..., -0.1187,  0.1570, -0.1522]],\n",
       " \n",
       "         [[ 0.0562, -0.2688,  0.0049,  ...,  0.0407,  0.1125,  0.0444],\n",
       "          [-0.0547, -0.1605, -0.0842,  ..., -0.0534,  0.0783, -0.1079],\n",
       "          [-0.2116, -0.1426,  0.0895,  ...,  0.0127,  0.0153, -0.1959],\n",
       "          ...,\n",
       "          [-0.2327, -0.0888,  0.1157,  ..., -0.0875,  0.1494, -0.1449],\n",
       "          [-0.2445, -0.0897,  0.1309,  ..., -0.1071,  0.1526, -0.1515],\n",
       "          [-0.2507, -0.0905,  0.1375,  ..., -0.1193,  0.1546, -0.1543]],\n",
       " \n",
       "         [[ 0.0939, -0.2747, -0.0100,  ...,  0.0325,  0.0592,  0.0686],\n",
       "          [ 0.0732, -0.0121, -0.1736,  ..., -0.0897,  0.1625,  0.0812],\n",
       "          [-0.0952, -0.0346,  0.0138,  ..., -0.0227,  0.2110, -0.0215],\n",
       "          ...,\n",
       "          [-0.1754, -0.1029,  0.0491,  ...,  0.0532,  0.1427, -0.0720],\n",
       "          [-0.2165, -0.0867,  0.1116,  ..., -0.0147,  0.1458, -0.1197],\n",
       "          [-0.2385, -0.0848,  0.1329,  ..., -0.0583,  0.1485, -0.1398]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([[[-0.1659, -0.0858,  0.1083,  ..., -0.0959,  0.1644, -0.1493],\n",
       "          [-0.1360, -0.1379, -0.0147,  ...,  0.1210,  0.0769, -0.1689],\n",
       "          [-0.2573, -0.0918,  0.1386,  ..., -0.1365,  0.1540, -0.1557],\n",
       "          ...,\n",
       "          [-0.2137, -0.0873,  0.1231,  ..., -0.1187,  0.1570, -0.1522],\n",
       "          [-0.2507, -0.0905,  0.1375,  ..., -0.1193,  0.1546, -0.1543],\n",
       "          [-0.2385, -0.0848,  0.1329,  ..., -0.0583,  0.1485, -0.1398]]],\n",
       "        grad_fn=<StackBackward>),\n",
       " tensor([[[-0.3943, -0.2642,  0.1793,  ..., -0.1893,  0.3430, -0.6627],\n",
       "          [-0.2326, -0.2282, -0.0487,  ...,  0.2444,  0.2512, -0.4200],\n",
       "          [-0.6707, -0.2831,  0.2294,  ..., -0.2651,  0.3149, -0.6987],\n",
       "          ...,\n",
       "          [-0.5310, -0.2679,  0.2028,  ..., -0.2319,  0.3244, -0.6820],\n",
       "          [-0.6504, -0.2789,  0.2273,  ..., -0.2316,  0.3170, -0.6934],\n",
       "          [-0.6153, -0.2632,  0.2203,  ..., -0.1132,  0.3075, -0.6239]]],\n",
       "        grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model = EncoderLSTM(train_dl.dataset.source_vocab_size, 100, 4)\n",
    "#import pdb\n",
    "#pdb.set_trace()\n",
    "hidden_cell, final_cell = encoder_model(batch[0]) # Alias for model.forward(batch)\n",
    "decoder_model = DecoderLSTM(train_dl.dataset.target_vocab_size, 100, 4)\n",
    "decoder_model(batch[1], hidden_cell, final_cell)\n",
    "#batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.11 The Seq2Seq model [14 points] </b>\n",
    "    \n",
    "Now comes the fun part: putting everything together!  Using our `EncoderLSTM` and `DecoderLSTM` blocks, let's implement the `Seq2Seq` model below.  There are three main functions to implement:\n",
    "- `__init__()`: Initializing the `Seq2Seq2` model.  You will need three blocks -- the `EncoderLSTM`, the `DecoderLSTM`, and a `Linear` projection layer to map hidden states of the decoder to the target vocabulary for word prediction.  This final projection layer should be something very familiar to you -- i.e. it serves the same role as the final projection layer in `CBOW` (word2vec) from Homework #1.\n",
    "- `forward()`: This function will be used for training the model.  Machine translation models are typically trained with a concept call *teacher forcing*, in which the ground-truth tokens from the true target translation (dubbed the \"teacher\") are used as context to predict the next word.  Let $B$ be the batch size, $T_i$ be the sequence length for the input sequence, $T_o$ be the sequence length for the output sequence, $V_o$ be the size of the output vocabulary, and $M_o$ be the number of non-pad tokens in `output_seqs`.  Our `forward()` function will take a $B \\times T_i$ `input_seqs` tensor and its corresponding target translation, the $B \\times T_o$ `output_seqs` tensor, and return a tuple of (1) a flattened predictions tensor `preds` of size $M_o \\times V_o$ and (2) a flattened target words tensor `targs` of length $M_o$ (Note: `targs` should not contain any padding indices).   \n",
    "- `generate()`: Recall that in machine translation, training is very different from inference.  In training, we use the `forward()` function to train the model with *teacher forcing*.  However, at inference time, we don't know what the true translation is, so the model must generate the entire target sequence from scratch.  It can do this by first encoding the source sequence with the encoder and then passing the EOS token to the decoder/output_layer to get a probability distribution for the first word.  Then, it can take the most likely token from this probability distribution as the next word.  We can then feed this next word back into the decoder to get the next word after that, until our model generates the EOS token again, telling us the translation is finished.  This is the logic you will implement in the `generate()` function.  For ease of implementation, we won't batch this function.  That is, the argument `source` will be a 1D tensor of size $t_i$ and you will return a 1D translation tensor with maximium length `max_steps`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function \n",
    "def flatten_tensors(context: torch.tensor, target: torch.tensor, padding_idx: int) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    trimmed_contexts = []\n",
    "    trimmed_targets = []\n",
    "    for pair in zip(context, target):\n",
    "        curr_context = pair[0]\n",
    "        curr_target = pair[1]\n",
    "        #ctxt_boolean_mask = ((curr_context == padding_idx).nonzero(as_tuple=True)[0])\n",
    "        targ_boolean_mask = ((curr_target == padding_idx).nonzero(as_tuple=True)[0])\n",
    "        if targ_boolean_mask.numel() > 0:\n",
    "            first_pad_idx_targ = targ_boolean_mask.min(dim=0).values.item()\n",
    "            first_pad_idx_ctxt = first_pad_idx_targ\n",
    "        else:\n",
    "            first_pad_idx_targ = len(curr_target)\n",
    "            first_pad_idx_ctxt = first_pad_idx_targ\n",
    "        cutting_idx = min(first_pad_idx_ctxt, first_pad_idx_targ)\n",
    "        trimmed_contexts.append(curr_context[:cutting_idx, :])\n",
    "        trimmed_targets.append(curr_target[:cutting_idx])\n",
    "    target = torch.cat(trimmed_targets, dim=0)\n",
    "    context = torch.cat(trimmed_contexts, dim=0)\n",
    "    return context, target\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_vocab_size: int, output_vocab_size: int, hidden_size: int, padding_idx: int):\n",
    "        super().__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.encoder = EncoderLSTM(input_vocab_size, hidden_size, padding_idx)\n",
    "        self.decoder = DecoderLSTM(output_vocab_size, hidden_size, padding_idx)\n",
    "        self.linear = nn.Linear(hidden_size, output_vocab_size)\n",
    "        \n",
    "    def forward(self, input_seqs: torch.tensor, output_seqs: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        enc_hidden_cell, enc_final_cell = self.encoder(input_seqs)\n",
    "        hidden_states, dec_final_hidden_state, dec_final_cell = self.decoder(output_seqs, enc_hidden_cell, enc_final_cell)\n",
    "        flatten_hidden_states, flatten_targs = flatten_tensors(hidden_states, output_seqs, self.padding_idx)\n",
    "        out = self.linear(flatten_hidden_states)\n",
    "        log_probs_2 = torch.nn.functional.log_softmax(out, dim=1)\n",
    "        return log_probs_2, flatten_targs\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(self, source: torch.tensor, max_steps: int, eos_idx: int) -> torch.tensor:\n",
    "        print(\"This is source in generate shape \", source.shape)\n",
    "        translation = []\n",
    "        translation.append(eos_idx)\n",
    "        source = torch.unsqueeze(source,0)\n",
    "        enc_hidden_cell, enc_final_cell = self.encoder(source)\n",
    "        blown_eos_idx = torch.unsqueeze(torch.unsqueeze(torch.tensor(eos_idx), 0), 0)\n",
    "        hidden_states, dec_final_hidden_state, dec_final_cell = self.decoder(blown_eos_idx, enc_hidden_cell, enc_final_cell)\n",
    "        flatten_hidden_states, flatten_targs = flatten_tensors(hidden_states, blown_eos_idx, self.padding_idx)\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        out = self.linear(flatten_hidden_states)\n",
    "        for i in range(1, max_steps):\n",
    "            next_idx = torch.argmax(torch.tensor(out))\n",
    "            translation.append(next_idx.item())\n",
    "            if next_idx == eos_idx:\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "                break\n",
    "            blown_next_idx = torch.unsqueeze(torch.unsqueeze(torch.tensor(next_idx), 0), 0)\n",
    "            hidden_states, dec_final_hidden_state, dec_final_cell = self.decoder(blown_next_idx, enc_hidden_cell, enc_final_cell)\n",
    "            flatten_hidden_states, flatten_targs = flatten_tensors(hidden_states, blown_next_idx, self.padding_idx)\n",
    "            out = self.linear(flatten_hidden_states)\n",
    "        return translation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "This is source in generate shape  torch.Size([4])\n",
      "> <ipython-input-78-d63cedabcc8d>(54)generate()\n",
      "-> out = self.linear(flatten_hidden_states)\n",
      "(Pdb) \n",
      "(Pdb) exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-e977d479f24b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_1D_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_1D_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is translation \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-d63cedabcc8d>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, source, max_steps, eos_idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mnext_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-d63cedabcc8d>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, source, max_steps, eos_idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mnext_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq2seq = Seq2Seq(train_dl.dataset.source_vocab_size, train_dl.dataset.target_vocab_size, 100, 4)\n",
    "batch = next(iter(train_dl))\n",
    "log_probs, targs = seq2seq(batch[0], batch[1])\n",
    "#print(batch[0])\n",
    "test_1D_tensor = torch.tensor([1,34,2,4])\n",
    "print(test_1D_tensor.shape)\n",
    "translation = seq2seq.generate(torch.tensor([1,34,2,4]), 40, 0)\n",
    "print(\"This is translation \", translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.12 Train the Seq2Seq model [8 points]</b>\n",
    "    \n",
    "Now it's time to train the model! Implement the `train()` function below to\n",
    "train your model. We've provided code that will call your `train()` to produce\n",
    "a new model, and optionally plot the losses. We recommend training for at least\n",
    "8 epochs to achieve desirable results.\n",
    "\n",
    "Please use your:\n",
    "* **train split** to train the model\n",
    "* **validation split** to compute losses\n",
    "* **test split** to evaluate the performance of your model using BLEU (in the next step)\n",
    "\n",
    "For convenience, you may use the helper function below to compute the loss\n",
    "of your model on your validation set.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "# you do not need to edit this cell\n",
    "def compute_test_loss(model, val_dl, loss_fn):\n",
    "    all_preds, all_targs = [], []\n",
    "    for i, (input_seqs, output_seqs) in enumerate(val_dl):\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        preds, targs = model(input_seqs, output_seqs)\n",
    "        all_preds.append(preds)\n",
    "        all_targs.append(targs)\n",
    "    preds = torch.cat(all_preds, dim=0)\n",
    "    targs = torch.cat(all_targs, dim=0)\n",
    "    return loss_fn(preds, targs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_ds: Dataset, train_dl: DataLoader, val_ds: Dataset, val_dl: DataLoader):\n",
    "    model = None\n",
    "    losses = {\n",
    "        'train': [], # keep track of your losses in these lists\n",
    "        'val': []\n",
    "    }\n",
    "    \n",
    "    model = Seq2Seq(train_dl.dataset.source_vocab_size, train_dl.dataset.target_vocab_size, 100, PADDING_IDX)\n",
    "    \n",
    "    loss_function = NLLLoss()\n",
    "    optimizer =  Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    epochs = 15\n",
    "    for epoch in range(epochs):\n",
    "        train_running_loss = 0.0\n",
    "        val_running_loss = 0.0\n",
    "        for i, (train_batch, val_batch) in enumerate(zip(train_dl, val_dl)):\n",
    "            \n",
    "            train_preds, train_targets = model(train_batch[0], train_batch[1])\n",
    "            train_loss = loss_function(train_preds, train_targets)\n",
    "            #train_loss = compute_test_loss(model, train_dl, loss_function)\n",
    "            \n",
    "            val_preds, val_targets = model(val_batch[0], val_batch[1])\n",
    "            val_loss = loss_function(val_preds, val_targets)\n",
    "            #val_loss = compute_test_loss(model, val_dl, loss_function)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_running_loss += train_loss.item()\n",
    "            val_running_loss += val_loss.item()\n",
    "            if i % 2 == 0:\n",
    "                print('Epoch %d, iteration %5d, training_loss %.3f, validation_loss %.3f' %\n",
    "                      (epoch + 1, i+1, train_running_loss / 2, val_running_loss / 2))\n",
    "        \n",
    "        losses['train'].append(train_running_loss)\n",
    "        losses['val'].append(val_running_loss)\n",
    "    print(\"Finished training!\")\n",
    "    \n",
    "    return model, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the cell below to train your model. For reference, it takes about 1 minute to train on our machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 1, iteration     1, training_loss 3.548, validation_loss 3.550\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 1, iteration     3, training_loss 10.646, validation_loss 10.642\n",
      "the input_seqs shape in encoder  torch.Size([32, 41])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 1, iteration     5, training_loss 17.735, validation_loss 17.727\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 1, iteration     7, training_loss 24.807, validation_loss 24.800\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 1, iteration     9, training_loss 31.876, validation_loss 31.864\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 1, iteration    11, training_loss 38.927, validation_loss 38.915\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 1, iteration    13, training_loss 45.974, validation_loss 45.958\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 1, iteration    15, training_loss 53.004, validation_loss 52.985\n",
      "the input_seqs shape in encoder  torch.Size([32, 42])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 1, iteration    17, training_loss 60.027, validation_loss 60.008\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 1, iteration    19, training_loss 67.038, validation_loss 67.019\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 1, iteration    21, training_loss 74.050, validation_loss 74.024\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([28, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 2, iteration     1, training_loss 3.499, validation_loss 3.491\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "Epoch 2, iteration     3, training_loss 10.479, validation_loss 10.476\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 2, iteration     5, training_loss 17.457, validation_loss 17.443\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 2, iteration     7, training_loss 24.420, validation_loss 24.392\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 49])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 2, iteration     9, training_loss 31.368, validation_loss 31.335\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 2, iteration    11, training_loss 38.318, validation_loss 38.277\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 2, iteration    13, training_loss 45.251, validation_loss 45.205\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 2, iteration    15, training_loss 52.177, validation_loss 52.119\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 2, iteration    17, training_loss 59.080, validation_loss 59.025\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 2, iteration    19, training_loss 65.974, validation_loss 65.918\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 2, iteration    21, training_loss 72.859, validation_loss 72.800\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([28, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 3, iteration     1, training_loss 3.440, validation_loss 3.428\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 3, iteration     3, training_loss 10.309, validation_loss 10.278\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 3, iteration     5, training_loss 17.156, validation_loss 17.107\n",
      "the input_seqs shape in encoder  torch.Size([32, 45])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 49])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "Epoch 3, iteration     7, training_loss 24.008, validation_loss 23.938\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 3, iteration     9, training_loss 30.842, validation_loss 30.757\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 3, iteration    11, training_loss 37.653, validation_loss 37.552\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 3, iteration    13, training_loss 44.466, validation_loss 44.331\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, iteration    15, training_loss 51.238, validation_loss 51.106\n",
      "the input_seqs shape in encoder  torch.Size([32, 39])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 3, iteration    17, training_loss 58.011, validation_loss 57.868\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 3, iteration    19, training_loss 64.765, validation_loss 64.621\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 3, iteration    21, training_loss 71.490, validation_loss 71.364\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([28, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 19])\n",
      "Epoch 4, iteration     1, training_loss 3.368, validation_loss 3.345\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 4, iteration     3, training_loss 10.097, validation_loss 10.045\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 4, iteration     5, training_loss 16.777, validation_loss 16.738\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 4, iteration     7, training_loss 23.441, validation_loss 23.390\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 4, iteration     9, training_loss 30.105, validation_loss 30.015\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 4, iteration    11, training_loss 36.749, validation_loss 36.652\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 4, iteration    13, training_loss 43.367, validation_loss 43.255\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 4, iteration    15, training_loss 49.984, validation_loss 49.855\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 4, iteration    17, training_loss 56.551, validation_loss 56.434\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 4, iteration    19, training_loss 63.090, validation_loss 62.970\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "Epoch 4, iteration    21, training_loss 69.638, validation_loss 69.445\n",
      "the input_seqs shape in encoder  torch.Size([32, 41])\n",
      "the input_seqs shape in encoder  torch.Size([28, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "Epoch 5, iteration     1, training_loss 3.238, validation_loss 3.248\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 5, iteration     3, training_loss 9.722, validation_loss 9.727\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 5, iteration     5, training_loss 16.178, validation_loss 16.175\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 5, iteration     7, training_loss 22.645, validation_loss 22.576\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "Epoch 5, iteration     9, training_loss 29.062, validation_loss 28.945\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 5, iteration    11, training_loss 35.469, validation_loss 35.303\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "Epoch 5, iteration    13, training_loss 41.824, validation_loss 41.639\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 5, iteration    15, training_loss 48.172, validation_loss 47.915\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 5, iteration    17, training_loss 54.503, validation_loss 54.203\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 19])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 5, iteration    19, training_loss 60.734, validation_loss 60.381\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 5, iteration    21, training_loss 66.944, validation_loss 66.553\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([28, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 6, iteration     1, training_loss 3.117, validation_loss 3.066\n",
      "the input_seqs shape in encoder  torch.Size([32, 35])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 6, iteration     3, training_loss 9.259, validation_loss 9.170\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 6, iteration     5, training_loss 15.393, validation_loss 15.183\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 6, iteration     7, training_loss 21.426, validation_loss 21.124\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 42])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, iteration     9, training_loss 27.414, validation_loss 27.067\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 6, iteration    11, training_loss 33.370, validation_loss 32.943\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 6, iteration    13, training_loss 39.235, validation_loss 38.809\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 6, iteration    15, training_loss 45.090, validation_loss 44.656\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 6, iteration    17, training_loss 50.819, validation_loss 50.407\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 33])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 6, iteration    19, training_loss 56.557, validation_loss 56.072\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 6, iteration    21, training_loss 62.204, validation_loss 61.692\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([28, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 7, iteration     1, training_loss 2.811, validation_loss 2.755\n",
      "the input_seqs shape in encoder  torch.Size([32, 45])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 7, iteration     3, training_loss 8.318, validation_loss 8.241\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 7, iteration     5, training_loss 13.820, validation_loss 13.687\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 49])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 7, iteration     7, training_loss 19.239, validation_loss 19.037\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 7, iteration     9, training_loss 24.654, validation_loss 24.341\n",
      "the input_seqs shape in encoder  torch.Size([32, 42])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 7, iteration    11, training_loss 29.973, validation_loss 29.579\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 7, iteration    13, training_loss 35.212, validation_loss 34.762\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 7, iteration    15, training_loss 40.392, validation_loss 39.935\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 41])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 7, iteration    17, training_loss 45.585, validation_loss 45.001\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 7, iteration    19, training_loss 50.660, validation_loss 50.087\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "Epoch 7, iteration    21, training_loss 55.748, validation_loss 55.203\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([28, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 8, iteration     1, training_loss 2.472, validation_loss 2.506\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 8, iteration     3, training_loss 7.514, validation_loss 7.392\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 8, iteration     5, training_loss 12.465, validation_loss 12.200\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 8, iteration     7, training_loss 17.352, validation_loss 17.025\n",
      "the input_seqs shape in encoder  torch.Size([32, 55])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 8, iteration     9, training_loss 22.242, validation_loss 21.848\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 8, iteration    11, training_loss 26.972, validation_loss 26.709\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 8, iteration    13, training_loss 31.734, validation_loss 31.434\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 8, iteration    15, training_loss 36.482, validation_loss 36.051\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "Epoch 8, iteration    17, training_loss 41.184, validation_loss 40.786\n",
      "the input_seqs shape in encoder  torch.Size([32, 44])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 8, iteration    19, training_loss 45.932, validation_loss 45.422\n",
      "the input_seqs shape in encoder  torch.Size([32, 19])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "Epoch 8, iteration    21, training_loss 50.611, validation_loss 50.048\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([28, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 9, iteration     1, training_loss 2.347, validation_loss 2.246\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, iteration     3, training_loss 6.937, validation_loss 6.783\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 9, iteration     5, training_loss 11.460, validation_loss 11.308\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 9, iteration     7, training_loss 16.062, validation_loss 15.780\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 9, iteration     9, training_loss 20.608, validation_loss 20.243\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 9, iteration    11, training_loss 25.039, validation_loss 24.616\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "Epoch 9, iteration    13, training_loss 29.522, validation_loss 29.093\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 9, iteration    15, training_loss 34.074, validation_loss 33.491\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "Epoch 9, iteration    17, training_loss 38.492, validation_loss 37.880\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 9, iteration    19, training_loss 42.854, validation_loss 42.119\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "Epoch 9, iteration    21, training_loss 47.213, validation_loss 46.459\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([28, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 10, iteration     1, training_loss 2.192, validation_loss 2.151\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 10, iteration     3, training_loss 6.525, validation_loss 6.357\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "Epoch 10, iteration     5, training_loss 10.795, validation_loss 10.715\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 10, iteration     7, training_loss 15.065, validation_loss 14.835\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 10, iteration     9, training_loss 19.267, validation_loss 19.084\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 10, iteration    11, training_loss 23.581, validation_loss 23.276\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 10, iteration    13, training_loss 27.905, validation_loss 27.410\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 41])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 10, iteration    15, training_loss 32.268, validation_loss 31.646\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 44])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 10, iteration    17, training_loss 36.489, validation_loss 35.762\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "Epoch 10, iteration    19, training_loss 40.666, validation_loss 39.967\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 10, iteration    21, training_loss 44.846, validation_loss 43.951\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([28, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 11, iteration     1, training_loss 2.165, validation_loss 2.046\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 11, iteration     3, training_loss 6.302, validation_loss 6.040\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 11, iteration     5, training_loss 10.470, validation_loss 10.031\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 11, iteration     7, training_loss 14.512, validation_loss 14.012\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 11, iteration     9, training_loss 18.552, validation_loss 17.893\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 11, iteration    11, training_loss 22.620, validation_loss 21.880\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 11, iteration    13, training_loss 26.679, validation_loss 25.934\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 11, iteration    15, training_loss 30.830, validation_loss 29.963\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 11, iteration    17, training_loss 35.007, validation_loss 33.953\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, iteration    19, training_loss 39.038, validation_loss 38.020\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 11, iteration    21, training_loss 43.046, validation_loss 42.041\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([28, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 12, iteration     1, training_loss 2.024, validation_loss 1.978\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 12, iteration     3, training_loss 6.150, validation_loss 5.892\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "Epoch 12, iteration     5, training_loss 10.144, validation_loss 9.837\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 12, iteration     7, training_loss 14.112, validation_loss 13.674\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 12, iteration     9, training_loss 18.208, validation_loss 17.618\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 12, iteration    11, training_loss 22.196, validation_loss 21.447\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 12, iteration    13, training_loss 26.154, validation_loss 25.251\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 12, iteration    15, training_loss 30.122, validation_loss 29.061\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 12, iteration    17, training_loss 34.083, validation_loss 32.928\n",
      "the input_seqs shape in encoder  torch.Size([32, 18])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 12, iteration    19, training_loss 38.001, validation_loss 36.727\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 55])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 12, iteration    21, training_loss 41.946, validation_loss 40.609\n",
      "the input_seqs shape in encoder  torch.Size([32, 41])\n",
      "the input_seqs shape in encoder  torch.Size([28, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 13, iteration     1, training_loss 1.939, validation_loss 1.861\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 13, iteration     3, training_loss 5.886, validation_loss 5.677\n",
      "the input_seqs shape in encoder  torch.Size([32, 19])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 13, iteration     5, training_loss 9.711, validation_loss 9.581\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 13, iteration     7, training_loss 13.638, validation_loss 13.238\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "Epoch 13, iteration     9, training_loss 17.578, validation_loss 16.999\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 13, iteration    11, training_loss 21.400, validation_loss 20.797\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 13, iteration    13, training_loss 25.257, validation_loss 24.497\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "Epoch 13, iteration    15, training_loss 28.891, validation_loss 28.233\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "Epoch 13, iteration    17, training_loss 32.823, validation_loss 31.921\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 13, iteration    19, training_loss 36.634, validation_loss 35.752\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 13, iteration    21, training_loss 40.402, validation_loss 39.415\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([28, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 39])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 14, iteration     1, training_loss 1.909, validation_loss 1.843\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 14, iteration     3, training_loss 5.765, validation_loss 5.526\n",
      "the input_seqs shape in encoder  torch.Size([32, 49])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 14, iteration     5, training_loss 9.482, validation_loss 9.252\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "Epoch 14, iteration     7, training_loss 13.195, validation_loss 12.925\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 14, iteration     9, training_loss 16.951, validation_loss 16.538\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, iteration    11, training_loss 20.666, validation_loss 20.221\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 14, iteration    13, training_loss 24.324, validation_loss 23.831\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 14, iteration    15, training_loss 28.022, validation_loss 27.494\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 14, iteration    17, training_loss 31.734, validation_loss 31.061\n",
      "the input_seqs shape in encoder  torch.Size([32, 42])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "Epoch 14, iteration    19, training_loss 35.334, validation_loss 34.739\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 14, iteration    21, training_loss 39.115, validation_loss 38.255\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([28, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 32])\n",
      "Epoch 15, iteration     1, training_loss 1.843, validation_loss 1.830\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 35])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 15, iteration     3, training_loss 5.539, validation_loss 5.309\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 15, iteration     5, training_loss 9.146, validation_loss 8.931\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 15, iteration     7, training_loss 12.704, validation_loss 12.473\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 22])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "Epoch 15, iteration     9, training_loss 16.292, validation_loss 16.113\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "Epoch 15, iteration    11, training_loss 19.922, validation_loss 19.825\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "the input_seqs shape in encoder  torch.Size([32, 28])\n",
      "Epoch 15, iteration    13, training_loss 23.510, validation_loss 23.302\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([32, 23])\n",
      "the input_seqs shape in encoder  torch.Size([32, 30])\n",
      "Epoch 15, iteration    15, training_loss 27.115, validation_loss 26.751\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 29])\n",
      "the input_seqs shape in encoder  torch.Size([32, 31])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 15, iteration    17, training_loss 30.814, validation_loss 30.214\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "the input_seqs shape in encoder  torch.Size([32, 25])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 27])\n",
      "Epoch 15, iteration    19, training_loss 34.469, validation_loss 33.680\n",
      "the input_seqs shape in encoder  torch.Size([32, 20])\n",
      "the input_seqs shape in encoder  torch.Size([32, 21])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "the input_seqs shape in encoder  torch.Size([32, 24])\n",
      "Epoch 15, iteration    21, training_loss 37.968, validation_loss 37.350\n",
      "the input_seqs shape in encoder  torch.Size([32, 26])\n",
      "the input_seqs shape in encoder  torch.Size([28, 26])\n",
      "Finished training!\n",
      "CPU times: user 17.1 s, sys: 2.64 s, total: 19.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcf490aafd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd1gVZ/rG8e9DUcCGIlhAROy9oTGWqIkmdk03fdPMpm82+0vfTXZTN3XTq4kajcYkakxiiyZ21GDHigoKqBQpooK09/fHHBOigEibcw7P57q85MycM+fGcjPnnXdmxBiDUkop9+JhdwCllFKVT8tdKaXckJa7Ukq5IS13pZRyQ1ruSinlhrTclVLKDWm5qwoREU8ROSEioU6QZbWI/KWqty0it4nIwqrIISLhInKifCmV+oOWew3jKOIzvwpFJLvI45sudHvGmAJjTF1jzKGqyFsZROQWEdlfzPJaIpIqIiMuZHvGmKnGmJGVlC1BRIYU2fYBY0zdytj2We/jJSJGRMIqe9vKOWm51zCOIq7rKJBDwNgiy2ac/XwR8ar+lJXuOyBQRAaetXwUkAv8XP2RlKpaWu7qT0TkBRH5WkRmikgWcLOIXCwi60QkQ0SOiMg7IuLteP6f9ghFZLpj/UIRyRKRSBFpVcJ7eYjItyJy1LHt5SLSscj6UrclIiNEZI+IZIrI24AU9z7GmFPAt8CtZ626FZhujCkQkQARWSAiKSKSLiI/iEhwCbnvEpHlZckhIm1F5FcROeb4lPCliDRwrJsJNAcWOj45/V1E2oiIKfL6EBH5UUTSRCRGRO446+9qpuPPKUtEokWkV3GZS+P4e/iXiBwUkWQRmSIi9R3r/ETkK0f+DBHZICKNHevuFJE4x3sfEJGJF/requpouaviXAl8BTQAvgbygYeBxsAAYARwTymvvxH4J9AI69PB86U890egLdAUiAa+LMu2RCQIq7CfcORKAC4q5X2mAteJiI/j9Q2B0cA0x3oP4FMgFGgJ5AFvl7I9yphDgBeAZkAnINzx/WCMuQE4DIx0fHJ6s5i3+BqIxfohcD3wqogMLrJ+AtafmT+wEHjnfJmLcRdwMzAEaA005I/v/XbADwgBAoD7gBxH+b8JDDfG1MP6d7GtHO+tqoiWuyrOamPMD8aYQmNMtjHmN2PMemNMvjHmAPAJMLiU139rjIkyxuQBM4AexT3Jsf0pxpgsY0wO8BzQW0TqlGFbY4Atxpi5jnVvACmlZFoJpAHjHI8nAtHGmGhHlhTHtrKNMceBl87zPZ5Rag5jzF5jzDJjTK4xJhl4q4zbxfEppS/whDEmxxizCfgCuKXI01YYYxYbYwqwSr7YP+vzuAl43RgTa4zJAp4CbhQRD6wfco2BNo7jK1HGmDMHfA3QRUR8jDFHjDE7y/HeqopouavixBd9ICIdROQnx/DJceA/WP/hS3K0yNengGIPEIo10+ZVx0f648A+x6qi2y5pW82L5jTGFGLtNRfLWFfI+5I/hmZuwdqbP5Oljoh8JiKHHFl+ofTv8YxSc4hIUxGZLSKJju1OKeN2z2w71Rhzssiyg0DR4aKz/3yK/mAsq+aO7RZ9j1pAIFbepcCZ7+EVEfFy/AC8AbgfOOoYOmpXjvdWVUTLXRXn7EuFfow1ZNLGGFMf+BcljG9foFuxDmpeijUE1MaxvCzbPgK0OPPAsZcZcp7XTAMuF5H+QAQws8i6x4BWQF/H93hpWb6BMuT4L3Aa6OrY7l/48/dX2mVZDwONz/okEwokljFbWR3GGooq+h65QIrjE8dzxpiOwECsIbubAIwxC40xw7CGnPZh/TtRTkLLXZVFPSATOOk44FnaePuFbvc0cAxrXPfFC3jtj0APERnvmNHzCNaeZomMMfuB9VjHExYaY4oO49TD2vNNF5EArB9glZGjHnASyBSRFsA/znp9EtY4fHF5Y4Eo4CURqS0iPbDGwM+Z1XQBaouIT5Ffnlg/5P4uImEiUg/r72GmMaZQRC4VkS6OH1rHsYZpCkSkmYiMFRE/rB8EJ4GCCuRSlUzLXZXFo8BtQBbW3tnXlbTdL7D2Gg8DO4C1ZX2hMSYJ6wDja1g/HEKxivt8pmLtpU47a/mbWJ8ejjlylHiS0gXmeBZr3DwTmI81LbOol4B/O2ai/K2Yt7ge64DzUawDt08ZY34tS7YS7Aayi/y6BetA8tfAKuAA1t/zw47nNwfmYBX7DqwhmpmAJ/B/WJ9cjgH9gQcqkEtVMtGbdSillPvRPXellHJDWu5KKeWGtNyVUsoNabkrpZQbcoqLQjVu3NiEhYXZHUMppVzKxo0bU40xxU4BdopyDwsLIyoqyu4YSinlUkTkYEnrdFhGKaXckJa7Ukq5IS13pZRyQ04x5q6UUuWRl5dHQkICOTk5dkepUj4+PoSEhODt7V3m12i5K6VcVkJCAvXq1SMsLAyRyrhQqfMxxnDs2DESEhJo1arYm5oVS4dllFIuKycnh4CAALctdgARISAg4II/nWi5K6VcmjsX+xnl+R5dutwTDsWx8Ms32LZnHwWFenVLpZQ6w6XLPWXzD4zc/x+6fBXB9v9cxI8fPMaK1as5np1rdzSlVA2QkZHBBx98cMGvGzVqFBkZGVWQ6A9OcT33iIgIU64zVI0hK24TiRvm4Be7hNCcvQAcNE2IrjsA2o+k40WXE97Ev5ITK6Wcwa5du+jYsaNt7x8XF8eYMWOIjo7+0/KCggI8PT0r9b2K+15FZKMxJqK457v2bBkR6rXqTYdWvYEXyU9PIGHDPAp2/cTwjB+otWkOGRvr8LNXbzJDhxPSZyy92rWklpdLf2BRSjmJJ554gv3799OjRw+8vb2pW7cuzZo1Y8uWLezcuZMJEyYQHx9PTk4ODz/8MJMmTQL+uOTKiRMnGDlyJAMHDmTt2rUEBwfz/fff4+vrW+Fsrr3nXprTJ0jdtoiMLfMJOrKC+oUZ5BlPouhEfOBg6nQbQ9+evQisV7ty31cpVW2K7s3++4cd7Dx8vFK336l5fZ4d27nE9UX33JcvX87o0aOJjo7+fcpiWloajRo1Ijs7mz59+rBixQoCAgL+VO5t2rQhKiqKHj16cN111zFu3DhuvvnmUr/XM9x3z700tevSuM81NO5zDRQWkBO3nqMb5hAe+zMXp74Hv7zHrqUtWF63P4VtR9K5zxA6B/vXiCPvSqmq0bdv3z/NRX/nnXeYO3cuAPHx8cTExBAQEPCn17Rq1YoePXoA0Lt3b+Li4ioli/uWe1EenviE9ycsvD8A5th+kn6bR91dC7gq8xs8t35NypYGzPeMILPFMJr3Gkn/ji3wq1Uz/niUcgel7WFXlzp16vz+9fLly1m6dCmRkZH4+fkxZMiQYueq1679x+iBp6cn2dnZlZKlRraXBLSm6YhHYcSjkJ1OVvRCcjbP5/KjK/E9uIycuH+xmu4cbDaS4IuuYnDnlvjWqtyDI0op11evXj2ysrKKXZeZmUnDhg3x8/Nj9+7drFu3rlqz1chy/xPfhtTrcyP1+twI+bnkxa4hLWoufQ4sYNjR5zkx7zUWz+tLYouxtL1oFJd0aIaPtxa9UgoCAgIYMGAAXbp0wdfXlyZNmvy+bsSIEXz00Ud069aN9u3b069fv2rN5r4HVCuqsICC2NWkrp1O/dgF+BaeIMU0YDH9SWk1nm59hzKoXZDOvFHKRnZPhaxOekC1snh44tl6ME1aD4b80+TvWQzrpnN9wjK84xYSe6AJn3sMIqPNlfTr05cBbRrj7alFr5RyDuctdxH5HBgDJBtjujiWPQfcDaQ4nvaUMWaBY92TwJ1AAfCQMWZxFeSuXl618eo8jsDO4yAnk/zo+TT47SsmJX2Hx75v2bo3nLc9B5HbYQKX9OpGv/BGeGnRK6VsVJY99ynAe8C0s5a/ZYx5vegCEekETAQ6A82BpSLSzhhTUAlZnYNPA7wibqFRxC1w/Ah5274lbONM/pE+lYKd01gb3ZkXvS/Bo/M4hvdsR5+wRnh66PRKpVT1Om+5G2NWikhYGbc3HphljDkNxIrIPqAvEFnuhM6sfjO8Bz5Ig4EPQspezNbZ9Nr8NYNOfsDpbZ+ydEtPnqg1hPpdRzGyR0t6hTbEQ4teKVUNKjLm/oCI3ApEAY8aY9KBYKDofJ8Ex7JziMgkYBJAaGhoBWI4icB2eA17Bq/LnobEjXhsnsWw6DmMPr2BzM3vsyCqL1/4DiE84gpuGxBO47p6ZqxSquqUd2D4Q6A10AM4ArzhWF7cbmmx03GMMZ8YYyKMMRGBgYHljOGERCAkAu+xr1P7sb1w83f4dRnNNbXX837es1y55kre+O+/eGbOZmJTT9qdVinlpsq1526MSTrztYh8CvzoeJgAtCjy1BDgcLnTuTpPL2gzDO82wyD3FOz+ieCV/+Pl1I9I3PotH28cQ3q767l9aEd6hTa0O61SqorVrVuXEydOVMt7lWvPXUSaFXl4JXDmepfzgYkiUltEWgFtgQ0Vi+gmavlBt2upff9quOlbgoLD+Y/3FJ6Nnciij5/k1g+WsXRnEoV60xGlVCUoy1TImcAQoLGIJADPAkNEpAfWkEsccA+AMWaHiMwGdgL5wP1uNVOmMohA2+HW3vzBNTRc/hpPxc3kePIPTJ5xBe83uoobLunO+J7Nqe2lZ8Iq5cwef/xxWrZsyX333QfAc889h4iwcuVK0tPTycvL44UXXmD8+PHVnk3PUHUGCRspXPk6HnsXcApfpuUP43uf8Ywd2JObLmpJA19vuxMq5ZT+dNbmwifg6PbKfYOmXWHkKyWu3rx5M3/7299YsWIFAJ06dWLRokX4+/tTv359UlNT6devHzExMYhIhYZl9AxVVxTSG48bZ0LSDnxXvck9O+ZwR/5iZi4dzNW/TGBI317cMbAVzf0rfgF/pVTl6dmzJ8nJyRw+fJiUlBQaNmxIs2bNeOSRR1i5ciUeHh4kJiaSlJRE06ZNqzWblrszadIZuWYyDH2KWqvf4pats7i58BfmrB/IrWvH0617b+6+JJyOzerbnVQp51PKHnZVuuaaa/j22285evQoEydOZMaMGaSkpLBx40a8vb0JCwsr9lK/VU3L3RkFtIbx7+Ex5AlY8w5Xb5zK1QWrWLTjIh7ZMp4mbXtzzyXhXNw6QG8uopTNJk6cyN13301qaiorVqxg9uzZBAUF4e3tza+//srBgwdtyaXl7swahMCoV/G45B8Q+T4jf/uMUR6RrIyP4LXJY8lvHsGkS8IZ2aWpXstGKZt07tyZrKwsgoODadasGTfddBNjx44lIiKCHj160KFDB1ty6QFVV5KdDus/waz/EMlOZ5NnN17LHktCg948M6YzV3Su3jE9peyml/wt+YCq7u65Et+GMORx5G/RMPx5evomMbPWi3yc+xTvTv+Gv83aTMapXLtTKqWcgJa7K6pdFwY8hDy8DUa/QUe/TL73eY4m0Z9yxZvL+WV30vm3oZRya1rurszbB/rchdy7Fs/2I3jSawbv8TKPTVnG/32zleM5eXYnVKrKOcPQclUrz/eo5e4O/BrB9dNh9BtEmB0sr/c0SZsXcsVbK1m5N+X8r1fKRfn4+HDs2DG3LnhjDMeOHcPHx+eCXqcHVN1N0g749g5I2c2sWlfxz+MTuPaicJ4a1ZG6tXVylHIveXl5JCQk2DKPvDr5+PgQEhKCt/efz1Yv7YCqlrs7yj0Fi5+CjV+QWKcTN6bdTYF/GK9e043+rRvbnU4pVUl0tkxNU8sPxv4Prp1KcEEiv9T9J1cUruTGT9fz3PwdnMrNtzuhUqqKabm7s84T4K+r8WzahX+efou5zacze+1uRr29iqi4NLvTKaWqkJa7u/MPhb/8BJc8Rs+0hWwMep5W+fu59uNIXvxpJzl5ekVmpdyRlntN4OkFlz4Nt/2ArznN5/lP8l6rdXy66gCj31nF5kPpdidUSlUyLfeapNUg+OtqpM0wRh9+h03hn1L7dBpXf7iW/y7azel83YtXyl1oudc0dQJg4lcw8jUaJUXyY60neKJ9Eh8u38/Yd1ezPSHT7oRKqUpw3nIXkc9FJFlEootZ9w8RMSLS2PFYROQdEdknIttEpFdVhFYVJAIXTYK7l+Hh04BJcX9nec8VZJ3MZsIHa3jz573k5hfanVIpVQFl2XOfAow4e6GItACGA4eKLB6JdVPstsAk4MOKR1RVpmlXmLQcet1C2K6PWRX0Gn/pKLyzLIYJ769h15HjdidUSpXTecvdGLMSKG7e3FvAY1g3yT5jPDDNWNYB/iLSrFKSqqpRqw6Mexeu+QKvYzH8M2ES3w9JIjkrh/HvrWH9gWN2J1RKlUO5xtxFZByQaIzZetaqYCC+yOMEx7LitjFJRKJEJColRa9/YrsuV8FfV0Fge7qve4TVHefSpqHw1+kbOXjspN3plFIX6ILLXUT8gKeBfxW3uphlxV7fwBjziTEmwhgTERgYeKExVFVo2BJuXwiDHsVn+1fM834a/8IM7pwapVeYVMrFlGfPvTXQCtgqInFACLBJRJpi7am3KPLcEOBwRUOqauTpDZf9C26dR62sROYHfsjh1Azun7GJ/AI9yKqUq7jgcjfGbDfGBBljwowxYViF3ssYcxSYD9zqmDXTD8g0xhyp3MiqWoQPgSs/pF7KJn5s9R2rYlJ4/seddqdSSpVRWaZCzgQigfYikiAid5by9AXAAWAf8ClwX6WkVPbofCUMfpzwxO/5pM16pkYe5MvIOLtTKaXK4LwX+DbG3HCe9WFFvjbA/RWPpZzG4CcgaQfD97zHQy2b8dwPQljjOgxqq8dJlHJmeoaqKp2HB1z5MRLUiUcyXmZoQAb3zdjEvuQTdidTSpVCy12dX+26cMNMxLMWH3q+TmPPU9w59TfST+banUwpVQItd1U2/qFw/Zd4Hz/EvCaTSc48yT3TN+plCpRyUlruquxa9ofRb9Dg8Cp+aL+EDbFpPDNvu1vfnFgpV6V3TFYXpvdtkLyTNus/4uPOLbgnCtoE1WXSJa3tTqaUKkL33NWFu/xFCB/C5bH/5YE2qby8cDc/70yyO5VSqggtd3XhPL3gmi8Q/xY8mvY8Q5ue5uFZm9l5WK8iqZSz0HJX5ePXCG6YhRSc5mOvNwiqXcBdU38jOSvH7mRKKbTcVUUEtoerJ+OdsoPvQ6aTceo0k6Zt1JtuK+UEtNxVxbS7HIb/hwaxC/i+61q2xGfw2LfbdAaNUjbTclcV1/9B6DaRtjvf5cNeCczfeph3lu2zO5VSNZqWu6o4ERj7NoT0YUTMczzQKZu3lu7lx216tWel7KLlriqHtw9cPx3x8efR1Ge5tIXw6OytbInPsDuZUjWSlruqPPWawsQZyKlUPq71P5rV9eDuaVEczsi2O5lSNY6Wu6pcwb1g/Pt4J67n+7A5ZOfmc9fUKE6ezrc7mVI1ipa7qnxdr4FBj9Jg9yzm9N7O7qPHeeTrLRQW6gwapaqLlruqGkOfgfajabf5JT7ol8mSnUm8uniP3amUqjHKcpu9z0UkWUSiiyx7XkS2icgWEVkiIs0dy0VE3hGRfY71vaoyvHJiHh5w1ccQ2IErdj3Bgz2Ej1bs55uoeLuTKVUjlGXPfQow4qxlrxljuhljegA/Av9yLB8JtHX8mgR8WEk5lSuqXc+6yYd48veUZxkW7sNTc7ezITbN7mRKub3zlrsxZiWQdtayoleIqgOcGUwdD0wzlnWAv4g0q6ywygU1DIPrpiHpB/iw9vu09K/NfTM2kZmdZ3cypdxaucfcReRFEYkHbuKPPfdgoOjn7gTHsuJeP0lEokQkKiUlpbwxlCtoNQhGvop37DJmtl5M2snTvLpot92plHJr5S53Y8zTxpgWwAzgAcdiKe6pJbz+E2NMhDEmIjAwsLwxlKvocyf0uYvAbR/zZvudfLXhEJsOpdudSim3VRmzZb4CrnZ8nQC0KLIuBNBz0JVlxCsQNojxiW/Sve5xnpqznbwCvQerUlWhXOUuIm2LPBwHnPmMPR+41TFrph+QaYw5UsGMyl14esOEDxGETwJmsvvocb5YE2t3KqXcUlmmQs4EIoH2IpIgIncCr4hItIhsAy4HHnY8fQFwANgHfArcVzWxlcvybwFDnyLo6AoeC93DWz/HkJB+yu5USrkdcYbrbkdERJioqCi7Y6jqUpAPnw6hICuZfsdfoXubFnx6awQixR2yUUqVREQ2GmMiilunZ6iq6ufpBWPfxvNkMl+0XMTSXcks3qE32FaqMmm5K3sE94a+k+icMJtxjY/w3PwdnNCLiylVabTclX0ufQap15RXak0mNeskbyzRa88oVVm03JV9fOrDyFfxS9vJO63WM3VtHNGJmXanUsotaLkre3UcC+1GMjLlczr5ZfLU3O0U6KWBlaowLXdlLxEY9SoCfBb4NdsSMvgyMs7mUEq5Pi13ZT//UBj6FE2PLueR4D28vmQvRzNz7E6llEvTclfO4aJ7oUlX7s/5hFoFJ/jPjzvsTqSUS9NyV87BMffd62QSX4QuZsH2o/y6O9nuVEq5LC135TxCekPfu+l2eDajA47wzLxoTuXq3HelykPLXTkXx9z3V2tN5mjGCd5eFmN3IqVckpa7ci4+DWDkf6mTvpP/ha1j8qpYdh89fv7XKaX+RMtdOZ+O46DdCMYc+4J2Phk8NWc7hTr3XakLouWunI8IjHrt97nvmw6lM+u3+PO+TCn1By135Zwcc9+bJy3nwWa7eWXhLlKyTtudSimXoeWunJdj7vvDuZ/imZfFiz/ttDuRUi6jLHdi+lxEkkUkusiy10Rkt4hsE5G5IuJfZN2TIrJPRPaIyBVVFVzVAEXmvk8OXcK8LYdZHZNqdyqlXEJZ9tynACPOWvYz0MUY0w3YCzwJICKdgIlAZ8drPhARz0pLq2qekN7Q5y56HpnNFf6JPDNvOzl5BXanUsrpnbfcjTErgbSzli0xxpw5u2QdEOL4ejwwyxhz2hgTi3Uv1b6VmFfVRJf9E6nbhDd8pxB/LIsPft1ndyKlnF5ljLnfASx0fB0MFJ3WkOBYdg4RmSQiUSISlZKSUgkxlNtyzH2vm76D10PX8eGK/exLPmF3KqWcWoXKXUSeBvKBGWcWFfO0YicoG2M+McZEGGMiAgMDKxJD1QSdxkPbK5iQ/gWtvNN5eu52nOHm7ko5q3KXu4jcBowBbjJ//C9LAFoUeVoIcLj88ZRyKDL3/fOg2ayPPcZ3mxLtTqWU0ypXuYvICOBxYJwx5lSRVfOBiSJSW0RaAW2BDRWPqRTQsCUMeZKQ5OXc22QXLy3YRfrJXLtTKeWUyjIVciYQCbQXkQQRuRN4D6gH/CwiW0TkIwBjzA5gNrATWATcb4zRqQ2q8vSz5r7/Pf8zCrKP8/LCXXYnUsopiTOMW0ZERJioqCi7YyhXkRAFnw3jt6bXcW3ceL6e1I+LwgPsTqVUtRORjcaYiOLW6RmqyvWERECfu4hI+obLGiTy9LxocvML7U6llFPRcleu6bJ/InWC+J/fFGKTM/lk5X67EynlVLTclWtyzH2vl76DV0IiefeXfRw8dtLuVEo5DS135bocc9+vzpxCC880/j57qw7PKOWg5a5cl2Puuwcwvfl3bDyYzgt65UilAC135eoatoQhT9D0yDJe73yQaZEH+XZjgt2plLKdlrtyff3ug6bduDrxv4wLPc3Tc7cTnZhpdyqlbKXlrlyfpzdcNw3B8CZv0MzPcM+XG/XsVVWjabkr99CoFVw9Ga/kHcwJ/YaUrBwemrWZAr2xtqqhtNyV+2g7HIY8SaN9c5jZYzurYlJ5Y8keu1MpZQstd+VeLvk/aDeC3rte5fHOmXywfD+Loo/YnUqpaqflrtyLhwdc+TE0aMFfk/7N4OBCHp29VW/uoWocLXflfnz9YeIM5PRxPvF5j7pehnu+jCIrJ8/uZEpVGy135Z6adIZx71I7cR3z2i0i7tgp/vHNVr17k6oxtNyV++p6DVx0L812T+HTnrEs3pHEhyv0AmOqZtByV+7t8uchtD9D9zzPPe2zeX3xHlbu1RuyK/en5a7cm6c3XDsF8fXn8eMv0CNQeGjWZuLTTp3/tUq5sLLcZu9zEUkWkegiy64VkR0iUigiEWc9/0kR2Scie0TkiqoIrdQFqdcErp2KR2YC0xtNprCwgL9O30hOnt4BUrmvsuy5TwFGnLUsGrgKWFl0oYh0AiYCnR2v+UBEPCseU6kKCr0IRryMX9xS5nVZy47Dx3lq7nY9wKrc1nnL3RizEkg7a9kuY0xxp/6NB2YZY04bY2KBfUDfSkmqVEX1uQu630B49Lv8r1cyczYlMn3dQbtTKVUlKnvMPRiIL/I4wbHsHCIySUSiRCQqJUUPcKlqIAJj3oKmXRh/4Fkmts7n3z/sZOPBtPO/VikXU9nlLsUsK/ZzrzHmE2NMhDEmIjAwsJJjKFUCb1+4fjqC8GLeq4T7e3Dv9E0kH8+xO5lSlaqyyz0BaFHkcQhwuJLfQ6mKaRgGV0/GM3kH3wTPIisnj/tmbNJb9Cm3UtnlPh+YKCK1RaQV0BbYUMnvoVTFtR0GQ5+mQcxcZvfcTtTBdF5asMvuVEpVmrJMhZwJRALtRSRBRO4UkStFJAG4GPhJRBYDGGN2ALOBncAi4H5jjM43U85p0KPQbiRdo//Lv7sfZ8raOOZs0lv0KfcgzjAVLCIiwkRFRdkdQ9VE2Rnw6VBM7knurfMmvyZ68t29/ekS3MDuZEqdl4hsNMZEFLdOz1BVNZuvP1w/AzmdxbtebxPkJ/x1+kYyTukt+pRr03JXqkknGPcu3okbmNNmIcnHT/PQrC16iz7l0rTclQLrCpL97idw5xSmRhxg5d4U3vp5r92plCo3LXelzhj+b2g5gIt3PM8jXU/z3q/7WLzjqN2plCoXLXelznBcQRJffx5K+Tf9gz15dPZWtidk2p1MqQum5a5UUXWD4LppyPFEPq/3Cf4+nlzz0Vq+35JodzKlLoiWu1Jna9EXRr6CT9wylvSKpHuIPw/P2sLLC3bpQVblMrTclSpOxJ3Q/Ub8Il/nqz77ublfKB+vPMAdU34j85TeaFs5Py13pYojAmPehFaX4PXD/bzQaBEvX0GhS5sAABXDSURBVNmFtftTGf/+amKSsuxOqFSptNyVKom3L9z0HXS9Dn55gRuS32LmnRGcOF3AhPfXsERn0ignpuWuVGm8asGVH8PAR2DjF0Sse5Af/9qD1kF1mfTlRt5eGkOhjsMrJ6TlrtT5eHjAsOdg1OsQs4Smc69j9s1tuKpnMG8t3ct9MzZx8nS+3SmV+hMtd6XKqu/dcP10SNqBz9QRvDGsHs+M7siSnUe56oO1HDx20u6ESv1Oy12pC9FhNNz2A+RkIpOHc1erNKbdcRFHj+cw7r01rI5JtTuhUoCWu1IXrkVfuPNnqFUXpoxhYOFvzH9gAE3r+3Dr5+v5bNUBnOFS2qpm03JXqjwat4G7lkJQB5h1Iy1jv2bOff25vFNTXvhpF4/O3kpOnt6nRtlHy12p8qobBLf9CG2GwY+PUGf1S3xwY0/+PrwdczYnct3HkRzJzLY7paqhynKbvc9FJFlEoossayQiP4tIjOP3ho7lIiLviMg+EdkmIr2qMrxStqtdFybOhF63wqo38Jh/Hw8Nbsknt/Rmf/IJxr67hqi4NLtTqhqoLHvuU4ARZy17AlhmjGkLLHM8BhiJdVPstsAk4MPKiamUE/P0grHvwNCnYetM+Oo6Lm/tx7z7B1C3tic3fLqOmRsO2Z1S1TDnLXdjzErg7F2P8cBUx9dTgQlFlk8zlnWAv4g0q6ywSjktERj8GIx/H2JXwhejaOt7gu/vH8jFrRvz5JztPDNvO7n5hXYnVTVEecfcmxhjjgA4fg9yLA8G4os8L8Gx7BwiMklEokQkKiUlpZwxlHIyPW+Gm2ZDeixMHk6DE/v54i99uOeScKavO8TNk9eTeuK03SlVDVDZB1SlmGXFzgkzxnxijIkwxkQEBgZWcgylbNRmGPzlJyjIhc8vxzM+kidHdeTtiT3YGp/BuHdXE52oNwBRVau85Z50ZrjF8XuyY3kC0KLI80KAw+WPp5SLat7DmgtfJwimTYAdcxnfI5jv7u0PwFUfruWNJXvIztXpkqpqlLfc5wO3Ob6+Dfi+yPJbHbNm+gGZZ4ZvlKpxGraEO5dA857wze0Q+QFdghsw/8GBjOzSlHd/2cdlbyznp21H9KQnVenKMhVyJhAJtBeRBBG5E3gFGC4iMcBwx2OABcABYB/wKXBflaRWylX4NYJb50HHMbD4SVj0FI39vHl7Yk9m33MxDfxqcf9Xm7jx0/XsOarXiFeVR5xhjyEiIsJERUXZHUOpqlNYAIufgvUfQecrYcJH4O1DQaHhqw2HeGPJHrJy8rmlX0seGd6OBr7edidWLkBENhpjIopbp2eoKlUdPDxhxCtw+QuwYy58OQHS4/D0EG7p15JfHx3CDX1bMC0yjqGvL2fWhkN6nXhVIVruSlUXEej/IFzzORzdDu/3g1VvQkEeDevU4oUJXZn/wEBaB9bhiTnbmfDBGjYdSrc7tXJROiyjlB0yE2Dh47D7RwjsCGPegpYXA2CMYf7Ww7y0YBdJx09zda8QHh/ZnqB6PjaHVs6mtGEZLXel7LRnISz4P8iMh563wPD/WAdhgROn83nvl31MXn2A2l6e/G1YW27rH4a3p37gVhYtd6WcWe5JWP4yRH4Avv5w+YvQfaI1jAPEpp7kPz/s4Nc9KbQOrMNz4zozqK2e+Kf0gKpSzq1WHetA6z0roVE4zPsrTB0LqTEAtGpchy9u78vk2yLILzTcMnkD93wZRXzaKZuDK2eme+5KOZPCQtg0BZY+B3nZMOBvMOhR8LbG20/nF/DZqlje+2UfhcZwz+DW3Du4Nb61PG2NreyhwzJKuZoTyda8+O3fWHvzo9+E1kN/X30kM5uXF+xm/tbDBPv78vTojozs0hSR4i7vpNyVDsso5WrqBsHVn8Et86zHX06A7+6ySh9o1sCXd27oydeT+lHPx4v7Zmzips/Ws+vIcRtDK2eie+5KObu8HFj9Jqx+C7x8Ydiz0Pt28LD2zfILCpm54RCvL9lLZnYeg9o25o4BrRjcLhAPD92Td2c6LKOUO0iNgR8fgbhVENIHxvwPmnb5fXXGqVxmrD/EtMg4ko6fJrxxHW4fEMbVvUPwq+VlX25VZbTclXIXxsC2r63x+OwMuPg+GPKkNePGIa+gkAXbj/D56li2JmRS38eLGy4K5daLwwj297UxvKpsWu5KuZtTabD0Wdg0DRq0gJGvQodRf3qKMYZNhzL4fE0si6KPAjCiS1PuGNCKXqH+evDVDWi5K+WuDq2zhmqSd0KHMdbFyfxbnPO0xIxspkXGMXP9IY7n5NM9pAF3DGzFyC7NqOWl8ypclZa7Uu6sIA8i34Pl/4XCfOh2PfR/AII6nvPUU7n5fLcpkS9Wx3Ig9SRN6tfm1ovDuKFvKI3q1LIhvKoILXelaoKMQ7D2Xdg8HfJOQZvhMOAhCBv0+6UMzigsNKyISeHz1bGsikmltpcHV/UK5vYBrWjXpJ5N34C6UFVW7iLyMHA31o2xPzXG/E9EGgFfA2FAHHCdMabU65ZquStViU6lwW+TYcPHcDIFmnWH/g9Bpwngee6smb1JWXyxJpY5mxI5nV+oUyldSJWUu4h0AWYBfYFcYBFwL1bZpxljXhGRJ4CGxpjHS9uWlrtSVSAvx5pZs/ZdOBZjHXjtdx/0ugVqn7t3nnYyl5kbdCqlK6mqcr8WuMIYc5fj8T+B08CdwBBjzBERaQYsN8a0L21bWu5KVaHCQohZbJX8wTXg0wAi7oC+90D9Zuc8PTe/kIXRR5i8OpZtjqmUo7s1Y3C7IAa2bUzd2lr0zqKqyr0j8D1wMZANLAOigFuMMf5FnpdujGlY2ra03JWqJgkbYe07sGs+iCd0uw4ufgCadDrnqdZUynSmrj3Ir7uTyTqdj7en0CesEUPaBzK0fRBtgurqlEobVeWY+53A/cAJYCdWyd9elnIXkUnAJIDQ0NDeBw8eLHcOpdQFSouFdR/8+eBr/weh1SXnHHwF68SojQfTWb4nheV7ktl9NAuAYH/f34u+f5sAHb6pZtUyW0ZEXgISgIfRYRmlXMOpNIiaDOs/gZPJRQ6+jgdP7xJfdjgj+/eiX7MvlZO5BdTy9OCi8EYMbhfI0A5BhDeuo3v1Vawq99yDjDHJIhIKLMEaonkKOFbkgGojY8xjpW1Hy10pm505+Br5HqTudRx8vRd63VrswdeicvMLiYpL49c9yfy6J4V9yScACG3k9/tefb/wAL3mfBWoynJfBQQAecDfjTHLRCQAmA2EAoeAa40xaaVtR8tdKSdx9sHX2g0g4nboc1exZ74WJz7tFMv3prB8dzJr9x8jO6+A2l4e9AsPYGj7QIa0DyKscZ3zb0idl57EpJS6cAkbIfJd2Pm9dcGysIHQ40boOA5q1y3TJnLyCtgQa+3Vr9iTwoHUk4B168Ah7QO5smcwXYMb6PBNOWm5K6XKL/2gNWSz5StIjwVvP6vge9wAYZf8fl35sohLPcnyPcks35vC2v3HyM0vpGOz+kzs04IJPYJp4FfyOL86l5a7UqrijIH49bB1JkTPhdOZUD/Emk7Z40Zo3PaCNpeZncf8rYeZ/Vs82xMzqeXlwaguTbm+Tyj9whvp3nwZaLkrpSpXXjbsWQBbZ8G+pWAKIbg3dL8BulwNfo0uaHPRiZnMjopn7uZEsnLyCQvw47o+LbimVwhB9X2q6JtwfVruSqmqk5UE22fDlpmQvAM8vKH9COh+I7QdXuqUyrPl5BWwMPoIszbEsz42DU8PYWj7ICb2acGQ9oF4eerliYvScldKVT1j4Oh2a9hm22w4lQp+AdD1WmuPvln3Yk+QKsmBlBPMjkrg240JpJ44TVC92lwbEcJ1ES1oGaCzbUDLXSlV3QryYN8y2PoV7FkIBbkQ2NE6CNv1umKvaVOSvIJCft2dzNe/xfPrnmQKDfRvHcD1fVpwReem+HjX3PnzWu5KKftkp0P0HGuPPuE3EA8IH2odhG17OfjUL/Omjmbm8O3GeL6Oiic+LZsGvt5c2TOY6/u0oGOzsm/HXWi5K6WcQ2qMdRB26yw4nmCNz4cNhHYjrHH6hmFl2kxhoSHywDFm/RbP4uij5BYU0j2kAdf3CWVs92bU86kZUyq13JVSzqWwEA5Fwt6FsGeRdb15sIZu2o+wyj6kD3icf8gl/WQu87YkMmtDPHuSsqjt5UH/1gFc1rEJl3YIorm/bxV/M/bRcldKObdj+2HvImt8/lCkdS9YvwBr2KbdFdD6svMO3xhj2JqQyfdbElm2K5lDaacA6NSsPpd1DOKyjk3oFtzAre4upeWulHId2Rmwf5m1Rx+zBHIyHMM3A6DdSKvsG7UqdRPGGPannGDZrmSW7Uom6mAahQYa163NpR0CuaxjEwa2aUwdF7/xiJa7Uso1FeRbZ8XuXWT9St1rLQ/sYA3dtBsBLfqed/gm41Quy/eksHRXEiv2ppCVk08tLw8uDg/gso5BXNohiJCGftXwDVUuLXellHs4th/2LrbG6g+utYZvfBtZwzftR0DrS63bCJYir6CQ3+LS+GVXMst2JxPruJhZh6b1fh++6R7ij6cLDN9ouSul3E92Buz/xdqjj1liTbn08IKW/SF8CLQaAs17nHev/sCZ4ZvdSfwWl05BoSGgTi2Gdgjisg5BDGoX6LT3jdVyV0q5t4J8aw793oUQs9S6DAJY16MPGwjhg6HVYAhsX+pZspmn8lgRk8KyXUks35NCZnYe3p7iuBZ9EJe0a0zrQOe5b6yWu1KqZjmRDLErIXYFHFgBGY57NNdtat0n9kzZl3IDknzHfWOX7U5m6a4kDqRYwzfNGvgwqG1jBrUNZGCbxjSsU6s6vqNiabkrpWq29Dir5GNXWKV/MsVa3ijcKvnwwda16esElLiJ+LRTrN6XyqqYFFbHpHI8Jx8R6Brc4Pey7xXakFpe1Xdxs6q8zd4jwF2AAbYDtwPNgFlAI2ATcIsxJre07Wi5K6WqjTGQvPOPso9bA7lZ1rqmXR1lPwRCLy7xjlMFhYZtCRmsirHKftOhDAoKDX61PLk4PMAq+3aBVX6T8CopdxEJBlYDnYwx2SIyG1gAjALmGGNmichHwFZjzIelbUvLXSllm4I8OLz5j7KPX29d6MzDyzpL9syefXAEeBU/BHM8J491+4/9XvZxx6wTqIL9fX/fqx/QJgB/v8odwqnKcl8HdAeOA/OAd4EZQFNjTL6IXAw8Z4y5orRtabkrpZxG7imIX/dH2R/eAhjw8oWQCGg5wJqRE9IHahU/N/7QsVOs2pfCqr2prNmfSpZjCKdbiD+XOMq+Z6g/3hW8Pn1VDss8DLwIZANLgIeBdcaYNo71LYCFxpguxbx2EjAJIDQ0tPfBgwfLnUMppapMdjrErbaGbw6ugaRo685THl7QvKdV9KH9IfQi8G14zsvzCwrZmpDJqpgUVsWksiXeGsKpW9uLfuHWpYuHd2pSrmhVtefeEPgOuB7IAL5xPH72rHJfYIzpWtq2dM9dKeUycjIhfoNV9AcjIXEjFOYBAk06W2V/pvDrnVvamdl5RO4/xqqYFFbGpHBD31DuG9KmXFFKK/eKzMwfBsQaY1IcbzIH6A/4i4iXMSYfCAEOV+A9lFLKufg0sG4f2Ha49TgvGxKirAueHVwDm2fAhk+sdY1a/1H2LfuDf0sa+HozoktTRnRpClh79lWhIuV+COgnIn5YwzKXAVHAr8A1WDNmbgO+r2hIpZRyWt6+0GqQ9QusA7RHtllFfygSdv0Am7+01tUPtmbhtOxvjd0Htq+y+8JWdMz931jDMvnAZqxpkcH8MRVyM3CzMeZ0advRYRmllNsqLISUXda1cM78OnHUWufbCAb9Hfo/WK5N60lMSinlLIyB9FhH0UdC66HQ9ZpybaqqxtyVUkpdKBHrzNhG4dDz5ip7m+o7T1YppVS10XJXSik3pOWulFJuSMtdKaXckJa7Ukq5IS13pZRyQ1ruSinlhrTclVLKDTnFGaoikgKU95q/jYHUSoxT1VwprytlBdfK60pZwbXyulJWqFjelsaYwOJWOEW5V4SIRJV0+q0zcqW8rpQVXCuvK2UF18rrSlmh6vLqsIxSSrkhLXellHJD7lDun9gd4AK5Ul5XygquldeVsoJr5XWlrFBFeV1+zF0ppdS53GHPXSml1Fm03JVSyg25dLmLyAgR2SMi+0TkCbvzlEREWojIryKyS0R2iMjDdmcqCxHxFJHNIvKj3VlKIyL+IvKtiOx2/BlfbHem0ojII45/B9EiMlNEfOzOVJSIfC4iySISXWRZIxH5WURiHL83tDPjGSVkfc3xb2GbiMwVEX87MxZVXN4i6/4hIkZEGlfGe7lsuYuIJ/A+MBLoBNwgIp3sTVWifOBRY0xHoB9wvxNnLephYJfdIcrgbWCRMaYD0B0nziwiwcBDQIQxpgvgCUy0N9U5pgAjzlr2BLDMGNMWWOZ47AymcG7Wn4EuxphuwF7gyeoOVYopnJsXEWkBDAcOVdYbuWy5A32BfcaYA8aYXKybco+3OVOxjDFHjDGbHF9nYZVPsL2pSiciIcBo4DO7s5RGROoDlwCTAYwxucaYDHtTnZcX4CsiXoAfcNjmPH9ijFkJpJ21eDww1fH1VGBCtYYqQXFZjTFLjDH5jofrgJBqD1aCEv5sAd4CHgMqbYaLK5d7MBBf5HECTl6YACISBvQE1tub5Lz+h/WPrdDuIOcRDqQAXziGkD4TkTp2hyqJMSYReB1rD+0IkGmMWWJvqjJpYow5AtbOChBkc56yugNYaHeI0ojIOCDRGLO1MrfryuUuxSxz6nmdIlIX+A74mzHmuN15SiIiY4BkY8xGu7OUgRfQC/jQGNMTOInzDBmcwzFWPR5oBTQH6ohI1d0luQYTkaexhkRn2J2lJCLiBzwN/Kuyt+3K5Z4AtCjyOAQn+3hblIh4YxX7DGPMHLvznMcAYJyIxGENd10qItPtjVSiBCDBGHPmk9C3WGXvrIYBscaYFGNMHjAH6G9zprJIEpFmAI7fk23OUyoRuQ0YA9xknPtkntZYP+i3Ov6/hQCbRKRpRTfsyuX+G9BWRFqJSC2sg1Lzbc5ULBERrDHhXcaYN+3Ocz7GmCeNMSHGmDCsP9dfjDFOuXdpjDkKxItIe8eiy4CdNkY6n0NAPxHxc/y7uAwnPgBcxHzgNsfXtwHf25ilVCIyAngcGGeMOWV3ntIYY7YbY4KMMWGO/28JQC/Hv+sKcdlydxwweQBYjPWfY7YxZoe9qUo0ALgFaw94i+PXKLtDuZEHgRkisg3oAbxkc54SOT5hfAtsArZj/R90qtPlRWQmEAm0F5EEEbkTeAUYLiIxWLM6XrEz4xklZH0PqAf87Pi/9pGtIYsoIW/VvJdzf2JRSilVHi67566UUqpkWu5KKeWGtNyVUsoNabkrpZQb0nJXSik3pOWulFJuSMtdKaXc0P8DVeuCEcAmnUAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "model, losses = train(train_ds, train_dl, val_ds, val_dl)\n",
    "\n",
    "# optionally plot your losses\n",
    "pd.DataFrame(losses).plot(title='Train and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.13 Evaluate model performance with BLEU [2 points]</b>\n",
    "\n",
    "While the loss is a good way of keeping track of our training progress, it is not a universal metric for our model performance. Heuristically, we have no way of telling how well our model is working for an application standpoint beyond the handful of examples we can print out -- not to mention the difficulty of comparing the quality of the translations on different models which are trained and evaluated on different datasets!\n",
    "\n",
    "The BLEU metric was proposed to combat this discrepancy. BLEU is empirically proven to align with how humans perceive the quality of translations. In 1-2 sentences, explain what (unigram) BLEU evaluates. Only include equations as you find them necessary. May Google be your friend (or Mooogle if you've downloaded and saved a lot of web pages about Machine Translation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a method stub that translates all the examples from the test set, and\n",
    "returns both the model's translations as well as the original translations.\n",
    "This method will then be called to compute your BLEU score.\n",
    "\n",
    "**IMPORTANT:** Pay close attention to how your translations should be formatted:\n",
    "- `candidate_text`: these are the model's translations. It should have type\n",
    "`List[List[str]]`. Each inner list corresponds to a list of translated words.\n",
    "It should look like:\n",
    "\n",
    "```python\n",
    "    candidate_text = [\n",
    "        ['this', 'is', 'sentence', 'one'],\n",
    "        ['here', 'is', 'another', 'mighty', 'fine,' 'translation']\n",
    "    ]\n",
    "```\n",
    "\n",
    "- `reference_text`: these are the actual translations. When BLEU is computed,\n",
    "it allows for there to be more than one possible reference translation for\n",
    "every machine-translated example. As a result, it has a weirder type: `List[List[List[str]]]`.\n",
    "The extra nested list holds all the possible reference translation for every\n",
    "one machine translation. It should look like:\n",
    "```python\n",
    "    reference_text = [\n",
    "        [\n",
    "            # these are all possible reference translations for the first machine translated example\n",
    "            ['this', 'is', 'sentence', 'one'],\n",
    "            ['here', 'is', 'another', 'translation', 'for', 'sentence one'],\n",
    "        ],\n",
    "        [\n",
    "            # this example only contains one reference translation\n",
    "            ['here', 'is', 'another', 'mighty', 'fine,' 'translation'],\n",
    "        ]\n",
    "    ]\n",
    "```\n",
    "\n",
    "Practically, because we only have a single reference translation for every\n",
    "machine translation, your `reference_text` list should hold only a single\n",
    "example for every machine-translated text.\n",
    "\n",
    "For reference, we will use PyTorch's [bleu_score()](https://pytorch.org/text/stable/data_metrics.html) function\n",
    "to compute these scores.\n",
    "\n",
    "**NOTE**: `bleu_score()` is not included in the standard PyTorch installation.\n",
    "You will need to install the additional package [TorchText](https://pytorch.org/text/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_corpus(vocab: Dict[str, int], test_dl: DataLoader, model: nn.Module) -> Tuple[List[List[str]], List[List[List[str]]]]:\n",
    "    index_to_word = {v: k for k, v in vocab.items()}\n",
    "    candidate_text = []\n",
    "    reference_text = []\n",
    "\n",
    "    for source, target in test_dl:\n",
    "        print(\"THis is source shape \", source.shape)\n",
    "        print(\"This is target shape \", target.shape)\n",
    "        #import pdb\n",
    "        #pdb.set_trace()\n",
    "        current_translate_sentence = []\n",
    "        current_target_sentence = []\n",
    "        for source_item, target_item in zip(source, target):\n",
    "            print(\"This is source_item shape \", source_item.shape)\n",
    "            print(\"This is target_item shape \", target_item.shape)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            translation = model.generate(source_item, 100, EOS_IDX)\n",
    "            print(\"This is translation \", translation)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            #current_translate_sentence.append(index_to_word[idx])\n",
    "        candidate_text.append([index_to_word[idx] for idx in translation])\n",
    "        reference_text.append([[index_to_word[idx] for idx in target_item]])\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "        \n",
    "        \n",
    "    return candidate_text, reference_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to compute you BLEU score. We achieved scores of around 0.3 - 0.5\n",
    "\n",
    "Depending on how you implemented your model, your mileage may vary. Scores\n",
    "less than 0.1 probably indicate something is off with your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THis is source shape  torch.Size([32, 26])\n",
      "This is target shape  torch.Size([32, 14])\n",
      "This is source_item shape  torch.Size([26])\n",
      "This is target_item shape  torch.Size([14])\n",
      "> <ipython-input-35-f1cbee9ce715>(18)translate_corpus()\n",
      "-> translation = model.generate(source_item, 100, EOS_IDX)\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsoldevilla/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is source in generate shape  torch.Size([26])\n",
      "the input_seqs shape in encoder  torch.Size([1, 26])\n",
      "> <ipython-input-30-16a7dd1eabbf>(59)generate()\n",
      "-> break\n",
      "(Pdb) next_idx\n",
      "tensor(1)\n",
      "(Pdb) flatten_hidden_states\n",
      "tensor([[ 0.5831, -0.6609,  0.9591,  0.9806,  0.9889, -0.6912,  0.9157, -0.9294,\n",
      "          0.9717, -0.9743,  0.9490, -0.9385,  0.9701, -0.9862, -0.5556, -0.8832,\n",
      "          0.9822,  0.8999, -0.9649,  0.9592, -0.9509, -0.9620,  0.9639, -0.9595,\n",
      "          0.9914,  0.9841,  0.9231, -0.9755,  0.9759, -0.9759,  0.9735,  0.9111,\n",
      "         -0.6589,  0.7051, -0.9835,  0.9778, -0.7045, -0.9831, -0.9820,  0.9928,\n",
      "          0.9707, -0.9129, -0.9898, -0.9372, -0.8985,  0.9744, -0.9948, -0.9502,\n",
      "         -0.0017,  0.8915,  0.7007, -0.9672, -0.9434,  0.9888,  0.9526,  0.9759,\n",
      "          0.9164,  0.9673, -0.9146,  0.9800,  0.9841,  0.9866,  0.9278, -0.9459,\n",
      "         -0.9722, -0.8291,  0.9501, -0.9796,  0.9421,  0.8969,  0.6287,  0.9640,\n",
      "         -0.9642, -0.8944, -0.8339, -0.6119,  0.9276,  0.9643, -0.9794, -0.9894,\n",
      "          0.9766,  0.9280,  0.9927, -0.8815, -0.9818, -0.8953,  0.9810,  0.9107,\n",
      "          0.9221, -0.9710, -0.9677,  0.9796, -0.9844,  0.9920, -0.9599,  0.1752,\n",
      "         -0.9314, -0.9050,  0.7906, -0.9800]])\n",
      "(Pdb) flatten_hidden_states.shape\n",
      "torch.Size([1, 100])\n",
      "(Pdb) out.shape\n",
      "torch.Size([1, 1218])\n",
      "(Pdb) out\n",
      "tensor([[-2.9906,  6.8921,  2.4446,  ..., -1.6797, -2.5197, -1.9665]])\n",
      "(Pdb) exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-69e4b27e8c13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcandidate_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-f1cbee9ce715>\u001b[0m in \u001b[0;36mtranslate_corpus\u001b[0;34m(vocab, test_dl, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOS_IDX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is translation \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-16a7dd1eabbf>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, source, max_steps, eos_idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mblown_next_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_final_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_final_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblown_next_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_final_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-16a7dd1eabbf>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, source, max_steps, eos_idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mblown_next_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_final_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_final_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblown_next_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_final_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "candidate_text, reference_text = translate_corpus(train_ds.target_vocab, test_dl, model)\n",
    "bleu_score(candidate_text, reference_text, max_n=1, weights=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ASK WHERE WE SHOULD USE PADDING IDX\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int, padding_idx: int):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.embeddings = nn.Embedding(input_size, hidden_size)#, padding_idx=self.padding_idx) \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True) ## ASK WHAT TO DO WITH THESE DIMENSIONS\n",
    "        \n",
    "    def forward(self, input_seqs: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        # Finding the size of \n",
    "        seqs_length = [((t == 0).nonzero(as_tuple=True)[0])[0].item() if len(((t == 0).nonzero(as_tuple=True)[0])) != 0 else t.shape.numel() for t in input_seqs]\n",
    "        embedding_sequences = self.embeddings(input_seqs)\n",
    "        if 0 in seqs_length:\n",
    "            print(seqs_length)\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        packed_seq_batch = torch.nn.utils.rnn.pack_padded_sequence(embedding_sequences, lengths=seqs_length, batch_first=True, enforce_sorted=False)\n",
    "        output, (hn, cn) = self.lstm(packed_seq_batch.float()) \n",
    "        return (hn, cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_yellow'><b>2.14 Identify the mystery language [6 points]</b>\n",
    "    \n",
    "You are now ready to unveil the mystery language! Train a model for each of the five different pairings:\n",
    "* Danish --> Mystery\n",
    "* English --> Mystery\n",
    "* German --> Mystery\n",
    "* Finnish --> Mystery\n",
    "* Spanish --> Mystery\n",
    "\n",
    "**NOTE:** The direction of translation is the *opposite* of what you did before\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: train five different models, one for each of the five different pairings\n",
    "\n",
    "raise NotImplementedError   # TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the BLEU score of all five models on the test set. The one with the highest BLEU score, often by a noticeable shot, is the underlying language of the mystery language. Report your findings below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluate a BLEU score for all five models to identify the mystery language!\n",
    "\n",
    "raise NotImplementedError   # TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the mystery language?: YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_green'>\n",
    "    \n",
    "# 3. RESEARCH [20 points]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we build a foundation in NLP, it's also important to also see what the latest, cutting-edge work (research) looks like. It's incredibly worthwhile to learn about the types of problems people work on, their methodology and approach to the problem, the datasets they work on, the issues they raise, and the solutions they posit. The field moves incredibly fast, but the __approach__ to ML/NLP research is relatively stable -- different types of papers are accepted as the years progress, but that's a different story.\n",
    "\n",
    "We want to help you get practice reading research papers, which mostly entails thinking critically about the work, being able to discern the main takeaways/conclusions, and to reflect on the work in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.1: Read an NLP research paper [0 points]</b>\n",
    "    \n",
    "Select and read a paper that was published in ACL, NAACL, EMNLP, or COLING in 2020 or 2021. You can find a list of such published papers by searching Google (Mooogle can't help you here), a la \"ACL 2020 accepted papers\". For this assignment, you are allowed to pick either a short paper (4-5 pages) or long paper (8-9 pages), **but you must not select a workshop paper**. List below the name of the paper, authors, venue, and year published.\n",
    "\n",
    "While I highly encourage you to look at the aforementioned venues to find a paper that interests you, alternatively, you could select one of the following papers:\n",
    "\n",
    "- An LSTM-based precursor to BERT:\n",
    "> context2vec: Learning Generic Context Embedding with Bidirectional LSTM. Melamud et al. CoNLL 2016.\n",
    "\n",
    "- Clever, famous usage of Attention for NLP + Vision:\n",
    "> Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Xu et al. ICML 2015.\n",
    "\n",
    "- Breakthrough paper that illustrated the power of seq2seq for Machine Translation:\n",
    "> Neural Machine Translation by Jointly Learning to Align and Translate. Bahdanau, Cho, and Bengio. ICLR 2015.\n",
    "\n",
    "- Another famous MT paper around the same time:\n",
    "> Sequence to Sequence Learning with Neural Networks. Sutskever, Vinyals, and Le. NIPS 2014 (now called NeurIPS).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR PAPER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.2: Problem? [2 point]</b> What is the problem that it is trying to address? In other words, what is it trying to solve? (2-3 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.3: Solution? [2 point]</b> At a very high-level, what was their solution? (2-3 sentences). Here, you don't have room to go into the small details (e.g., about the model), so you'll need to summarize the most important elements that comprised the solution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.4: Data? [2 points]</b> What dataset(s) did they use? Are they freely available? What's the size of the data? (2-3 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.5: Model [2 points]</b> Very related to the 'solutions' question, describe here any models that they used, and what made it effective (2-3 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.6: Results? [2 points]</b> What are their main results? (~2 sentences)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.7: Strengths? [2 points]</b> List 2-3 strengths of the paper\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.8: Weaknesses? [2 points]</b> Although you may be new to this problem and all of its content, try to list 2-3 weaknesses of the paper (anything that you think could strengthen the paper is sufficient).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.9: Evaluation [2 points]</b>\n",
    "    \n",
    "How would you evaluate this paper in terms of:\n",
    "- scientific contribution\n",
    "- effectiveness to solve the problem\n",
    "- how convincing it was.\n",
    "    \n",
    "Give each of these elements a score from 1-10 (10 is best). No word explanation necessary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>3.10: Research Ideas [4 points]</b>\n",
    "    \n",
    "Think of 1-2 research ideas that you have based on this paper. It doesn't have to be grand; most research is very incremental. Specifically, your research idea should have a concrete question that you're aiming to answer. List it below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_green'><b>BONUS POINTS [5 points]</b> I mention the full details in the syllabus on the course website. However, in short, these bonus points cannot bring one's grade to exceed 100. That is, if someone received a 97 on this homework, doing this bonus could allow their grade to reach 100 points. If the person had an 83 on the homework, then the most they could achieve is an 88.\n",
    "    \n",
    "The task: read another research paper (allowed to be a Short Paper) and answer the same questions again. Please copy and paste all of the questions below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='header_blue'>\n",
    "    \n",
    "# 4. SELF-REFLECTION [0 points]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='q_blue'><b>4.1: Self-reflection and Feedback [0 points]</b>\n",
    "\n",
    "Are you thriving in the course? Are there elements that are particularly confusing to you? I want everyone to be and feel fully supported. Toward this, I strongly urge you all to think critically about your own learning and efforts. Please provide us with feedback about how you're doing in the course and if there's anything further or different we can do to better assist your learning. I want everyone to give their earnest account, so the form is completely anonymous.\n",
    "\n",
    "</div>\n",
    "\n",
    "[Anonymous Self-Reflection and Feedback Form](https://forms.gle/3LT6UfhtCtqp2G7X9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "interpreter": {
   "hash": "0d3e0cb9dfa8cf737dbf538b02aa0665a58b73e014f15f8af0ea4296e4e6b732"
  },
  "kernel_loop": {
   "byte_size": "82791"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
